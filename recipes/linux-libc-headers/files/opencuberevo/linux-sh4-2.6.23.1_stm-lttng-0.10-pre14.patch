This adds LTTng 0.10-pre14 into the kernel 2.6.23.1.
It includes: markers, ltt core, SUPERH instrumentation code and
timestamp support.

Signed-off-by: Giuseppe Cavallaro <peppe.cavallaro@st.com>

========================================================================

diff -uprN linux-2.6.23.1.orig/Makefile linux-2.6.23.1/Makefile
--- linux-2.6.23.1.orig/Makefile	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/Makefile	2007-11-28 10:13:52.381344838 +0100
@@ -560,7 +560,7 @@ export mod_strip_cmd
 
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/ ltt/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff -uprN linux-2.6.23.1.orig/arch/sh/Kconfig linux-2.6.23.1/arch/sh/Kconfig
--- linux-2.6.23.1.orig/arch/sh/Kconfig	2007-11-12 18:23:55.000000000 +0100
+++ linux-2.6.23.1/arch/sh/Kconfig	2007-11-28 10:13:52.306360471 +0100
@@ -93,6 +93,12 @@ config ARCH_HAS_ILOG2_U64
 config ARCH_NO_VIRT_TO_BUS
 	def_bool y
 
+config ARCH_SUPPORTS_LTT_CLOCK
+	def_bool y
+
+config ARCH_NEEDS_LTT_SYNTHETIC_TSC
+	def_bool y
+
 source "init/Kconfig"
 
 menu "System type"
@@ -874,31 +880,7 @@ source "fs/Kconfig"
 
 source "arch/sh/oprofile/Kconfig"
 
-menu "Instrumention Support"
-
-config KPROBES
-	bool "Kprobes (EXPERIMENTAL)"
-	depends on EXPERIMENTAL
-	help
-	  Kprobes allows you to trap at almost any kernel address and
-	  execute a callback function.  register_kprobe() establishes
-	  a probepoint and specifies the callback.  Kprobes is useful
-	  for kernel debugging, non-intrusive instrumentation and testing.
-	  If in doubt, say "N".
-
-config KPTRACE
-	tristate "KPTrace (EXPERIMENTAL)"
-	select KPROBES
-	select DEBUG_FS
-	select RELAY
-	select SYSFS
-	select KALLSYMS
-	help
-	  KPTrace is a Kprobes-based trace tool. It provides a full set
-	  of pre-defined tracepoints, plus a simple way to add and remove
-	  tracepoints at runtime.
-
-endmenu
+source "kernel/Kconfig.instrumentation"
 
 source "arch/sh/Kconfig.debug"
 
diff -uprN linux-2.6.23.1.orig/arch/sh/kernel/entry-common.S linux-2.6.23.1/arch/sh/kernel/entry-common.S
--- linux-2.6.23.1.orig/arch/sh/kernel/entry-common.S	2007-11-28 10:17:05.108165724 +0100
+++ linux-2.6.23.1/arch/sh/kernel/entry-common.S	2007-11-28 10:13:52.307360262 +0100
@@ -205,7 +205,7 @@ work_resched:
 syscall_exit_work:
 	! r0: current_thread_info->flags
 	! r8: current_thread_info
-	tst	#_TIF_SYSCALL_TRACE | _TIF_SINGLESTEP, r0
+	tst	#_TIF_SYSCALL_TRACE | _TIF_SINGLESTEP | _TIF_KERNEL_TRACE, r0
 	bt/s	work_pending
 	 tst	#_TIF_NEED_RESCHED, r0
 #ifdef CONFIG_TRACE_IRQFLAGS
@@ -214,7 +214,8 @@ syscall_exit_work:
 	 nop
 #endif
 	sti
-	! XXX setup arguments...
+	mov	r15,r4			! pass stacked regs as arg
+	mov	#0, r5			! trace entry [0]
 	mov.l	4f, r0			! do_syscall_trace
 	jsr	@r0
 	 nop
@@ -224,7 +225,8 @@ syscall_exit_work:
 	.align	2
 syscall_trace_entry:
 	!                     	Yes it is traced.
-	! XXX setup arguments...
+	mov	r15,r4		! pass stacked regs as arg
+	mov	#1, r5		! trace entry [1]
 	mov.l	4f, r11		! Call do_syscall_trace which notifies
 	jsr	@r11	    	! superior (will chomp R[0-7])
 	 nop
@@ -347,7 +349,7 @@ ENTRY(system_call)
 	!
 	get_current_thread_info r8, r10
 	mov.l	@(TI_FLAGS,r8), r8
-	mov	#_TIF_SYSCALL_TRACE, r10
+	mov	#(_TIF_SYSCALL_TRACE | _TIF_KERNEL_TRACE), r10
 	tst	r10, r8
 	bf	syscall_trace_entry
 	!
diff -uprN linux-2.6.23.1.orig/arch/sh/kernel/process.c linux-2.6.23.1/arch/sh/kernel/process.c
--- linux-2.6.23.1.orig/arch/sh/kernel/process.c	2007-11-12 18:23:55.000000000 +0100
+++ linux-2.6.23.1/arch/sh/kernel/process.c	2007-11-28 10:15:09.389290249 +0100
@@ -168,6 +168,7 @@ __asm__(".align 5\n"
 /* Don't use this in BL=1(cli).  Or else, CPU resets! */
 int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 {
+	unsigned long pid;
 	struct pt_regs regs;
 
 	memset(&regs, 0, sizeof(regs));
@@ -178,8 +179,10 @@ int kernel_thread(int (*fn)(void *), voi
 	regs.sr = (1 << 30);
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0,
+	pid =  do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0,
 		       &regs, 0, NULL, NULL);
+	trace_mark(kernel_arch_kthread_create, "pid %ld fn %p", pid, fn);
+	return pid;
 }
 
 /*
diff -uprN linux-2.6.23.1.orig/arch/sh/kernel/ptrace.c linux-2.6.23.1/arch/sh/kernel/ptrace.c
--- linux-2.6.23.1.orig/arch/sh/kernel/ptrace.c	2007-11-12 18:23:52.000000000 +0100
+++ linux-2.6.23.1/arch/sh/kernel/ptrace.c	2007-11-28 10:13:52.308360054 +0100
@@ -263,10 +263,16 @@ long arch_ptrace(struct task_struct *chi
 	return ret;
 }
 
-asmlinkage void do_syscall_trace(void)
+asmlinkage void do_syscall_trace(struct pt_regs *regs, int entryexit)
 {
 	struct task_struct *tsk = current;
 
+	if (entryexit)
+		trace_mark(kernel_arch_syscall_entry, "syscall_id %d ip #p%ld",
+			regs->regs[3], instruction_pointer(regs));
+	else
+		trace_mark(kernel_arch_syscall_exit, MARK_NOARGS);
+
 	if (!test_thread_flag(TIF_SYSCALL_TRACE) &&
 	    !test_thread_flag(TIF_SINGLESTEP))
 		return;
diff -uprN linux-2.6.23.1.orig/arch/sh/kernel/sh_ksyms.c linux-2.6.23.1/arch/sh/kernel/sh_ksyms.c
--- linux-2.6.23.1.orig/arch/sh/kernel/sh_ksyms.c	2007-11-12 18:23:54.000000000 +0100
+++ linux-2.6.23.1/arch/sh/kernel/sh_ksyms.c	2007-11-28 10:13:52.308360054 +0100
@@ -26,7 +26,6 @@ EXPORT_SYMBOL(sh_mv);
 /* platform dependent support */
 EXPORT_SYMBOL(dump_fpu);
 EXPORT_SYMBOL(kernel_thread);
-EXPORT_SYMBOL(irq_desc);
 EXPORT_SYMBOL(no_irq_type);
 
 EXPORT_SYMBOL(strlen);
diff -uprN linux-2.6.23.1.orig/arch/sh/kernel/sys_sh.c linux-2.6.23.1/arch/sh/kernel/sys_sh.c
--- linux-2.6.23.1.orig/arch/sh/kernel/sys_sh.c	2007-11-12 18:23:55.000000000 +0100
+++ linux-2.6.23.1/arch/sh/kernel/sys_sh.c	2007-11-28 10:13:52.309359845 +0100
@@ -193,6 +193,8 @@ asmlinkage int sys_ipc(uint call, int fi
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_mark(kernel_arch_ipc_call, "call %u first %d", call, first);
+
 	if (call <= SEMCTL)
 		switch (call) {
 		case SEMOP:
diff -uprN linux-2.6.23.1.orig/arch/sh/kernel/traps.c linux-2.6.23.1/arch/sh/kernel/traps.c
--- linux-2.6.23.1.orig/arch/sh/kernel/traps.c	2007-11-28 10:17:05.106166140 +0100
+++ linux-2.6.23.1/arch/sh/kernel/traps.c	2007-11-28 10:13:52.310359637 +0100
@@ -554,6 +554,9 @@ asmlinkage void do_address_error(struct 
 	lookup_exception_vector(error_code);
 #endif
 
+	trace_mark(kernel_arch_trap_entry, "trap_id %ld ip #p%ld",
+		(error_code >> 5), instruction_pointer(regs));
+
 	oldfs = get_fs();
 
 	if (user_mode(regs)) {
@@ -580,8 +583,10 @@ asmlinkage void do_address_error(struct 
 		tmp = handle_unaligned_access(instruction, regs);
 		set_fs(oldfs);
 
-		if (tmp>=0)
+		if (tmp>=0){
+			trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 			return; /* sorted */
+		}
 #endif
 
 uspace_segv:
@@ -617,6 +622,7 @@ uspace_segv:
 		force_sig(SIGSEGV, current);
 #endif
 	}
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 }
 
 #ifdef CONFIG_SH_DSP
diff -uprN linux-2.6.23.1.orig/arch/sh/mm/fault.c linux-2.6.23.1/arch/sh/mm/fault.c
--- linux-2.6.23.1.orig/arch/sh/mm/fault.c	2007-11-28 10:17:05.107165932 +0100
+++ linux-2.6.23.1/arch/sh/mm/fault.c	2007-11-28 10:13:52.310359637 +0100
@@ -80,6 +80,14 @@ asmlinkage void __kprobes do_page_fault(
 		return;
 	}
 
+	trace_mark(kernel_arch_trap_entry, "trap_id %ld ip #p%ld",
+		({
+			unsigned long trapnr;
+			asm volatile("stc	r2_bank,%0": "=r" (trapnr));
+			trapnr;
+		}) >> 5,
+		instruction_pointer(regs));
+
 	/*
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
@@ -132,6 +140,7 @@ survive:
 		tsk->min_flt++;
 
 	up_read(&mm->mmap_sem);
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 	return;
 
 /*
@@ -148,6 +157,7 @@ bad_area_nosemaphore:
 		info.si_code = si_code;
 		info.si_addr = (void *) address;
 		force_sig_info(SIGSEGV, &info, tsk);
+		trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 		return;
 	}
 
@@ -227,4 +237,6 @@ do_sigbus:
 	/* Kernel mode? Handle exceptions or die */
 	if (!user_mode(regs))
 		goto no_context;
+
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 }
diff -uprN linux-2.6.23.1.orig/block/Kconfig linux-2.6.23.1/block/Kconfig
--- linux-2.6.23.1.orig/block/Kconfig	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/block/Kconfig	2007-11-28 10:13:52.312359220 +0100
@@ -32,6 +32,7 @@ config BLK_DEV_IO_TRACE
 	depends on SYSFS
 	select RELAY
 	select DEBUG_FS
+	select MARKERS
 	help
 	  Say Y here, if you want to be able to trace the block layer actions
 	  on a given queue. Tracing allows you to see any traffic happening
diff -uprN linux-2.6.23.1.orig/block/blktrace.c linux-2.6.23.1/block/blktrace.c
--- linux-2.6.23.1.orig/block/blktrace.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/block/blktrace.c	2007-11-28 10:13:52.311359428 +0100
@@ -23,11 +23,19 @@
 #include <linux/mutex.h>
 #include <linux/debugfs.h>
 #include <linux/time.h>
+#include <linux/marker.h>
 #include <asm/uaccess.h>
 
 static DEFINE_PER_CPU(unsigned long long, blk_trace_cpu_offset) = { 0, };
 static unsigned int blktrace_seq __read_mostly = 1;
 
+/* Global reference count of probes */
+static DEFINE_MUTEX(blk_probe_mutex);
+static int blk_probes_ref;
+
+int blk_probe_arm(void);
+void blk_probe_disarm(void);
+
 /*
  * Send out a notify message.
  */
@@ -179,7 +187,7 @@ void __blk_add_trace(struct blk_trace *b
 EXPORT_SYMBOL_GPL(__blk_add_trace);
 
 static struct dentry *blk_tree_root;
-static struct mutex blk_tree_mutex;
+static DEFINE_MUTEX(blk_tree_mutex);
 static unsigned int root_users;
 
 static inline void blk_remove_root(void)
@@ -229,6 +237,10 @@ static void blk_trace_cleanup(struct blk
 	blk_remove_tree(bt->dir);
 	free_percpu(bt->sequence);
 	kfree(bt);
+	mutex_lock(&blk_probe_mutex);
+	if (--blk_probes_ref == 0)
+		blk_probe_disarm();
+	mutex_unlock(&blk_probe_mutex);
 }
 
 static int blk_trace_remove(struct request_queue *q)
@@ -386,6 +398,11 @@ static int blk_trace_setup(struct reques
 		goto err;
 	}
 
+	mutex_lock(&blk_probe_mutex);
+	if (!blk_probes_ref++)
+		blk_probe_arm();
+	mutex_unlock(&blk_probe_mutex);
+
 	return 0;
 err:
 	if (dir)
@@ -549,9 +566,331 @@ static void blk_trace_set_ht_offsets(voi
 #endif
 }
 
+/**
+ * blk_add_trace_rq - Add a trace for a request oriented action
+ * Expected variable arguments :
+ * @q:		queue the io is for
+ * @rq:		the source request
+ *
+ * Description:
+ *     Records an action against a request. Will log the bio offset + size.
+ *
+ **/
+static void blk_add_trace_rq(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	va_list args;
+	u32 what;
+	struct blk_trace *bt;
+	int rw;
+	struct blk_probe_data *pinfo = mdata->private;
+	struct request_queue *q;
+	struct request *rq;
+
+	va_start(args, fmt);
+	q = va_arg(args, struct request_queue *);
+	rq = va_arg(args, struct request *);
+	va_end(args);
+
+	what = pinfo->flags;
+	bt = q->blk_trace;
+	rw = rq->cmd_flags & 0x03;
+
+	if (likely(!bt))
+		return;
+
+	if (blk_pc_request(rq)) {
+		what |= BLK_TC_ACT(BLK_TC_PC);
+		__blk_add_trace(bt, 0, rq->data_len, rw, what, rq->errors, sizeof(rq->cmd), rq->cmd);
+	} else  {
+		what |= BLK_TC_ACT(BLK_TC_FS);
+		__blk_add_trace(bt, rq->hard_sector, rq->hard_nr_sectors << 9, rw, what, rq->errors, 0, NULL);
+	}
+}
+
+/**
+ * blk_add_trace_bio - Add a trace for a bio oriented action
+ * Expected variable arguments :
+ * @q:		queue the io is for
+ * @bio:	the source bio
+ *
+ * Description:
+ *     Records an action against a bio. Will log the bio offset + size.
+ *
+ **/
+static void blk_add_trace_bio(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	va_list args;
+	u32 what;
+	struct blk_trace *bt;
+	struct blk_probe_data *pinfo = mdata->private;
+	struct request_queue *q;
+	struct bio *bio;
+
+	va_start(args, fmt);
+	q = va_arg(args, struct request_queue *);
+	bio = va_arg(args, struct bio *);
+	va_end(args);
+
+	what = pinfo->flags;
+	bt = q->blk_trace;
+
+	if (likely(!bt))
+		return;
+
+	__blk_add_trace(bt, bio->bi_sector, bio->bi_size, bio->bi_rw, what, !bio_flagged(bio, BIO_UPTODATE), 0, NULL);
+}
+
+/**
+ * blk_add_trace_generic - Add a trace for a generic action
+ * Expected variable arguments :
+ * @q:		queue the io is for
+ * @bio:	the source bio
+ * @rw:		the data direction
+ *
+ * Description:
+ *     Records a simple trace
+ *
+ **/
+static void blk_add_trace_generic(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	va_list args;
+	struct blk_trace *bt;
+	u32 what;
+	struct blk_probe_data *pinfo = mdata->private;
+	struct request_queue *q;
+	struct bio *bio;
+	int rw;
+
+	va_start(args, fmt);
+	q = va_arg(args, struct request_queue *);
+	bio = va_arg(args, struct bio *);
+	rw = va_arg(args, int);
+	va_end(args);
+
+	what = pinfo->flags;
+	bt = q->blk_trace;
+
+	if (likely(!bt))
+		return;
+
+	if (bio)
+		blk_add_trace_bio(mdata, "%p %p", NULL, q, bio);
+	else
+		__blk_add_trace(bt, 0, 0, rw, what, 0, 0, NULL);
+}
+
+/**
+ * blk_add_trace_pdu_ll - Add a trace for a bio with any integer payload
+ * Expected variable arguments :
+ * @q:		queue the io is for
+ * @bio:	the source bio
+ * @pdu:	the long long integer payload
+ *
+ **/
+static inline void blk_trace_integer(struct request_queue *q, struct bio *bio, unsigned long long pdu,
+					u32 what)
+{
+	struct blk_trace *bt;
+	__be64 rpdu;
+
+	bt = q->blk_trace;
+	rpdu = cpu_to_be64(pdu);
+
+	if (likely(!bt))
+		return;
+
+	if (bio)
+		__blk_add_trace(bt, bio->bi_sector, bio->bi_size, bio->bi_rw, what,
+					!bio_flagged(bio, BIO_UPTODATE), sizeof(rpdu), &rpdu);
+	else
+		__blk_add_trace(bt, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu);
+}
+
+/**
+ * blk_add_trace_pdu_ll - Add a trace for a bio with an long long integer
+ * payload
+ * Expected variable arguments :
+ * @q:		queue the io is for
+ * @bio:	the source bio
+ * @pdu:	the long long integer payload
+ *
+ * Description:
+ *     Adds a trace with some long long integer payload. This might be an unplug
+ *     option given as the action, with the depth at unplug time given as the
+ *     payload
+ *
+ **/
+static void blk_add_trace_pdu_ll(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	va_list args;
+	struct blk_probe_data *pinfo = mdata->private;
+	struct request_queue *q;
+	struct bio *bio;
+	unsigned long long pdu;
+	u32 what;
+
+	what = pinfo->flags;
+
+	va_start(args, fmt);
+	q = va_arg(args, struct request_queue *);
+	bio = va_arg(args, struct bio *);
+	pdu = va_arg(args, unsigned long long);
+	va_end(args);
+
+	blk_trace_integer(q, bio, pdu, what);
+}
+
+
+/**
+ * blk_add_trace_pdu_int - Add a trace for a bio with an integer payload
+ * Expected variable arguments :
+ * @q:		queue the io is for
+ * @bio:	the source bio
+ * @pdu:	the integer payload
+ *
+ * Description:
+ *     Adds a trace with some integer payload. This might be an unplug
+ *     option given as the action, with the depth at unplug time given
+ *     as the payload
+ *
+ **/
+static void blk_add_trace_pdu_int(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	va_list args;
+	struct blk_probe_data *pinfo = mdata->private;
+	struct request_queue *q;
+	struct bio *bio;
+	unsigned int pdu;
+	u32 what;
+
+	what = pinfo->flags;
+
+	va_start(args, fmt);
+	q = va_arg(args, struct request_queue *);
+	bio = va_arg(args, struct bio *);
+	pdu = va_arg(args, unsigned int);
+	va_end(args);
+
+	blk_trace_integer(q, bio, pdu, what);
+}
+
+/**
+ * blk_add_trace_remap - Add a trace for a remap operation
+ * Expected variable arguments :
+ * @q:		queue the io is for
+ * @bio:	the source bio
+ * @dev:	target device
+ * @from:	source sector
+ * @to:		target sector
+ *
+ * Description:
+ *     Device mapper or raid target sometimes need to split a bio because
+ *     it spans a stripe (or similar). Add a trace for that action.
+ *
+ **/
+static void blk_add_trace_remap(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	va_list args;
+	struct blk_trace *bt;
+	struct blk_io_trace_remap r;
+	u32 what;
+	struct blk_probe_data *pinfo = mdata->private;
+	struct request_queue *q;
+	struct bio *bio;
+	u64 dev, from, to;
+
+	va_start(args, fmt);
+	q = va_arg(args, struct request_queue *);
+	bio = va_arg(args, struct bio *);
+	dev = va_arg(args, u64);
+	from = va_arg(args, u64);
+	to = va_arg(args, u64);
+	va_end(args);
+
+	what = pinfo->flags;
+	bt = q->blk_trace;
+
+	if (likely(!bt))
+		return;
+
+	r.device = cpu_to_be32(dev);
+	r.device_from = cpu_to_be32(bio->bi_bdev->bd_dev);
+	r.sector = cpu_to_be64(to);
+
+	__blk_add_trace(bt, from, bio->bi_size, bio->bi_rw, BLK_TA_REMAP, !bio_flagged(bio, BIO_UPTODATE), sizeof(r), &r);
+}
+
+#define FACILITY_NAME "blk"
+
+static struct blk_probe_data probe_array[] =
+{
+	{ "blk_bio_queue", "%p %p", BLK_TA_QUEUE, blk_add_trace_bio },
+	{ "blk_bio_backmerge", "%p %p", BLK_TA_BACKMERGE, blk_add_trace_bio },
+	{ "blk_bio_frontmerge", "%p %p", BLK_TA_FRONTMERGE, blk_add_trace_bio },
+	{ "blk_get_request", "%p %p %d", BLK_TA_GETRQ, blk_add_trace_generic },
+	{ "blk_sleep_request", "%p %p %d", BLK_TA_SLEEPRQ,
+		blk_add_trace_generic },
+	{ "blk_requeue", "%p %p", BLK_TA_REQUEUE, blk_add_trace_rq },
+	{ "blk_request_issue", "%p %p", BLK_TA_ISSUE, blk_add_trace_rq },
+	{ "blk_request_complete", "%p %p", BLK_TA_COMPLETE, blk_add_trace_rq },
+	{ "blk_plug_device", "%p %p %d", BLK_TA_PLUG, blk_add_trace_generic },
+	{ "blk_pdu_unplug_io", "%p %p %d", BLK_TA_UNPLUG_IO,
+		blk_add_trace_pdu_int },
+	{ "blk_pdu_unplug_timer", "%p %p %d", BLK_TA_UNPLUG_TIMER,
+		blk_add_trace_pdu_int },
+	{ "blk_request_insert", "%p %p", BLK_TA_INSERT,
+		blk_add_trace_rq },
+	{ "blk_pdu_split", "%p %p %llu", BLK_TA_SPLIT,
+		blk_add_trace_pdu_ll },
+	{ "blk_bio_bounce", "%p %p", BLK_TA_BOUNCE, blk_add_trace_bio },
+	{ "blk_remap", "%p %p %llu %llu %llu", BLK_TA_REMAP,
+		blk_add_trace_remap },
+};
+
+
+int blk_probe_arm(void)
+{
+	int result;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		result = marker_probe_register(probe_array[i].name,
+				probe_array[i].format,
+				probe_array[i].callback, &probe_array[i]);
+		if (result)
+			printk(KERN_INFO
+				"blktrace unable to register probe %s\n",
+				probe_array[i].name);
+		result = marker_arm(probe_array[i].name);
+		if (result)
+			printk(KERN_INFO
+				"blktrace unable to arm probe %s\n",
+				probe_array[i].name);
+	}
+	return 0;
+}
+
+void blk_probe_disarm(void)
+{
+	int i, err;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		err = marker_disarm(probe_array[i].name);
+		WARN_ON(err);
+		err = IS_ERR(marker_probe_unregister(probe_array[i].name));
+		WARN_ON(err);
+	}
+}
+
+
 static __init int blk_trace_init(void)
 {
-	mutex_init(&blk_tree_mutex);
 	on_each_cpu(blk_trace_check_cpu_time, NULL, 1, 1);
 	blk_trace_set_ht_offsets();
 
diff -uprN linux-2.6.23.1.orig/block/elevator.c linux-2.6.23.1/block/elevator.c
--- linux-2.6.23.1.orig/block/elevator.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/block/elevator.c	2007-11-28 10:13:52.312359220 +0100
@@ -32,7 +32,7 @@
 #include <linux/init.h>
 #include <linux/compiler.h>
 #include <linux/delay.h>
-#include <linux/blktrace_api.h>
+#include <linux/marker.h>
 #include <linux/hash.h>
 
 #include <asm/uaccess.h>
@@ -548,7 +548,7 @@ void elv_insert(struct request_queue *q,
 	unsigned ordseq;
 	int unplug_it = 1;
 
-	blk_add_trace_rq(q, rq, BLK_TA_INSERT);
+	trace_mark(blk_request_insert, "%p %p", q, rq);
 
 	rq->q = q;
 
@@ -727,7 +727,7 @@ struct request *elv_next_request(struct 
 			 * not be passed by new incoming requests
 			 */
 			rq->cmd_flags |= REQ_STARTED;
-			blk_add_trace_rq(q, rq, BLK_TA_ISSUE);
+			trace_mark(blk_request_issue, "%p %p", q, rq);
 		}
 
 		if (!q->boundary_rq || q->boundary_rq == rq) {
diff -uprN linux-2.6.23.1.orig/block/ll_rw_blk.c linux-2.6.23.1/block/ll_rw_blk.c
--- linux-2.6.23.1.orig/block/ll_rw_blk.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/block/ll_rw_blk.c	2007-11-28 10:13:52.315358595 +0100
@@ -28,6 +28,7 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/interrupt.h>
 #include <linux/cpu.h>
+#include <linux/marker.h>
 #include <linux/blktrace_api.h>
 #include <linux/fault-inject.h>
 
@@ -1566,7 +1567,7 @@ void blk_plug_device(struct request_queu
 
 	if (!test_and_set_bit(QUEUE_FLAG_PLUGGED, &q->queue_flags)) {
 		mod_timer(&q->unplug_timer, jiffies + q->unplug_delay);
-		blk_add_trace_generic(q, NULL, 0, BLK_TA_PLUG);
+		trace_mark(blk_plug_device, "%p %p %d", q, NULL, 0);
 	}
 }
 
@@ -1632,7 +1633,7 @@ static void blk_backing_dev_unplug(struc
 	 * devices don't necessarily have an ->unplug_fn defined
 	 */
 	if (q->unplug_fn) {
-		blk_add_trace_pdu_int(q, BLK_TA_UNPLUG_IO, NULL,
+		trace_mark(blk_pdu_unplug_io, "%p %p %d", q, NULL,
 					q->rq.count[READ] + q->rq.count[WRITE]);
 
 		q->unplug_fn(q);
@@ -1644,7 +1645,7 @@ static void blk_unplug_work(struct work_
 	struct request_queue *q =
 		container_of(work, struct request_queue, unplug_work);
 
-	blk_add_trace_pdu_int(q, BLK_TA_UNPLUG_IO, NULL,
+	trace_mark(blk_pdu_unplug_io, "%p %p %d", q, NULL,
 				q->rq.count[READ] + q->rq.count[WRITE]);
 
 	q->unplug_fn(q);
@@ -1654,7 +1655,7 @@ static void blk_unplug_timeout(unsigned 
 {
 	struct request_queue *q = (struct request_queue *)data;
 
-	blk_add_trace_pdu_int(q, BLK_TA_UNPLUG_TIMER, NULL,
+	trace_mark(blk_pdu_unplug_timer, "%p %p %d", q, NULL,
 				q->rq.count[READ] + q->rq.count[WRITE]);
 
 	kblockd_schedule_work(&q->unplug_work);
@@ -2167,7 +2168,7 @@ rq_starved:
 	
 	rq_init(q, rq);
 
-	blk_add_trace_generic(q, bio, rw, BLK_TA_GETRQ);
+	trace_mark(blk_get_request, "%p %p %d", q, bio, rw);
 out:
 	return rq;
 }
@@ -2197,7 +2198,7 @@ static struct request *get_request_wait(
 		if (!rq) {
 			struct io_context *ioc;
 
-			blk_add_trace_generic(q, bio, rw, BLK_TA_SLEEPRQ);
+			trace_mark(blk_sleep_request, "%p %p %d", q, bio, rw);
 
 			__generic_unplug_device(q);
 			spin_unlock_irq(q->queue_lock);
@@ -2271,7 +2272,7 @@ EXPORT_SYMBOL(blk_start_queueing);
  */
 void blk_requeue_request(struct request_queue *q, struct request *rq)
 {
-	blk_add_trace_rq(q, rq, BLK_TA_REQUEUE);
+	trace_mark(blk_requeue, "%p %p", q, rq);
 
 	if (blk_rq_tagged(rq))
 		blk_queue_end_tag(q, rq);
@@ -2959,7 +2960,7 @@ static int __make_request(struct request
 			if (!ll_back_merge_fn(q, req, bio))
 				break;
 
-			blk_add_trace_bio(q, bio, BLK_TA_BACKMERGE);
+			trace_mark(blk_bio_backmerge, "%p %p", q, bio);
 
 			req->biotail->bi_next = bio;
 			req->biotail = bio;
@@ -2976,7 +2977,7 @@ static int __make_request(struct request
 			if (!ll_front_merge_fn(q, req, bio))
 				break;
 
-			blk_add_trace_bio(q, bio, BLK_TA_FRONTMERGE);
+			trace_mark(blk_bio_frontmerge, "%p %p", q, bio);
 
 			bio->bi_next = req->bio;
 			req->bio = bio;
@@ -3059,9 +3060,10 @@ static inline void blk_partition_remap(s
 		bio->bi_sector += p->start_sect;
 		bio->bi_bdev = bdev->bd_contains;
 
-		blk_add_trace_remap(bdev_get_queue(bio->bi_bdev), bio,
-				    bdev->bd_dev, bio->bi_sector,
-				    bio->bi_sector - p->start_sect);
+		trace_mark(blk_remap, "%p %p %llu %llu %llu",
+				    bdev_get_queue(bio->bi_bdev), bio,
+				    (u64)bdev->bd_dev, (u64)bio->bi_sector,
+				    (u64)bio->bi_sector - p->start_sect);
 	}
 }
 
@@ -3210,10 +3212,11 @@ end_io:
 		blk_partition_remap(bio);
 
 		if (old_sector != -1)
-			blk_add_trace_remap(q, bio, old_dev, bio->bi_sector, 
-					    old_sector);
+			trace_mark(blk_remap, "%p %p %llu %llu %llu",
+				q, bio, (u64)old_dev,
+				(u64)bio->bi_sector, (u64)old_sector);
 
-		blk_add_trace_bio(q, bio, BLK_TA_QUEUE);
+		trace_mark(blk_bio_queue, "%p %p", q, bio);
 
 		old_sector = bio->bi_sector;
 		old_dev = bio->bi_bdev->bd_dev;
@@ -3406,7 +3409,7 @@ static int __end_that_request_first(stru
 	int total_bytes, bio_nbytes, error, next_idx = 0;
 	struct bio *bio;
 
-	blk_add_trace_rq(req->q, req, BLK_TA_COMPLETE);
+	trace_mark(blk_request_complete, "%p %p", req->q, req);
 
 	/*
 	 * extend uptodate bool to allow < 0 value to be direct io error
diff -uprN linux-2.6.23.1.orig/drivers/block/cciss.c linux-2.6.23.1/drivers/block/cciss.c
--- linux-2.6.23.1.orig/drivers/block/cciss.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/drivers/block/cciss.c	2007-11-28 10:13:52.317358178 +0100
@@ -37,7 +37,7 @@
 #include <linux/hdreg.h>
 #include <linux/spinlock.h>
 #include <linux/compat.h>
-#include <linux/blktrace_api.h>
+#include <linux/marker.h>
 #include <asm/uaccess.h>
 #include <asm/io.h>
 
@@ -2504,7 +2504,7 @@ after_error_processing:
 	}
 	cmd->rq->data_len = 0;
 	cmd->rq->completion_data = cmd;
-	blk_add_trace_rq(cmd->rq->q, cmd->rq, BLK_TA_COMPLETE);
+	trace_mark(blk_request_complete, "%p %p", cmd->rq->q, cmd->rq);
 	blk_complete_request(cmd->rq);
 }
 
diff -uprN linux-2.6.23.1.orig/drivers/input/input.c linux-2.6.23.1/drivers/input/input.c
--- linux-2.6.23.1.orig/drivers/input/input.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/drivers/input/input.c	2007-11-28 10:13:52.318357969 +0100
@@ -52,6 +52,8 @@ void input_event(struct input_dev *dev, 
 
 	add_input_randomness(type, code, value);
 
+	trace_mark(input_event, "type %u code %u value %d", type, code, value);
+
 	switch (type) {
 
 		case EV_SYN:
diff -uprN linux-2.6.23.1.orig/drivers/md/dm.c linux-2.6.23.1/drivers/md/dm.c
--- linux-2.6.23.1.orig/drivers/md/dm.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/drivers/md/dm.c	2007-11-28 10:13:52.319357761 +0100
@@ -19,7 +19,7 @@
 #include <linux/slab.h>
 #include <linux/idr.h>
 #include <linux/hdreg.h>
-#include <linux/blktrace_api.h>
+#include <linux/marker.h>
 #include <linux/smp_lock.h>
 
 #define DM_MSG_PREFIX "core"
@@ -481,8 +481,8 @@ static void dec_pending(struct dm_io *io
 			wake_up(&io->md->wait);
 
 		if (io->error != DM_ENDIO_REQUEUE) {
-			blk_add_trace_bio(io->md->queue, io->bio,
-					  BLK_TA_COMPLETE);
+			trace_mark(blk_request_complete, "%p %p",
+				io->md->queue, io->bio);
 
 			bio_endio(io->bio, io->bio->bi_size, io->error);
 		}
@@ -578,10 +578,10 @@ static void __map_bio(struct dm_target *
 	r = ti->type->map(ti, clone, &tio->info);
 	if (r == DM_MAPIO_REMAPPED) {
 		/* the bio has been remapped so dispatch it */
-
-		blk_add_trace_remap(bdev_get_queue(clone->bi_bdev), clone,
-				    tio->io->bio->bi_bdev->bd_dev,
-				    clone->bi_sector, sector);
+		trace_mark(blk_remap, "%p %p %llu %llu %llu",
+			bdev_get_queue(clone->bi_bdev), clone,
+			(u64)tio->io->bio->bi_bdev->bd_dev, (u64)sector,
+			(u64)clone->bi_sector);
 
 		generic_make_request(clone);
 	} else if (r < 0 || r == DM_MAPIO_REQUEUE) {
diff -uprN linux-2.6.23.1.orig/drivers/net/bonding/bond_3ad.c linux-2.6.23.1/drivers/net/bonding/bond_3ad.c
--- linux-2.6.23.1.orig/drivers/net/bonding/bond_3ad.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/drivers/net/bonding/bond_3ad.c	2007-11-28 10:13:52.320357552 +0100
@@ -126,7 +126,7 @@ static struct aggregator *__get_active_a
 
 // ================= main 802.3ad protocol functions ==================
 static int ad_lacpdu_send(struct port *port);
-static int ad_marker_send(struct port *port, struct marker *marker);
+static int ad_marker_send(struct port *port, struct bond_marker *marker);
 static void ad_mux_machine(struct port *port);
 static void ad_rx_machine(struct lacpdu *lacpdu, struct port *port);
 static void ad_tx_machine(struct port *port);
@@ -139,8 +139,8 @@ static void ad_initialize_port(struct po
 static void ad_initialize_lacpdu(struct lacpdu *Lacpdu);
 static void ad_enable_collecting_distributing(struct port *port);
 static void ad_disable_collecting_distributing(struct port *port);
-static void ad_marker_info_received(struct marker *marker_info, struct port *port);
-static void ad_marker_response_received(struct marker *marker, struct port *port);
+static void ad_marker_info_received(struct bond_marker *marker_info, struct port *port);
+static void ad_marker_response_received(struct bond_marker *marker, struct port *port);
 
 
 /////////////////////////////////////////////////////////////////////////////////
@@ -912,12 +912,12 @@ static int ad_lacpdu_send(struct port *p
  * Returns:   0 on success
  *          < 0 on error
  */
-static int ad_marker_send(struct port *port, struct marker *marker)
+static int ad_marker_send(struct port *port, struct bond_marker *marker)
 {
 	struct slave *slave = port->slave;
 	struct sk_buff *skb;
-	struct marker_header *marker_header;
-	int length = sizeof(struct marker_header);
+	struct bond_marker_header *marker_header;
+	int length = sizeof(struct bond_marker_header);
 	struct mac_addr lacpdu_multicast_address = AD_MULTICAST_LACPDU_ADDR;
 
 	skb = dev_alloc_skb(length + 16);
@@ -932,7 +932,7 @@ static int ad_marker_send(struct port *p
 	skb->network_header = skb->mac_header + ETH_HLEN;
 	skb->protocol = PKT_TYPE_LACPDU;
 
-	marker_header = (struct marker_header *)skb_put(skb, length);
+	marker_header = (struct bond_marker_header *)skb_put(skb, length);
 
 	marker_header->ad_header.destination_address = lacpdu_multicast_address;
 	/* Note: source addres is set to be the member's PERMANENT address, because we use it
@@ -1732,7 +1732,7 @@ static void ad_disable_collecting_distri
  */
 static void ad_marker_info_send(struct port *port)
 {
-	struct marker marker;
+	struct bond_marker marker;
 	u16 index;
 
 	// fill the marker PDU with the appropriate values
@@ -1765,13 +1765,14 @@ static void ad_marker_info_send(struct p
  * @port: the port we're looking at
  *
  */
-static void ad_marker_info_received(struct marker *marker_info,struct port *port)
+static void ad_marker_info_received(struct bond_marker *marker_info,
+	struct port *port)
 {
-	struct marker marker;
+	struct bond_marker marker;
 
 	// copy the received marker data to the response marker
 	//marker = *marker_info;
-	memcpy(&marker, marker_info, sizeof(struct marker));
+	memcpy(&marker, marker_info, sizeof(struct bond_marker));
 	// change the marker subtype to marker response
 	marker.tlv_type=AD_MARKER_RESPONSE_SUBTYPE;
 	// send the marker response
@@ -1790,7 +1791,8 @@ static void ad_marker_info_received(stru
  * response for marker PDU's, in this stage, but only to respond to marker
  * information.
  */
-static void ad_marker_response_received(struct marker *marker, struct port *port)
+static void ad_marker_response_received(struct bond_marker *marker,
+	struct port *port)
 {
 	marker=NULL; // just to satisfy the compiler
 	port=NULL;  // just to satisfy the compiler
@@ -2187,15 +2189,15 @@ static void bond_3ad_rx_indication(struc
 		case AD_TYPE_MARKER:
 			// No need to convert fields to Little Endian since we don't use the marker's fields.
 
-			switch (((struct marker *)lacpdu)->tlv_type) {
+			switch (((struct bond_marker *)lacpdu)->tlv_type) {
 			case AD_MARKER_INFORMATION_SUBTYPE:
 				dprintk("Received Marker Information on port %d\n", port->actor_port_number);
-				ad_marker_info_received((struct marker *)lacpdu, port);
+				ad_marker_info_received((struct bond_marker *)lacpdu, port);
 				break;
 
 			case AD_MARKER_RESPONSE_SUBTYPE:
 				dprintk("Received Marker Response on port %d\n", port->actor_port_number);
-				ad_marker_response_received((struct marker *)lacpdu, port);
+				ad_marker_response_received((struct bond_marker *)lacpdu, port);
 				break;
 
 			default:
diff -uprN linux-2.6.23.1.orig/drivers/net/bonding/bond_3ad.h linux-2.6.23.1/drivers/net/bonding/bond_3ad.h
--- linux-2.6.23.1.orig/drivers/net/bonding/bond_3ad.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/drivers/net/bonding/bond_3ad.h	2007-11-28 10:13:52.321357344 +0100
@@ -92,7 +92,7 @@ typedef enum {
 typedef enum {
 	AD_MARKER_INFORMATION_SUBTYPE = 1, // marker imformation subtype
 	AD_MARKER_RESPONSE_SUBTYPE     // marker response subtype
-} marker_subtype_t;
+} bond_marker_subtype_t;
 
 // timers types(43.4.9 in the 802.3ad standard)
 typedef enum {
@@ -148,7 +148,7 @@ typedef struct lacpdu_header {
 } lacpdu_header_t;
 
 // Marker Protocol Data Unit(PDU) structure(43.5.3.2 in the 802.3ad standard)
-typedef struct marker {
+typedef struct bond_marker {
 	u8 subtype;		 //  = 0x02  (marker PDU)
 	u8 version_number;	 //  = 0x01
 	u8 tlv_type;		 //  = 0x01  (marker information)
@@ -161,12 +161,12 @@ typedef struct marker {
 	u8 tlv_type_terminator;	     //  = 0x00
 	u8 terminator_length;	     //  = 0x00
 	u8 reserved_90[90];	     //  = 0
-} marker_t;
+} bond_marker_t;
 
-typedef struct marker_header {
+typedef struct bond_marker_header {
 	struct ad_header ad_header;
-	struct marker marker;
-} marker_header_t;
+	struct bond_marker marker;
+} bond_marker_header_t;
 
 #pragma pack()
 
diff -uprN linux-2.6.23.1.orig/drivers/scsi/qla4xxx/ql4_fw.h linux-2.6.23.1/drivers/scsi/qla4xxx/ql4_fw.h
--- linux-2.6.23.1.orig/drivers/scsi/qla4xxx/ql4_fw.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/drivers/scsi/qla4xxx/ql4_fw.h	2007-11-28 10:13:52.322357136 +0100
@@ -671,7 +671,7 @@ struct continuation_t1_entry {
 #define ET_CONTINUE	ET_CONT_T1
 
 /* Marker entry structure*/
-struct marker_entry {
+struct qla4_marker_entry {
 	struct qla4_header hdr;	/* 00-03 */
 
 	uint32_t system_defined; /* 04-07 */
diff -uprN linux-2.6.23.1.orig/drivers/scsi/qla4xxx/ql4_iocb.c linux-2.6.23.1/drivers/scsi/qla4xxx/ql4_iocb.c
--- linux-2.6.23.1.orig/drivers/scsi/qla4xxx/ql4_iocb.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/drivers/scsi/qla4xxx/ql4_iocb.c	2007-11-28 10:13:52.322357136 +0100
@@ -69,7 +69,7 @@ static int qla4xxx_get_req_pkt(struct sc
 static int qla4xxx_send_marker_iocb(struct scsi_qla_host *ha,
 				    struct ddb_entry *ddb_entry, int lun)
 {
-	struct marker_entry *marker_entry;
+	struct qla4_marker_entry *marker_entry;
 	unsigned long flags = 0;
 	uint8_t status = QLA_SUCCESS;
 
diff -uprN linux-2.6.23.1.orig/fs/bio.c linux-2.6.23.1/fs/bio.c
--- linux-2.6.23.1.orig/fs/bio.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/bio.c	2007-11-28 10:13:52.323356927 +0100
@@ -25,7 +25,7 @@
 #include <linux/module.h>
 #include <linux/mempool.h>
 #include <linux/workqueue.h>
-#include <linux/blktrace_api.h>
+#include <linux/marker.h>
 #include <scsi/sg.h>		/* for struct sg_iovec */
 
 #define BIO_POOL_SIZE 2
@@ -1081,8 +1081,8 @@ struct bio_pair *bio_split(struct bio *b
 	if (!bp)
 		return bp;
 
-	blk_add_trace_pdu_int(bdev_get_queue(bi->bi_bdev), BLK_TA_SPLIT, bi,
-				bi->bi_sector + first_sectors);
+	trace_mark(blk_pdu_split, "%p %p %llu", bdev_get_queue(bi->bi_bdev), bi,
+				(u64)bi->bi_sector + first_sectors);
 
 	BUG_ON(bi->bi_vcnt != 1);
 	BUG_ON(bi->bi_idx != 0);
diff -uprN linux-2.6.23.1.orig/fs/buffer.c linux-2.6.23.1/fs/buffer.c
--- linux-2.6.23.1.orig/fs/buffer.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/buffer.c	2007-11-28 10:13:52.325356510 +0100
@@ -89,7 +89,9 @@ void fastcall unlock_buffer(struct buffe
  */
 void __wait_on_buffer(struct buffer_head * bh)
 {
+	trace_mark(fs_buffer_wait_start, "bh %p", bh);
 	wait_on_bit(&bh->b_state, BH_Lock, sync_buffer, TASK_UNINTERRUPTIBLE);
+	trace_mark(fs_buffer_wait_end, "bh %p", bh);
 }
 
 static void
diff -uprN linux-2.6.23.1.orig/fs/compat.c linux-2.6.23.1/fs/compat.c
--- linux-2.6.23.1.orig/fs/compat.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/compat.c	2007-11-28 10:13:52.326356302 +0100
@@ -1408,6 +1408,7 @@ int compat_do_execve(char * filename,
 
 	retval = search_binary_handler(bprm, regs);
 	if (retval >= 0) {
+		trace_mark(fs_exec, "filename %s", filename);
 		/* execve success */
 		security_bprm_free(bprm);
 		acct_update_integrals(current);
diff -uprN linux-2.6.23.1.orig/fs/exec.c linux-2.6.23.1/fs/exec.c
--- linux-2.6.23.1.orig/fs/exec.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/exec.c	2007-11-28 10:13:52.327356093 +0100
@@ -1397,6 +1397,7 @@ int do_execve(char * filename,
 
 	retval = search_binary_handler(bprm,regs);
 	if (retval >= 0) {
+		trace_mark(fs_exec, "filename %s", filename);
 		/* execve success */
 		free_arg_pages(bprm);
 		security_bprm_free(bprm);
diff -uprN linux-2.6.23.1.orig/fs/ioctl.c linux-2.6.23.1/fs/ioctl.c
--- linux-2.6.23.1.orig/fs/ioctl.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/ioctl.c	2007-11-28 10:13:52.328355885 +0100
@@ -164,6 +164,8 @@ asmlinkage long sys_ioctl(unsigned int f
 	if (!filp)
 		goto out;
 
+	trace_mark(fs_ioctl, "fd %u cmd %u arg %lu", fd, cmd, arg);
+
 	error = security_file_ioctl(filp, cmd, arg);
 	if (error)
 		goto out_fput;
diff -uprN linux-2.6.23.1.orig/fs/open.c linux-2.6.23.1/fs/open.c
--- linux-2.6.23.1.orig/fs/open.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/open.c	2007-11-28 10:13:52.329355677 +0100
@@ -1038,6 +1038,7 @@ long do_sys_open(int dfd, const char __u
 				fsnotify_open(f->f_path.dentry);
 				fd_install(fd, f);
 			}
+			trace_mark(fs_open, "fd %d filename %s", fd, tmp);
 		}
 		putname(tmp);
 	}
@@ -1128,6 +1129,7 @@ asmlinkage long sys_close(unsigned int f
 	filp = fdt->fd[fd];
 	if (!filp)
 		goto out_unlock;
+	trace_mark(fs_close, "fd %u", fd);
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	FD_CLR(fd, fdt->close_on_exec);
 	__put_unused_fd(files, fd);
diff -uprN linux-2.6.23.1.orig/fs/read_write.c linux-2.6.23.1/fs/read_write.c
--- linux-2.6.23.1.orig/fs/read_write.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/read_write.c	2007-11-28 10:13:52.329355677 +0100
@@ -146,6 +146,9 @@ asmlinkage off_t sys_lseek(unsigned int 
 		if (res != (loff_t)retval)
 			retval = -EOVERFLOW;	/* LFS: should only happen on 32 bit platforms */
 	}
+
+	trace_mark(fs_lseek, "fd %u offset %ld origin %u", fd, offset, origin);
+
 	fput_light(file, fput_needed);
 bad:
 	return retval;
@@ -173,6 +176,9 @@ asmlinkage long sys_llseek(unsigned int 
 	offset = vfs_llseek(file, ((loff_t) offset_high << 32) | offset_low,
 			origin);
 
+	trace_mark(fs_llseek, "fd %u offset %lld origin %u", fd, offset,
+			origin);
+
 	retval = (int)offset;
 	if (offset >= 0) {
 		retval = -EFAULT;
@@ -363,6 +369,7 @@ asmlinkage ssize_t sys_read(unsigned int
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		trace_mark(fs_read, "fd %u count %zu", fd, count);
 		ret = vfs_read(file, buf, count, &pos);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
@@ -381,6 +388,7 @@ asmlinkage ssize_t sys_write(unsigned in
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		trace_mark(fs_write, "fd %u count %zu", fd, count);
 		ret = vfs_write(file, buf, count, &pos);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
@@ -402,8 +410,12 @@ asmlinkage ssize_t sys_pread64(unsigned 
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		ret = -ESPIPE;
-		if (file->f_mode & FMODE_PREAD)
+		if (file->f_mode & FMODE_PREAD) {
+			trace_mark(fs_pread64, "fd %u count %zu pos %lld",
+				fd, count, pos);
 			ret = vfs_read(file, buf, count, &pos);
+		}
+
 		fput_light(file, fput_needed);
 	}
 
@@ -423,8 +435,11 @@ asmlinkage ssize_t sys_pwrite64(unsigned
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		ret = -ESPIPE;
-		if (file->f_mode & FMODE_PWRITE)  
+		if (file->f_mode & FMODE_PWRITE) {
+			trace_mark(fs_pwrite64, "fd %u count %zu pos %lld",
+				fd, count, pos);
 			ret = vfs_write(file, buf, count, &pos);
+		}
 		fput_light(file, fput_needed);
 	}
 
@@ -670,6 +685,7 @@ sys_readv(unsigned long fd, const struct
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		trace_mark(fs_readv, "fd %lu vlen %lu", fd, vlen);
 		ret = vfs_readv(file, vec, vlen, &pos);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
@@ -691,6 +707,7 @@ sys_writev(unsigned long fd, const struc
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		trace_mark(fs_writev, "fd %lu vlen %lu", fd, vlen);
 		ret = vfs_writev(file, vec, vlen, &pos);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
diff -uprN linux-2.6.23.1.orig/fs/select.c linux-2.6.23.1/fs/select.c
--- linux-2.6.23.1.orig/fs/select.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/select.c	2007-11-28 10:13:52.330355468 +0100
@@ -236,6 +236,9 @@ int do_select(int n, fd_set_bits *fds, s
 				file = fget_light(i, &fput_needed);
 				if (file) {
 					f_op = file->f_op;
+					trace_mark(fs_select,
+							"fd %d timeout #8d%llu",
+							i, *timeout);
 					mask = DEFAULT_POLLMASK;
 					if (f_op && f_op->poll)
 						mask = (*f_op->poll)(file, retval ? NULL : wait);
@@ -564,6 +567,7 @@ static inline unsigned int do_pollfd(str
 		file = fget_light(fd, &fput_needed);
 		mask = POLLNVAL;
 		if (file != NULL) {
+			trace_mark(fs_pollfd, "fd %d", fd);
 			mask = DEFAULT_POLLMASK;
 			if (file->f_op && file->f_op->poll)
 				mask = file->f_op->poll(file, pwait);
diff -uprN linux-2.6.23.1.orig/fs/seq_file.c linux-2.6.23.1/fs/seq_file.c
--- linux-2.6.23.1.orig/fs/seq_file.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/fs/seq_file.c	2007-11-28 10:13:52.331355260 +0100
@@ -483,5 +483,48 @@ struct list_head *seq_list_next(void *v,
 	++*ppos;
 	return lh == head ? NULL : lh;
 }
-
 EXPORT_SYMBOL(seq_list_next);
+
+struct list_head *seq_sorted_list_start(struct list_head *head, loff_t *ppos)
+{
+	struct list_head *lh;
+
+	list_for_each(lh, head)
+		if ((unsigned long)lh >= *ppos) {
+			*ppos = (unsigned long)lh;
+			return lh;
+		}
+	return NULL;
+}
+EXPORT_SYMBOL(seq_sorted_list_start);
+
+struct list_head *seq_sorted_list_start_head(struct list_head *head,
+		loff_t *ppos)
+{
+	struct list_head *lh;
+
+	if (!*ppos) {
+		*ppos = (unsigned long)head;
+		return head;
+	}
+	list_for_each(lh, head)
+		if ((unsigned long)lh >= *ppos) {
+			*ppos = (long)lh->prev;
+			return lh->prev;
+		}
+	return NULL;
+}
+EXPORT_SYMBOL(seq_sorted_list_start_head);
+
+struct list_head *seq_sorted_list_next(void *p, struct list_head *head,
+		loff_t *ppos)
+{
+	struct list_head *lh;
+	void *next;
+
+	lh = ((struct list_head *)p)->next;
+	next = (lh == head) ? NULL : lh;
+	*ppos = next ? (unsigned long)next : -1UL;
+	return next;
+}
+EXPORT_SYMBOL(seq_sorted_list_next);
diff -uprN linux-2.6.23.1.orig/include/asm-generic/cmpxchg-local.h linux-2.6.23.1/include/asm-generic/cmpxchg-local.h
--- linux-2.6.23.1.orig/include/asm-generic/cmpxchg-local.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/asm-generic/cmpxchg-local.h	2007-11-28 10:13:52.331355260 +0100
@@ -0,0 +1,65 @@
+#ifndef __ASM_GENERIC_CMPXCHG_LOCAL_H
+#define __ASM_GENERIC_CMPXCHG_LOCAL_H
+
+#include <linux/types.h>
+
+extern unsigned long wrong_size_cmpxchg(volatile void *ptr);
+
+/*
+ * Generic version of __cmpxchg_local (disables interrupts). Takes an unsigned
+ * long parameter, supporting various types of architectures.
+ */
+static inline unsigned long __cmpxchg_local_generic(volatile void *ptr,
+		unsigned long old, unsigned long new, int size)
+{
+	unsigned long flags, prev;
+
+	/*
+	 * Sanity checking, compile-time.
+	 */
+	if (size == 8 && sizeof(unsigned long) != 8)
+		wrong_size_cmpxchg(ptr);
+
+	local_irq_save(flags);
+	switch (size) {
+	case 1: prev = *(u8 *)ptr;
+		if (prev == old)
+			*(u8 *)ptr = (u8)new;
+		break;
+	case 2: prev = *(u16 *)ptr;
+		if (prev == old)
+			*(u16 *)ptr = (u16)new;
+		break;
+	case 4: prev = *(u32 *)ptr;
+		if (prev == old)
+			*(u32 *)ptr = (u32)new;
+		break;
+	case 8: prev = *(u64 *)ptr;
+		if (prev == old)
+			*(u64 *)ptr = (u64)new;
+		break;
+	default:
+		wrong_size_cmpxchg(ptr);
+	}
+	local_irq_restore(flags);
+	return prev;
+}
+
+/*
+ * Generic version of __cmpxchg64_local. Takes an u64 parameter.
+ */
+static inline u64 __cmpxchg64_local_generic(volatile void *ptr,
+		u64 old, u64 new)
+{
+	u64 prev;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	prev = *(u64 *)ptr;
+	if (prev == old)
+		*(u64 *)ptr = new;
+	local_irq_restore(flags);
+	return prev;
+}
+
+#endif
diff -uprN linux-2.6.23.1.orig/include/asm-generic/cmpxchg.h linux-2.6.23.1/include/asm-generic/cmpxchg.h
--- linux-2.6.23.1.orig/include/asm-generic/cmpxchg.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/asm-generic/cmpxchg.h	2007-11-28 10:13:52.331355260 +0100
@@ -0,0 +1,22 @@
+#ifndef __ASM_GENERIC_CMPXCHG_H
+#define __ASM_GENERIC_CMPXCHG_H
+
+/*
+ * Generic cmpxchg
+ *
+ * Uses the local cmpxchg. Does not support SMP.
+ */
+#ifdef CONFIG_SMP
+#error "Cannot use generic cmpxchg on SMP"
+#endif
+
+/*
+ * Atomic compare and exchange.
+ *
+ * Do not define __HAVE_ARCH_CMPXCHG because we want to use it to check whether
+ * a cmpxchg primitive faster than repeated local irq save/restore exists.
+ */
+#define cmpxchg(ptr, o, n)	cmpxchg_local((ptr), (o), (n))
+#define cmpxchg64(ptr, o, n)	cmpxchg64_local((ptr), (o), (n))
+
+#endif
diff -uprN linux-2.6.23.1.orig/include/asm-generic/ltt.h linux-2.6.23.1/include/asm-generic/ltt.h
--- linux-2.6.23.1.orig/include/asm-generic/ltt.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/asm-generic/ltt.h	2007-11-28 10:13:52.332355051 +0100
@@ -0,0 +1,53 @@
+#ifndef _ASM_GENERIC_LTT_H
+#define _ASM_GENERIC_LTT_H
+
+/*
+ * linux/include/asm-generic/ltt.h
+ *
+ * Copyright (C) 2007 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Generic definitions for LTT
+ * Architectures without TSC
+ */
+
+#include <linux/param.h>	/* For HZ */
+#include <asm/atomic.h>
+
+#define LTT_GENERIC_CLOCK_SHIFT 13
+
+u64 ltt_read_synthetic_tsc(void);
+
+extern atomic_t lttng_generic_clock;
+
+static inline u32 ltt_get_timestamp32_generic(void)
+{
+	return atomic_add_return(1, &lttng_generic_clock);
+}
+
+static inline u64 ltt_get_timestamp64_generic(void)
+{
+	return ltt_read_synthetic_tsc();
+}
+
+static inline void ltt_add_timestamp_generic(unsigned long ticks)
+{
+	int old_clock, new_clock;
+
+	do {
+		old_clock = atomic_read(&lttng_generic_clock);
+		new_clock = (old_clock + (ticks << LTT_GENERIC_CLOCK_SHIFT))
+			& (~((1 << LTT_GENERIC_CLOCK_SHIFT) - 1));
+	} while (atomic_cmpxchg(&lttng_generic_clock, old_clock, new_clock)
+			!= old_clock);
+}
+
+static inline unsigned int ltt_frequency_generic(void)
+{
+	return HZ << LTT_GENERIC_CLOCK_SHIFT;
+}
+
+static inline u32 ltt_freq_scale_generic(void)
+{
+	return 1;
+}
+#endif /* _ASM_GENERIC_LTT_H */
diff -uprN linux-2.6.23.1.orig/include/asm-generic/vmlinux.lds.h linux-2.6.23.1/include/asm-generic/vmlinux.lds.h
--- linux-2.6.23.1.orig/include/asm-generic/vmlinux.lds.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/asm-generic/vmlinux.lds.h	2007-11-28 10:13:52.332355051 +0100
@@ -12,7 +12,11 @@
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
-	*(.data.init.refok)
+	*(.data.init.refok)						\
+	. = ALIGN(8);							\
+	VMLINUX_SYMBOL(__start___markers) = .;				\
+	*(__markers)							\
+	VMLINUX_SYMBOL(__stop___markers) = .;
 
 #define RO_DATA(align)							\
 	. = ALIGN((align));						\
@@ -20,6 +24,11 @@
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
 		*(__vermagic)		/* Kernel version magic */	\
+		*(__markers_strings)	/* Markers: strings */		\
+		. = ALIGN(8);						\
+		VMLINUX_SYMBOL(__start___immediate) = .;		\
+		*(__immediate)		/* Immediate values: pointers */ \
+		VMLINUX_SYMBOL(__stop___immediate) = .;			\
 	}								\
 									\
 	.rodata1          : AT(ADDR(.rodata1) - LOAD_OFFSET) {		\
diff -uprN linux-2.6.23.1.orig/include/asm-sh/ltt.h linux-2.6.23.1/include/asm-sh/ltt.h
--- linux-2.6.23.1.orig/include/asm-sh/ltt.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/asm-sh/ltt.h	2007-11-28 10:13:52.332355051 +0100
@@ -0,0 +1,48 @@
+/*
+ * Copyright (C) 2007, Giuseppe Cavallaro <peppe.cavallaro@st.com>
+ *                     Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * SuperH definitions for tracing system
+ */
+
+#ifndef _ASM_SH_LTT_H
+#define _ASM_SH_LTT_H
+
+#include <linux/ltt-core.h>
+#include <linux/timer.h>
+#include <asm/clock.h>
+
+#define LTT_HAS_TSC
+
+u64 ltt_read_synthetic_tsc(void);
+
+static inline u32 ltt_get_timestamp32(void)
+{
+	return get_cycles();
+}
+
+static inline u64 ltt_get_timestamp64(void)
+{
+	return ltt_read_synthetic_tsc();
+}
+
+static inline void ltt_add_timestamp(unsigned long ticks)
+{ }
+
+static inline unsigned int ltt_frequency(void)
+{
+	unsigned long rate;
+	struct clk *tmu1_clk;
+
+	tmu1_clk = clk_get(NULL, "tmu1_clk");
+	rate = (clk_get_rate(tmu1_clk));
+
+	return (unsigned int)(rate);
+}
+
+static inline u32 ltt_freq_scale(void)
+{
+	return 1;
+}
+
+#endif /* _ASM_SH_LTT_H */
diff -uprN linux-2.6.23.1.orig/include/asm-sh/system.h linux-2.6.23.1/include/asm-sh/system.h
--- linux-2.6.23.1.orig/include/asm-sh/system.h	2007-11-12 18:23:54.000000000 +0100
+++ linux-2.6.23.1/include/asm-sh/system.h	2007-11-28 10:13:52.333354843 +0100
@@ -249,6 +249,7 @@ extern void __xchg_called_with_bad_point
 #define xchg(ptr,x)	\
 	((__typeof__(*(ptr)))__xchg((ptr),(unsigned long)(x), sizeof(*(ptr))))
 
+#ifndef CONFIG_LTT
 #if defined(CONFIG_SH_GRB)
 static inline unsigned long __cmpxchg_u32(volatile u32 * m, unsigned long old,
         unsigned long new)
@@ -311,6 +312,20 @@ static inline unsigned long __cmpxchg(vo
      (__typeof__(*(ptr))) __cmpxchg((ptr), (unsigned long)_o_,		 \
 				    (unsigned long)_n_, sizeof(*(ptr))); \
   })
+#else
+#include <asm-generic/cmpxchg-local.h>
+/*
+* cmpxchg_local and cmpxchg64_local are atomic wrt current CPU. Always make
+* them available.
+*/
+#define cmpxchg_local(ptr,o,n) \
+	(__typeof__(*(ptr)))__cmpxchg_local_generic((ptr), (unsigned long)(o), \
+	(unsigned long)(n), sizeof(*(ptr)))
+#define cmpxchg64_local(ptr,o,n) __cmpxchg64_local_generic((ptr), (o), (n))
+#ifndef CONFIG_SMP
+#include <asm-generic/cmpxchg.h>
+#endif
+#endif /* CONFIG_LTT */
 
 extern void die(const char *str, struct pt_regs *regs, long err) __attribute__ ((noreturn));
 
diff -uprN linux-2.6.23.1.orig/include/asm-sh/thread_info.h linux-2.6.23.1/include/asm-sh/thread_info.h
--- linux-2.6.23.1.orig/include/asm-sh/thread_info.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/asm-sh/thread_info.h	2007-11-28 10:13:52.333354843 +0100
@@ -111,6 +111,7 @@ static inline struct thread_info *curren
 #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
 #define TIF_RESTORE_SIGMASK	3	/* restore signal mask in do_signal() */
 #define TIF_SINGLESTEP		4	/* singlestepping active */
+#define TIF_KERNEL_TRACE	5	/* kernel trace active */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_MEMDIE		18
@@ -121,11 +122,12 @@ static inline struct thread_info *curren
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_SINGLESTEP		(1<<TIF_SINGLESTEP)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 
-#define _TIF_WORK_MASK		0x000000FE	/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK		0x000000DE	/* work to do on interrupt/exception return */
 #define _TIF_ALLWORK_MASK	0x000000FF	/* work to do on any return to u-space */
 
 #endif /* __KERNEL__ */
diff -uprN linux-2.6.23.1.orig/include/asm-sh/timex.h linux-2.6.23.1/include/asm-sh/timex.h
--- linux-2.6.23.1.orig/include/asm-sh/timex.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/asm-sh/timex.h	2007-11-28 10:13:52.334354634 +0100
@@ -6,12 +6,18 @@
 #ifndef __ASM_SH_TIMEX_H
 #define __ASM_SH_TIMEX_H
 
-#define CLOCK_TICK_RATE		(CONFIG_SH_PCLK_FREQ / 4) /* Underlying HZ */
+#include <linux/io.h>
+#include <asm/cpu/timer.h>
+
+#define CLOCK_TICK_RATE		(HZ * 100000UL)
 
 typedef unsigned long long cycles_t;
 
 static __inline__ cycles_t get_cycles (void)
 {
+#ifdef CONFIG_LTT
+	return (0xffffffff - ctrl_inl(TMU1_TCNT));
+#endif
 	return 0;
 }
 
diff -uprN linux-2.6.23.1.orig/include/linux/blktrace_api.h linux-2.6.23.1/include/linux/blktrace_api.h
--- linux-2.6.23.1.orig/include/linux/blktrace_api.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/linux/blktrace_api.h	2007-11-28 10:13:52.334354634 +0100
@@ -3,6 +3,7 @@
 
 #include <linux/blkdev.h>
 #include <linux/relay.h>
+#include <linux/marker.h>
 
 /*
  * Trace categories
@@ -142,150 +143,22 @@ struct blk_user_trace_setup {
 	u32 pid;
 };
 
+/* Probe data used for probe-marker connection */
+struct blk_probe_data {
+	const char *name;
+	const char *format;
+	u32 flags;
+	marker_probe_func *callback;
+};
+
 #if defined(CONFIG_BLK_DEV_IO_TRACE)
 extern int blk_trace_ioctl(struct block_device *, unsigned, char __user *);
 extern void blk_trace_shutdown(struct request_queue *);
 extern void __blk_add_trace(struct blk_trace *, sector_t, int, int, u32, int, int, void *);
 
-/**
- * blk_add_trace_rq - Add a trace for a request oriented action
- * @q:		queue the io is for
- * @rq:		the source request
- * @what:	the action
- *
- * Description:
- *     Records an action against a request. Will log the bio offset + size.
- *
- **/
-static inline void blk_add_trace_rq(struct request_queue *q, struct request *rq,
-				    u32 what)
-{
-	struct blk_trace *bt = q->blk_trace;
-	int rw = rq->cmd_flags & 0x03;
-
-	if (likely(!bt))
-		return;
-
-	if (blk_pc_request(rq)) {
-		what |= BLK_TC_ACT(BLK_TC_PC);
-		__blk_add_trace(bt, 0, rq->data_len, rw, what, rq->errors, sizeof(rq->cmd), rq->cmd);
-	} else  {
-		what |= BLK_TC_ACT(BLK_TC_FS);
-		__blk_add_trace(bt, rq->hard_sector, rq->hard_nr_sectors << 9, rw, what, rq->errors, 0, NULL);
-	}
-}
-
-/**
- * blk_add_trace_bio - Add a trace for a bio oriented action
- * @q:		queue the io is for
- * @bio:	the source bio
- * @what:	the action
- *
- * Description:
- *     Records an action against a bio. Will log the bio offset + size.
- *
- **/
-static inline void blk_add_trace_bio(struct request_queue *q, struct bio *bio,
-				     u32 what)
-{
-	struct blk_trace *bt = q->blk_trace;
-
-	if (likely(!bt))
-		return;
-
-	__blk_add_trace(bt, bio->bi_sector, bio->bi_size, bio->bi_rw, what, !bio_flagged(bio, BIO_UPTODATE), 0, NULL);
-}
-
-/**
- * blk_add_trace_generic - Add a trace for a generic action
- * @q:		queue the io is for
- * @bio:	the source bio
- * @rw:		the data direction
- * @what:	the action
- *
- * Description:
- *     Records a simple trace
- *
- **/
-static inline void blk_add_trace_generic(struct request_queue *q,
-					 struct bio *bio, int rw, u32 what)
-{
-	struct blk_trace *bt = q->blk_trace;
-
-	if (likely(!bt))
-		return;
-
-	if (bio)
-		blk_add_trace_bio(q, bio, what);
-	else
-		__blk_add_trace(bt, 0, 0, rw, what, 0, 0, NULL);
-}
-
-/**
- * blk_add_trace_pdu_int - Add a trace for a bio with an integer payload
- * @q:		queue the io is for
- * @what:	the action
- * @bio:	the source bio
- * @pdu:	the integer payload
- *
- * Description:
- *     Adds a trace with some integer payload. This might be an unplug
- *     option given as the action, with the depth at unplug time given
- *     as the payload
- *
- **/
-static inline void blk_add_trace_pdu_int(struct request_queue *q, u32 what,
-					 struct bio *bio, unsigned int pdu)
-{
-	struct blk_trace *bt = q->blk_trace;
-	__be64 rpdu = cpu_to_be64(pdu);
-
-	if (likely(!bt))
-		return;
-
-	if (bio)
-		__blk_add_trace(bt, bio->bi_sector, bio->bi_size, bio->bi_rw, what, !bio_flagged(bio, BIO_UPTODATE), sizeof(rpdu), &rpdu);
-	else
-		__blk_add_trace(bt, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu);
-}
-
-/**
- * blk_add_trace_remap - Add a trace for a remap operation
- * @q:		queue the io is for
- * @bio:	the source bio
- * @dev:	target device
- * @from:	source sector
- * @to:		target sector
- *
- * Description:
- *     Device mapper or raid target sometimes need to split a bio because
- *     it spans a stripe (or similar). Add a trace for that action.
- *
- **/
-static inline void blk_add_trace_remap(struct request_queue *q, struct bio *bio,
-				       dev_t dev, sector_t from, sector_t to)
-{
-	struct blk_trace *bt = q->blk_trace;
-	struct blk_io_trace_remap r;
-
-	if (likely(!bt))
-		return;
-
-	r.device = cpu_to_be32(dev);
-	r.device_from = cpu_to_be32(bio->bi_bdev->bd_dev);
-	r.sector = cpu_to_be64(to);
-
-	__blk_add_trace(bt, from, bio->bi_size, bio->bi_rw, BLK_TA_REMAP, !bio_flagged(bio, BIO_UPTODATE), sizeof(r), &r);
-}
-
 #else /* !CONFIG_BLK_DEV_IO_TRACE */
 #define blk_trace_ioctl(bdev, cmd, arg)		(-ENOTTY)
 #define blk_trace_shutdown(q)			do { } while (0)
-#define blk_add_trace_rq(q, rq, what)		do { } while (0)
-#define blk_add_trace_bio(q, rq, what)		do { } while (0)
-#define blk_add_trace_generic(q, rq, rw, what)	do { } while (0)
-#define blk_add_trace_pdu_int(q, what, bio, pdu)	do { } while (0)
-#define blk_add_trace_remap(q, bio, dev, f, t)	do {} while (0)
 #endif /* CONFIG_BLK_DEV_IO_TRACE */
 
 #endif
diff -uprN linux-2.6.23.1.orig/include/linux/immediate.h linux-2.6.23.1/include/linux/immediate.h
--- linux-2.6.23.1.orig/include/linux/immediate.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/linux/immediate.h	2007-11-28 10:13:52.335354426 +0100
@@ -0,0 +1,133 @@
+#ifndef _LINUX_IMMEDIATE_H
+#define _LINUX_IMMEDIATE_H
+
+/*
+ * Immediate values, can be updated at runtime and save cache lines.
+ *
+ * (C) Copyright 2007 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ */
+
+#ifdef CONFIG_IMMEDIATE
+
+#include <asm/immediate.h>
+
+/**
+ * immediate_set - set immediate variable (with locking)
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name, taking the module_mutex if required by
+ * the architecture.
+ */
+#define immediate_set(name, i)						\
+	do {								\
+		name##__immediate = (i);				\
+		core_immediate_update();				\
+		module_immediate_update();				\
+	} while (0)
+
+
+/**
+ * _immediate_set - set immediate variable (without locking)
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name. Must be called with module_mutex held.
+ */
+#define _immediate_set(name, i)						\
+	do {								\
+		name##__immediate = (i);				\
+		core_immediate_update();				\
+		_module_immediate_update();				\
+	} while (0)
+
+/**
+ * immediate_set_early - set immediate variable at early boot
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name. Should be used for updates at early boot, when only
+ * one CPU is active and interrupts are disabled.
+ */
+#define immediate_set_early(name, i)					\
+	do {								\
+		name##__immediate = (i);				\
+		immediate_update_early();				\
+	} while (0)
+
+/*
+ * Internal update functions.
+ */
+extern void core_immediate_update(void);
+extern void immediate_update_range(const struct __immediate *begin,
+	const struct __immediate *end);
+extern void immediate_update_early(void);
+
+#else
+
+/*
+ * Generic immediate values: a simple, standard, memory load.
+ */
+
+/**
+ * immediate_read - read immediate variable
+ * @name: immediate value name
+ *
+ * Reads the value of @name.
+ */
+#define immediate_read(name)		_immediate_read(name)
+
+/**
+ * immediate_set - set immediate variable (with locking)
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name, taking the module_mutex if required by
+ * the architecture.
+ */
+#define immediate_set(name, i)		(name##__immediate = (i))
+
+/**
+ * _immediate_set - set immediate variable (without locking)
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name. Must be called with module_mutex held.
+ */
+#define _immediate_set(name, i)		immediate_set(name, i)
+
+/**
+ * immediate_set_early - set immediate variable at early boot
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name. Should be used for early boot updates.
+ */
+#define immediate_set_early(name, i)	immediate_set(name, i)
+
+/*
+ * Internal update functions.
+ */
+static inline void immediate_update_early(void)
+{ }
+#endif
+
+#define DECLARE_IMMEDIATE(type, name) extern __typeof__(type) name##__immediate
+#define DEFINE_IMMEDIATE(type, name)  __typeof__(type) name##__immediate
+
+#define EXPORT_IMMEDIATE_SYMBOL(name) EXPORT_SYMBOL(name##__immediate)
+#define EXPORT_IMMEDIATE_SYMBOL_GPL(name) EXPORT_SYMBOL_GPL(name##__immediate)
+
+/**
+ * _immediate_read - Read immediate value with standard memory load.
+ * @name: immediate value name
+ *
+ * Force a data read of the immediate value instead of the immediate value
+ * based mechanism. Useful for __init and __exit section data read.
+ */
+#define _immediate_read(name)		(name##__immediate)
+
+#endif
diff -uprN linux-2.6.23.1.orig/include/linux/kernel.h linux-2.6.23.1/include/linux/kernel.h
--- linux-2.6.23.1.orig/include/linux/kernel.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/linux/kernel.h	2007-11-28 10:13:52.335354426 +0100
@@ -14,6 +14,7 @@
 #include <linux/compiler.h>
 #include <linux/bitops.h>
 #include <linux/log2.h>
+#include <linux/marker.h>
 #include <asm/byteorder.h>
 #include <asm/bug.h>
 
@@ -381,4 +382,6 @@ struct sysinfo {
 #define NUMA_BUILD 0
 #endif
 
+#define INIT_ARRAY(type, val, len) ((type [len]) { [0 ... (len)-1] = (val) })
+
 #endif
diff -uprN linux-2.6.23.1.orig/include/linux/kprobes.h linux-2.6.23.1/include/linux/kprobes.h
--- linux-2.6.23.1.orig/include/linux/kprobes.h	2007-11-12 18:23:55.000000000 +0100
+++ linux-2.6.23.1/include/linux/kprobes.h	2007-11-28 10:13:52.336354217 +0100
@@ -35,7 +35,6 @@
 #include <linux/percpu.h>
 #include <linux/spinlock.h>
 #include <linux/rcupdate.h>
-#include <linux/mutex.h>
 
 #ifdef CONFIG_KPROBES
 #include <asm/kprobes.h>
@@ -177,7 +176,6 @@ static inline void kretprobe_assert(stru
 }
 
 extern spinlock_t kretprobe_lock;
-extern struct mutex kprobe_mutex;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);
diff -uprN linux-2.6.23.1.orig/include/linux/ltt-core.h linux-2.6.23.1/include/linux/ltt-core.h
--- linux-2.6.23.1.orig/include/linux/ltt-core.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/linux/ltt-core.h	2007-11-28 10:13:52.336354217 +0100
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2005,2006 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * This contains the core definitions for the Linux Trace Toolkit.
+ */
+
+#ifndef LTT_CORE_H
+#define LTT_CORE_H
+
+#include <linux/list.h>
+#include <linux/ltt.h>
+
+/*
+ * All modifications of ltt_traces must be done by ltt-tracer.c, while holding
+ * the semaphore. Only reading of this information can be done elsewhere, with
+ * the RCU mechanism : the preemption must be disabled while reading the
+ * list.
+ */
+struct ltt_traces {
+	struct list_head head;		/* Traces list */
+	unsigned int num_active_traces;	/* Number of active traces */
+} ____cacheline_aligned;
+
+extern struct ltt_traces ltt_traces;
+
+
+/* Keep track of trap nesting inside LTT */
+extern unsigned int ltt_nesting[];
+
+typedef int (*ltt_run_filter_functor)(void *trace, uint16_t eID);
+extern ltt_run_filter_functor ltt_run_filter;
+extern void ltt_filter_register(ltt_run_filter_functor func);
+extern void ltt_filter_unregister(void);
+
+#endif /* LTT_CORE_H */
diff -uprN linux-2.6.23.1.orig/include/linux/ltt-tracer.h linux-2.6.23.1/include/linux/ltt-tracer.h
--- linux-2.6.23.1.orig/include/linux/ltt-tracer.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/linux/ltt-tracer.h	2007-11-28 10:13:52.338353801 +0100
@@ -0,0 +1,829 @@
+/*
+ * Copyright (C) 2005,2006 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * This contains the definitions for the Linux Trace Toolkit tracer.
+ */
+
+#ifndef _LTT_TRACER_H
+#define _LTT_TRACER_H
+
+#include <stdarg.h>
+#include <linux/types.h>
+#include <linux/limits.h>
+#include <linux/list.h>
+#include <linux/cache.h>
+#include <linux/kernel.h>
+#include <linux/timex.h>
+#include <linux/wait.h>
+#include <linux/relay.h>
+#include <linux/ltt-core.h>
+#include <linux/marker.h>
+#include <linux/ltt.h>
+#include <asm/semaphore.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+
+/* Number of bytes to log with a read/write event */
+#define LTT_LOG_RW_SIZE			32L
+
+/* Interval (in jiffies) at which the LTT per-CPU timer fires */
+#define LTT_PERCPU_TIMER_INTERVAL	1
+
+#ifndef LTT_ARCH_TYPE
+#define LTT_ARCH_TYPE			LTT_ARCH_TYPE_UNDEFINED
+#endif
+
+#ifndef LTT_ARCH_VARIANT
+#define LTT_ARCH_VARIANT		LTT_ARCH_VARIANT_NONE
+#endif
+
+struct ltt_active_marker;
+
+/* Maximum number of callbacks per marker */
+#define LTT_NR_CALLBACKS	10
+
+struct ltt_serialize_closure;
+struct ltt_probe_private_data;
+
+/* Serialization callback '%k' */
+typedef char *(*ltt_serialize_cb)(char *buffer, char *str,
+			struct ltt_serialize_closure *closure,
+			void *serialize_private, int align,
+			const char *fmt, va_list *args);
+
+struct ltt_serialize_closure {
+	ltt_serialize_cb *callbacks;
+	long cb_args[LTT_NR_CALLBACKS];
+	unsigned int cb_idx;
+};
+
+char *ltt_serialize_data(char *buffer, char *str,
+			struct ltt_serialize_closure *closure,
+			void *serialize_private,
+			int align,
+			const char *fmt, va_list *args);
+
+struct ltt_available_probe {
+	const char *name;		/* probe name */
+	const char *format;
+	marker_probe_func *probe_func;
+	ltt_serialize_cb callbacks[LTT_NR_CALLBACKS];
+	struct list_head node;		/* registered probes list */
+};
+
+struct ltt_probe_private_data {
+	uint16_t channel;		/*
+					 * Channel override for probe, used
+					 * if non 0.
+					 */
+	struct ltt_trace_struct *trace;	/*
+					 * Specify to which trace the
+					 * information must be recorded.
+					 * Used if !NULL.
+					 */
+	int force;			/*
+					 * Should we write to traces even if
+					 * tracing is stopped ? Internal use
+					 * only.
+					 */
+	uint16_t id;			/*
+					 * Core event static ID.
+					 * Unused if >= MARKER_CORE_IDS.
+					 */
+	void *serialize_private;	/*
+					 * Private data for serialization
+					 * functions.
+					 */
+	int cpu;			/* CPU on behalf of which the data
+					 * must be written. -1 if unset.
+					 * Only used if we are in forced
+					 * write mode.
+					 */
+};
+
+struct ltt_active_marker {
+	struct list_head node;		/* active markers list */
+	const char *name;
+	const char *format;
+	struct ltt_available_probe *probe;
+	int align;
+	uint16_t id;
+	uint16_t channel;
+};
+
+extern void ltt_vtrace(const struct marker *mdata, void *call_data,
+	const char *fmt, va_list *args);
+extern void ltt_trace(const struct marker *mdata, void *call_data,
+	const char *fmt, ...);
+
+/*
+ * Unique ID assigned to each registered probe.
+ */
+enum marker_id {
+	MARKER_ID_SET_MARKER_ID = 0,	/* Static IDs available (range 0-7) */
+	MARKER_ID_SET_MARKER_FORMAT,
+	MARKER_ID_HEARTBEAT_32,
+	MARKER_ID_HEARTBEAT_64,
+	MARKER_ID_COMPACT,		/* Compact IDs (range: 8-127)	    */
+	MARKER_ID_DYNAMIC,		/* Dynamic IDs (range: 128-65535)   */
+};
+
+/* static ids 0-7 reserved for internal use. */
+#define MARKER_CORE_IDS		8
+/* dynamic ids 8-127 reserved for compact events. */
+#define MARKER_COMPACT_IDS	128
+static inline enum marker_id marker_id_type(uint16_t id)
+{
+	if (id < MARKER_CORE_IDS)
+		return (enum marker_id)id;
+	else if (id < MARKER_COMPACT_IDS)
+		return MARKER_ID_COMPACT;
+	else
+		return MARKER_ID_DYNAMIC;
+}
+
+#ifdef CONFIG_LTT
+
+struct ltt_trace_struct;
+
+/* LTTng lockless logging buffer info */
+struct ltt_channel_buf_struct {
+	/* Use the relay void *start as buffer start pointer */
+	local_t offset;			/* Current offset in the buffer */
+	atomic_long_t consumed;		/* Current offset in the buffer
+					 * standard atomic access (shared)
+					 */
+	atomic_long_t active_readers;	/* Active readers count
+					   standard atomic access (shared) */
+	atomic_t wakeup_readers;	/* Boolean : wakeup readers waiting ? */
+	local_t *commit_count;		/* Commit count per sub-buffer */
+	spinlock_t full_lock;		/* buffer full condition spinlock, only
+					 * for userspace tracing blocking mode
+					 * synchronization with reader.
+					 */
+	local_t events_lost;
+	local_t corrupted_subbuffers;
+	struct timeval	current_subbuffer_start_time;
+	wait_queue_head_t write_wait;	/* Wait queue for blocking user space
+					 * writers */
+	struct work_struct wake_writers;/* Writers wake-up work struct */
+} ____cacheline_aligned;
+
+struct ltt_channel_struct {
+	char channel_name[PATH_MAX];
+	struct ltt_trace_struct	*trace;
+	struct ltt_channel_buf_struct buf[NR_CPUS];
+	int overwrite;
+	struct kref kref;
+	int compact;
+
+	void *trans_channel_data;
+
+	/*
+	 * buffer_begin - called on buffer-switch to a new sub-buffer
+	 * @buf: the channel buffer containing the new sub-buffer
+	 */
+	void (*buffer_begin) (struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx);
+	/*
+	 * buffer_end - called on buffer-switch to a new sub-buffer
+	 * @buf: the channel buffer containing the previous sub-buffer
+	 */
+	void (*buffer_end) (struct rchan_buf *buf,
+			u64 tsc, unsigned int offset, unsigned int subbuf_idx);
+};
+
+struct user_dbg_data {
+	unsigned long avail_size;
+	unsigned long write;
+	unsigned long read;
+};
+
+struct ltt_trace_ops {
+	int (*create_dirs) (struct ltt_trace_struct *new_trace);
+	void (*remove_dirs) (struct ltt_trace_struct *new_trace);
+	int (*create_channel) (char *trace_name, struct ltt_trace_struct *trace,
+				struct dentry *dir, char *channel_name,
+				struct ltt_channel_struct **ltt_chan,
+				unsigned int subbuf_size,
+				unsigned int n_subbufs, int overwrite);
+	void (*wakeup_channel) (struct ltt_channel_struct *ltt_channel);
+	void (*finish_channel) (struct ltt_channel_struct *channel);
+	void (*remove_channel) (struct ltt_channel_struct *channel);
+	void *(*reserve_slot) (struct ltt_trace_struct *trace,
+				struct ltt_channel_struct *channel,
+				void **transport_data, size_t data_size,
+				size_t *slot_size, u64 *tsc, int cpu);
+	void (*commit_slot) (struct ltt_channel_struct *channel,
+				void **transport_data, void *reserved,
+				size_t slot_size);
+	int (*user_blocking) (struct ltt_trace_struct *trace,
+				unsigned int index, size_t data_size,
+				struct user_dbg_data *dbg);
+	void (*user_errors) (struct ltt_trace_struct *trace,
+				unsigned int index, size_t data_size,
+				struct user_dbg_data *dbg, int cpu);
+#ifdef CONFIG_HOTPLUG_CPU
+	int (*handle_cpuhp) (struct notifier_block *nb,
+				unsigned long action, void *hcpu,
+				struct ltt_trace_struct *trace);
+#endif
+};
+
+struct ltt_transport {
+	char *name;
+	struct module *owner;
+	struct list_head node;
+	struct ltt_trace_ops ops;
+};
+
+
+enum trace_mode { LTT_TRACE_NORMAL, LTT_TRACE_FLIGHT, LTT_TRACE_HYBRID };
+
+/* Per-trace information - each trace/flight recorder represented by one */
+struct ltt_trace_struct {
+	struct list_head list;
+	int active;
+	char trace_name[NAME_MAX];
+	int paused;
+	enum trace_mode mode;
+	struct ltt_transport *transport;
+	struct ltt_trace_ops *ops;
+	struct kref ltt_transport_kref;
+	u32 freq_scale;
+	u64 start_freq;
+	u64 start_tsc;
+	unsigned long long start_monotonic;
+	struct timeval		start_time;
+	struct {
+		struct dentry			*trace_root;
+		struct dentry			*control_root;
+	} dentry;
+	struct {
+		struct ltt_channel_struct	*facilities;
+		struct ltt_channel_struct	*interrupts;
+		struct ltt_channel_struct	*processes;
+		struct ltt_channel_struct	*modules;
+		struct ltt_channel_struct	*network;
+		struct ltt_channel_struct	*cpu;
+		struct ltt_channel_struct	*compact;
+	} channel;
+	struct rchan_callbacks callbacks;
+	struct kref kref; /* Each channel has a kref of the trace struct */
+} ____cacheline_aligned;
+
+/*
+ * First and last channels in ltt_trace_struct.
+ */
+#define ltt_channel_index_size()	sizeof(struct ltt_channel_struct *)
+#define ltt_channel_index_begin()	GET_CHANNEL_INDEX(facilities)
+#define ltt_channel_index_end()	\
+	(GET_CHANNEL_INDEX(compact) + ltt_channel_index_size())
+
+enum ltt_channels { LTT_CHANNEL_FACILITIES, LTT_CHANNEL_INTERRUPTS,
+	LTT_CHANNEL_PROCESSES, LTT_CHANNEL_MODULES, LTT_CHANNEL_CPU,
+	LTT_CHANNEL_COMPACT, LTT_CHANNEL_NETWORK };
+
+/* Hardcoded event headers
+ *
+ * event header for a trace with active heartbeat : 32 bits timestamps
+ *
+ * headers are 8 bytes aligned : that means members are aligned on memory
+ * boundaries *if* structure starts on a 8 bytes boundary. In order to insure
+ * such alignment, a dynamic per trace alignment value must be set.
+ *
+ * Remember that the C compiler does align each member on the boundary
+ * equivalent to their own size.
+ *
+ * As relay subbuffers are aligned on pages, we are sure that they are 8 bytes
+ * aligned, so the buffer header and trace header are aligned.
+ *
+ * Event headers are aligned depending on the trace alignment option.
+ */
+
+struct ltt_event_header_hb {
+	uint32_t timestamp;
+	uint16_t event_id;
+	uint16_t event_size;
+} __attribute((packed));
+
+struct ltt_event_header_nohb {
+	uint64_t timestamp;
+	uint16_t event_id;
+	uint16_t event_size;
+} __attribute((packed));
+
+struct ltt_event_header_compact {
+	uint32_t bitfield; /* E bits for event ID, 32-E bits for timestamp */
+} __attribute((packed));
+
+struct ltt_trace_header {
+	uint32_t magic_number;
+	uint32_t arch_type;
+	uint32_t arch_variant;
+	uint32_t float_word_order;	 /* Only useful for user space traces */
+	uint8_t arch_size;
+	uint8_t major_version;
+	uint8_t minor_version;
+	uint8_t flight_recorder;
+	uint8_t has_heartbeat;
+	uint8_t alignment;		/* Event header alignment */
+	uint8_t tsc_lsb_truncate;	/* LSB truncate for compact channel */
+	uint8_t tscbits;		/* TSC bits kept for compact channel */
+	uint8_t compact_data_shift;     /* bit shift for compact data */
+	uint32_t freq_scale;
+	uint64_t start_freq;
+	uint64_t start_tsc;
+	uint64_t start_monotonic;
+	uint64_t start_time_sec;
+	uint64_t start_time_usec;
+} __attribute((packed));
+
+
+/*
+ * We use asm/timex.h : cpu_khz/HZ variable in here : we might have to deal
+ * specifically with CPU frequency scaling someday, so using an interpolation
+ * between the start and end of buffer values is not flexible enough. Using an
+ * immediate frequency value permits to calculate directly the times for parts
+ * of a buffer that would be before a frequency change.
+ */
+struct ltt_block_start_header {
+	struct {
+		uint64_t cycle_count;
+		uint64_t freq; /* khz */
+	} begin;
+	struct {
+		uint64_t cycle_count;
+		uint64_t freq; /* khz */
+	} end;
+	uint32_t lost_size;	/* Size unused at the end of the buffer */
+	uint32_t buf_size;	/* The size of this sub-buffer */
+	struct ltt_trace_header	trace;
+} __attribute((packed));
+
+#ifdef CONFIG_LTT_ALIGNMENT
+
+/* Calculate the offset needed to align the type */
+static inline unsigned int ltt_align(size_t align_drift,
+		 size_t size_of_type)
+{
+	size_t alignment = min(sizeof(void *), size_of_type);
+	return ((alignment - align_drift) & (alignment-1));
+}
+/* Default arch alignment */
+#define LTT_ALIGN
+
+static inline int ltt_get_alignment(struct ltt_active_marker *marker)
+{
+	return (likely(!marker || marker->align) ? sizeof(void *) : 0);
+}
+
+#else
+static inline unsigned int ltt_align(size_t align_drift,
+		 size_t size_of_type)
+{
+	return 0;
+}
+
+#define LTT_ALIGN __attribute__((packed))
+
+static inline int ltt_get_alignment(struct ltt_active_marker *marker)
+{
+	return 0;
+}
+#endif /* CONFIG_LTT_ALIGNMENT */
+
+/*
+ * ltt_subbuf_header_len - called on buffer-switch to a new sub-buffer
+ *
+ * returns the client header size at the beginning of the buffer.
+ */
+static inline unsigned int ltt_subbuf_header_len(void)
+{
+	return sizeof(struct ltt_block_start_header);
+}
+
+/* Get the offset of the channel in the ltt_trace_struct */
+#define GET_CHANNEL_INDEX(chan)	\
+	(unsigned int)&((struct ltt_trace_struct *)NULL)->channel.chan
+
+static inline struct ltt_channel_struct *ltt_get_channel_from_index(
+		struct ltt_trace_struct *trace, unsigned int index)
+{
+	return *(struct ltt_channel_struct **)((void *)trace+index);
+}
+
+
+/*
+ * ltt_get_header_size
+ *
+ * Calculate alignment offset for arch size void*. This is the
+ * alignment offset of the event header.
+ *
+ * Important note :
+ * The event header must be a size multiple of the void* size. This is necessary
+ * to be able to calculate statically the alignment offset of the variable
+ * length data fields that follows. The total offset calculated here :
+ *
+ *	 Alignment of header struct on arch size
+ * + sizeof(header struct)
+ * + padding added to end of struct to align on arch size.
+ * */
+static inline unsigned char ltt_get_header_size(
+		struct ltt_channel_struct *channel,
+		void *address,
+		size_t data_size,
+		size_t *before_hdr_pad)
+{
+	unsigned int padding;
+	unsigned int header;
+	size_t after_hdr_pad;
+
+#ifdef CONFIG_LTT_HEARTBEAT
+	if (unlikely(channel->compact))
+		header = sizeof(struct ltt_event_header_compact);
+	else
+		header = sizeof(struct ltt_event_header_hb);
+#else
+	header = sizeof(struct ltt_event_header_nohb);
+#endif
+
+	/* Padding before the header. Calculated dynamically */
+	*before_hdr_pad = ltt_align((unsigned long)address, header);
+	padding = *before_hdr_pad;
+
+	/*
+	 * Padding after header, considering header aligned on ltt_align.
+	 * Calculated statically if header size is known. For compact
+	 * channels, do not align the data.
+	 */
+#ifdef CONFIG_LTT_HEARTBEAT
+	if (unlikely(channel->compact))
+		after_hdr_pad = 0;
+	else
+		after_hdr_pad = ltt_align(header, sizeof(void *));
+#else
+	after_hdr_pad = ltt_align(header, sizeof(void *));
+#endif
+	padding += after_hdr_pad;
+
+	return header+padding;
+}
+
+enum ltt_heartbeat_functor_msg { LTT_HEARTBEAT_START, LTT_HEARTBEAT_STOP };
+
+#ifdef CONFIG_LTT_HEARTBEAT
+
+extern int ltt_tsc_lsb_truncate;
+extern int ltt_tscbits;
+extern int ltt_compact_data_shift;
+
+extern void ltt_init_compact_markers(unsigned int num_compact_events);
+extern int ltt_heartbeat_trigger(enum ltt_heartbeat_functor_msg msg);
+extern void ltt_write_full_tsc(struct ltt_trace_struct *trace);
+
+
+static inline char *ltt_write_compact_header(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void *ptr,
+		uint16_t eID, size_t event_size,
+		u64 tsc, u32 data)
+{
+	struct ltt_event_header_compact *compact_hdr;
+	int tscbits = ltt_tscbits;
+	int lsb_truncate = ltt_tsc_lsb_truncate;
+	u32 compact_tsc;
+
+	compact_hdr = (struct ltt_event_header_compact *)ptr;
+	compact_tsc = ((u32)tsc >> lsb_truncate) & ((1 << tscbits) - 1);
+	compact_hdr->bitfield = (data << ltt_compact_data_shift)
+				| ((u32)eID << tscbits) | compact_tsc;
+	return ptr + sizeof(*compact_hdr);
+}
+
+/*
+ * ltt_write_event_header
+ *
+ * Writes the event header to the pointer.
+ *
+ * @channel : pointer to the channel structure
+ * @ptr : buffer pointer
+ * @eID : event ID
+ * @event_size : size of the event, excluding the event header.
+ * @tsc : time stamp counter.
+ *
+ * returns : pointer where the event data must be written.
+ */
+static inline char *ltt_write_event_header(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void *ptr,
+		uint16_t eID, size_t event_size,
+		u64 tsc)
+{
+	size_t after_hdr_pad;
+	struct ltt_event_header_hb *hb;
+
+	event_size = min(event_size, (size_t)0xFFFFU);
+	hb = (struct ltt_event_header_hb *)ptr;
+	hb->timestamp = (u32)tsc;
+	hb->event_id = eID;
+	hb->event_size = (uint16_t)event_size;
+	if (unlikely(channel->compact))
+		after_hdr_pad = 0;
+	else
+		after_hdr_pad = ltt_align(sizeof(*hb), sizeof(void *));
+	return ptr + sizeof(*hb) + after_hdr_pad;
+}
+
+#else /* CONFIG_LTT_HEARTBEAT */
+
+static inline void ltt_init_compact_markers(unsigned int num_compact_events) { }
+static inline void ltt_write_full_tsc(struct ltt_trace_struct *trace) { }
+
+static inline char *ltt_write_compact_header(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void *ptr,
+		uint16_t eID, size_t event_size,
+		u64 tsc, u32 data)
+{
+	return ptr;
+}
+
+static inline char *ltt_write_event_header(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void *ptr,
+		uint16_t eID, size_t event_size,
+		u64 tsc)
+{
+	size_t after_hdr_pad;
+	struct ltt_event_header_nohb *nohb;
+
+	event_size = min(event_size, (size_t)0xFFFFU);
+	nohb = (struct ltt_event_header_nohb *)ptr;
+	nohb->timestamp = (u64)tsc;
+	nohb->event_id = eID;
+	nohb->event_size = (uint16_t)event_size;
+	after_hdr_pad = ltt_align(sizeof(*nohb), sizeof(void *));
+	return ptr + sizeof(*nohb) + after_hdr_pad;
+}
+
+#endif /* CONFIG_LTT_HEARTBEAT */
+
+/* Lockless LTTng */
+
+/* Buffer offset macros */
+
+#define BUFFER_OFFSET(offset, chan) ((offset) & (chan->alloc_size-1))
+#define SUBBUF_OFFSET(offset, chan) ((offset) & (chan->subbuf_size-1))
+#define SUBBUF_ALIGN(offset, chan) \
+	(((offset) + chan->subbuf_size) & (~(chan->subbuf_size-1)))
+#define SUBBUF_TRUNC(offset, chan) \
+	((offset) & (~(chan->subbuf_size-1)))
+#define SUBBUF_INDEX(offset, chan) \
+	(BUFFER_OFFSET((offset), chan) / chan->subbuf_size)
+
+/*
+ * for flight recording. must be called after relay_commit.
+ * This function decrements de subbuffer's lost_size each time the commit count
+ * reaches back the reserve offset (module subbuffer size). It is useful for
+ * crash dump.
+ * We use slot_size - 1 to make sure we deal correctly with the case where we
+ * fill the subbuffer completely (so the subbuf index stays in the previous
+ * subbuffer).
+ */
+#ifdef CONFIG_LTT_VMCORE
+static inline void ltt_write_commit_counter(struct rchan_buf *buf,
+		void *reserved, size_t slot_size)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_block_start_header *header =
+		(struct ltt_block_start_header *)buf->data;
+	long offset, subbuf_idx, commit_count;
+	uint32_t lost_old, lost_new;
+
+	offset = reserved + slot_size - buf->start;
+	subbuf_idx = (offset - 1) / buf->chan->subbuf_size;
+	for (;;) {
+		lost_old = header->lost_size;
+		commit_count =
+			local_read(&channel->buf[buf->cpu].commit_count[
+				subbuf_idx]);
+		if (!SUBBUF_OFFSET(offset - commit_count, buf->chan)) {
+			lost_new = (uint32_t)buf->chan->subbuf_size
+				- SUBBUF_OFFSET(commit_count, buf->chan);
+			lost_old = cmpxchg_local(&header->lost_size, lost_old,
+				lost_new);
+			if (lost_old <= lost_new)
+				break;
+		} else {
+			break;
+		}
+	}
+}
+#else
+static inline void ltt_write_commit_counter(struct rchan_buf *buf,
+		void *reserved, size_t slot_size)
+{
+}
+#endif
+
+/*
+ * ltt_reserve_slot
+ *
+ * Atomic slot reservation in a LTTng buffer. It will take care of
+ * sub-buffer switching.
+ *
+ * Parameters:
+ *
+ * @trace : the trace structure to log to.
+ * @buf : the buffer to reserve space into.
+ * @data_size : size of the variable length data to log.
+ * @slot_size : pointer to total size of the slot (out)
+ * @tsc : pointer to the tsc at the slot reservation (out)
+ * @cpu : cpu id
+ *
+ * Return : NULL if not enough space, else returns the pointer
+ * 					to the beginning of the reserved slot.
+ */
+static inline void *ltt_reserve_slot(
+		struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void **transport_data,
+		size_t data_size,
+		size_t *slot_size,
+		u64 *tsc,
+		int cpu)
+{
+	return trace->ops->reserve_slot(trace, channel, transport_data,
+			data_size, slot_size, tsc, cpu);
+}
+
+
+/*
+ * ltt_commit_slot
+ *
+ * Atomic unordered slot commit. Increments the commit count in the
+ * specified sub-buffer, and delivers it if necessary.
+ *
+ * Parameters:
+ *
+ * @buf : the buffer to commit to.
+ * @reserved : address of the beginning of the reserved slot.
+ * @slot_size : size of the reserved slot.
+ */
+static inline void ltt_commit_slot(
+		struct ltt_channel_struct *channel,
+		void **transport_data,
+		void *reserved,
+		size_t slot_size)
+{
+	struct ltt_trace_struct *trace = channel->trace;
+
+	trace->ops->commit_slot(channel, transport_data, reserved, slot_size);
+}
+
+/*
+ * 4 control channels :
+ * ltt/control/facilities
+ * ltt/control/interrupts
+ * ltt/control/processes
+ * ltt/control/network
+ *
+ * 2 cpu channels :
+ * ltt/cpu
+ * ltt/compact
+ */
+#define LTT_RELAY_ROOT		"ltt"
+#define LTT_CONTROL_ROOT	"control"
+#define LTT_FACILITIES_CHANNEL	"facilities"
+#define LTT_INTERRUPTS_CHANNEL	"interrupts"
+#define LTT_PROCESSES_CHANNEL	"processes"
+#define LTT_MODULES_CHANNEL	"modules"
+#define LTT_NETWORK_CHANNEL	"network"
+#define LTT_CPU_CHANNEL		"cpu"
+#define LTT_COMPACT_CHANNEL	"compact"
+#define LTT_FLIGHT_PREFIX	"flight-"
+
+/* System types */
+#define LTT_SYS_TYPE_VANILLA_LINUX	1
+
+/* Architecture types */
+#define LTT_ARCH_TYPE_I386		1
+#define LTT_ARCH_TYPE_PPC		2
+#define LTT_ARCH_TYPE_SH		3
+#define LTT_ARCH_TYPE_S390		4
+#define LTT_ARCH_TYPE_MIPS		5
+#define LTT_ARCH_TYPE_ARM		6
+#define LTT_ARCH_TYPE_PPC64		7
+#define LTT_ARCH_TYPE_X86_64		8
+#define LTT_ARCH_TYPE_C2		9
+#define LTT_ARCH_TYPE_POWERPC		10
+#define LTT_ARCH_TYPE_X86		11
+#define LTT_ARCH_TYPE_UNDEFINED		12
+
+/* Standard definitions for variants */
+#define LTT_ARCH_VARIANT_NONE		0
+
+/* Tracer properties */
+#define LTT_DEFAULT_SUBBUF_SIZE_LOW	65536
+#define LTT_DEFAULT_N_SUBBUFS_LOW	2
+#define LTT_DEFAULT_SUBBUF_SIZE_MED	262144
+#define LTT_DEFAULT_N_SUBBUFS_MED	2
+#define LTT_DEFAULT_SUBBUF_SIZE_HIGH	1048576
+#define LTT_DEFAULT_N_SUBBUFS_HIGH	2
+#define LTT_TRACER_MAGIC_NUMBER		0x00D6B7ED
+#define LTT_TRACER_VERSION_MAJOR	1
+#define LTT_TRACER_VERSION_MINOR	0
+
+/*
+ * Size reserved for high priority events (interrupts, NMI, BH) at the end of a
+ * nearly full buffer. User space won't use this last amount of space when in
+ * blocking mode. This space also includes the event header that would be
+ * written by this user space event.
+ */
+#define LTT_RESERVE_CRITICAL		4096
+
+/* Register and unregister function pointers */
+
+enum ltt_module_function {
+	LTT_FUNCTION_RUN_FILTER,
+	LTT_FUNCTION_FILTER_CONTROL,
+	LTT_FUNCTION_STATEDUMP
+};
+
+extern int ltt_module_register(enum ltt_module_function name, void *function,
+		struct module *owner);
+extern void ltt_module_unregister(enum ltt_module_function name);
+
+void ltt_transport_register(struct ltt_transport *transport);
+void ltt_transport_unregister(struct ltt_transport *transport);
+
+/* Exported control function */
+
+enum ltt_control_msg {
+	LTT_CONTROL_START,
+	LTT_CONTROL_STOP,
+	LTT_CONTROL_CREATE_TRACE,
+	LTT_CONTROL_DESTROY_TRACE
+};
+
+union ltt_control_args {
+	struct {
+		enum trace_mode mode;
+		unsigned subbuf_size_low;
+		unsigned n_subbufs_low;
+		unsigned subbuf_size_med;
+		unsigned n_subbufs_med;
+		unsigned subbuf_size_high;
+		unsigned n_subbufs_high;
+	} new_trace;
+};
+
+extern int ltt_control(enum ltt_control_msg msg, char *trace_name,
+		char *trace_type, union ltt_control_args args);
+
+enum ltt_filter_control_msg {
+	LTT_FILTER_DEFAULT_ACCEPT,
+	LTT_FILTER_DEFAULT_REJECT };
+
+extern int ltt_filter_control(enum ltt_filter_control_msg msg,
+		char *trace_name);
+
+void ltt_write_trace_header(struct ltt_trace_struct *trace,
+		struct ltt_trace_header *header);
+extern void ltt_buffer_destroy(struct ltt_channel_struct *ltt_chan);
+extern void ltt_wakeup_writers(struct work_struct *work);
+
+void ltt_core_register(int (*function)(u8, void *));
+
+void ltt_core_unregister(void);
+
+void ltt_release_trace(struct kref *kref);
+void ltt_release_transport(struct kref *kref);
+
+extern int ltt_probe_register(struct ltt_available_probe *pdata);
+extern int ltt_probe_unregister(struct ltt_available_probe *pdata);
+extern int ltt_marker_connect(const char *mname, const char *pname,
+		enum marker_id id, uint16_t channel, int user, int align);
+extern int ltt_marker_disconnect(const char *mname, int user);
+extern void ltt_dump_marker_state(struct ltt_trace_struct *trace);
+extern void probe_id_defrag(void);
+
+void ltt_lock_traces(void);
+void ltt_unlock_traces(void);
+
+/* Relay IOCTL */
+
+/* Get the next sub buffer that can be read. */
+#define RELAY_GET_SUBBUF		_IOR(0xF5, 0x00, __u32)
+/* Release the oldest reserved (by "get") sub buffer. */
+#define RELAY_PUT_SUBBUF		_IOW(0xF5, 0x01, __u32)
+/* returns the number of sub buffers in the per cpu channel. */
+#define RELAY_GET_N_SUBBUFS		_IOR(0xF5, 0x02, __u32)
+/* returns the size of the sub buffers. */
+#define RELAY_GET_SUBBUF_SIZE		_IOR(0xF5, 0x03, __u32)
+
+#endif /* CONFIG_LTT */
+
+#endif /* _LTT_TRACER_H */
diff -uprN linux-2.6.23.1.orig/include/linux/ltt.h linux-2.6.23.1/include/linux/ltt.h
--- linux-2.6.23.1.orig/include/linux/ltt.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/linux/ltt.h	2007-11-28 10:13:52.336354217 +0100
@@ -0,0 +1,27 @@
+#ifndef _LINUX_LTT_H
+#define _LINUX_LTT_H
+
+/*
+ * Generic LTT clock.
+ *
+ * Chooses between an architecture specific clock or an atomic logical clock.
+ *
+ * Copyright (C) 2007 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ */
+
+#ifdef CONFIG_LTT_TIMESTAMP
+#ifdef CONFIG_ARCH_SUPPORTS_LTT_CLOCK
+#include <asm/ltt.h>
+#else
+#include <asm-generic/ltt.h>
+
+#define ltt_get_timestamp32	ltt_get_timestamp32_generic
+#define ltt_get_timestamp64	ltt_get_timestamp64_generic
+#define ltt_add_timestamp	ltt_add_timestamp_generic
+#define ltt_frequency		ltt_frequency_generic
+#define ltt_freq_scale		ltt_freq_scale_generic
+#endif /* CONFIG_ARCH_SUPPORTS_LTT_CLOCK */
+#else
+#define ltt_add_timestamp(ticks)
+#endif /* CONFIG_LTT_TIMESTAMP */
+#endif /* _LINUX_LTT_H */
diff -uprN linux-2.6.23.1.orig/include/linux/marker.h linux-2.6.23.1/include/linux/marker.h
--- linux-2.6.23.1.orig/include/linux/marker.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/include/linux/marker.h	2007-11-28 10:13:52.338353801 +0100
@@ -0,0 +1,168 @@
+#ifndef _LINUX_MARKER_H
+#define _LINUX_MARKER_H
+
+/*
+ * Code markup for dynamic and static tracing.
+ *
+ * See Documentation/marker.txt.
+ *
+ * (C) Copyright 2006 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ */
+
+#include <linux/immediate.h>
+#include <linux/types.h>
+
+struct module;
+struct marker;
+
+/**
+ * marker_probe_func - Type of a marker probe function
+ * @mdata: pointer of type struct marker
+ * @private_data: caller site private data
+ * @fmt: format string
+ * @...: variable argument list
+ *
+ * Type of marker probe functions. They receive the mdata and need to parse the
+ * format string to recover the variable argument list.
+ */
+typedef void marker_probe_func(const struct marker *mdata,
+	void *private_data, const char *fmt, ...);
+
+struct marker {
+	const char *name;	/* Marker name */
+	const char *format;	/* Marker format string, describing the
+				 * variable argument list.
+				 */
+	DEFINE_IMMEDIATE(char, state);	/* Immediate value state. */
+	marker_probe_func *call;/* Probe handler function pointer */
+	void *private;		/* Private probe data */
+} __attribute__((aligned(8)));
+
+#ifdef CONFIG_MARKERS
+
+/*
+ * Generic marker flavor always available.
+ * Note : the empty asm volatile with read constraint is used here instead of a
+ * "used" attribute to fix a gcc 4.1.x bug.
+ * Make sure the alignment of the structure in the __markers section will
+ * not add unwanted padding between the beginning of the section and the
+ * structure. Force alignment to the same alignment as the section start.
+ */
+#define __trace_mark(generic, name, call_data, format, args...)		\
+	do {								\
+		static const char __mstrtab_name_##name[]		\
+		__attribute__((section("__markers_strings")))		\
+		= #name;						\
+		static const char __mstrtab_format_##name[]		\
+		__attribute__((section("__markers_strings")))		\
+		= format;						\
+		static struct marker __mark_##name			\
+		__attribute__((section("__markers"), aligned(8))) =	\
+		{ __mstrtab_name_##name, __mstrtab_format_##name,	\
+		0, __mark_empty_function, NULL };			\
+		asm volatile("" : : "i" (&__mark_##name));		\
+		__mark_check_format(format, ## args);			\
+		if (!generic) {						\
+			if (unlikely(immediate_read(__mark_##name.state))) { \
+				preempt_disable();			\
+				(*__mark_##name.call)			\
+					(&__mark_##name, call_data,	\
+					format, ## args);		\
+				preempt_enable();			\
+			}						\
+		} else {						\
+			if (unlikely(_immediate_read(__mark_##name.state))) { \
+				preempt_disable();			\
+				(*__mark_##name.call)			\
+					(&__mark_##name, call_data,	\
+					format, ## args);		\
+				preempt_enable();			\
+			}						\
+		}							\
+	} while (0)
+
+extern void marker_update_probe_range(struct marker *begin,
+	struct marker *end, struct module *probe_module, int *refcount);
+#else /* !CONFIG_MARKERS */
+#define __trace_mark(generic, name, call_data, format, args...) \
+		__mark_check_format(format, ## args)
+static inline void marker_update_probe_range(struct marker *begin,
+	struct marker *end, struct module *probe_module, int *refcount)
+{ }
+#endif /* CONFIG_MARKERS */
+
+/**
+ * trace_mark - Marker using code patching
+ * @name: marker name, not quoted.
+ * @format: format string
+ * @args...: variable argument list
+ *
+ * Places a marker using optimized code patching technique (immediate_read())
+ * to be enabled.
+ */
+#define trace_mark(name, format, args...) \
+	__trace_mark(0, name, NULL, format, ## args)
+
+/**
+ * _trace_mark - Marker using variable read
+ * @name: marker name, not quoted.
+ * @format: format string
+ * @args...: variable argument list
+ *
+ * Places a marker using a standard memory read (_immediate_read()) to be
+ * enabled. Should be used for markers in __init and __exit functions and in
+ * lockdep code.
+ */
+#define _trace_mark(name, format, args...) \
+	__trace_mark(1, name, NULL, format, ## args)
+
+#define MARK_MAX_FORMAT_LEN	1024
+
+/**
+ * MARK_NOARGS - Format string for a marker with no argument.
+ */
+#define MARK_NOARGS " "
+
+/* To be used for string format validity checking with gcc */
+static inline void __printf(1, 2) __mark_check_format(const char *fmt, ...)
+{
+}
+
+extern marker_probe_func __mark_empty_function;
+
+/*
+ * Connect a probe to a marker.
+ * private data pointer must be a valid allocated memory address, or NULL.
+ */
+extern int marker_probe_register(const char *name, const char *format,
+				marker_probe_func *probe, void *private);
+
+/*
+ * Returns the private data given to marker_probe_register.
+ */
+extern void *marker_probe_unregister(const char *name);
+/*
+ * Unregister a marker by providing the registered private data.
+ */
+extern void *marker_probe_unregister_private_data(void *private);
+
+extern int marker_arm(const char *name);
+extern int marker_disarm(const char *name);
+extern void *marker_get_private_data(const char *name);
+
+struct marker_iter {
+	struct module *module;
+	struct marker *marker;
+};
+
+extern void marker_iter_start(struct marker_iter *iter);
+extern void marker_iter_next(struct marker_iter *iter);
+extern void marker_iter_stop(struct marker_iter *iter);
+extern void marker_iter_reset(struct marker_iter *iter);
+extern int marker_get_iter_range(struct marker **marker, struct marker *begin,
+	struct marker *end);
+
+#endif
diff -uprN linux-2.6.23.1.orig/include/linux/memory.h linux-2.6.23.1/include/linux/memory.h
--- linux-2.6.23.1.orig/include/linux/memory.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/linux/memory.h	2007-11-28 10:13:52.339353592 +0100
@@ -86,4 +86,11 @@ extern int remove_memory_block(unsigned 
 	register_memory_notifier(&fn##_mem_nb);			\
 }
 
+/*
+ * Take and release the kernel text modification lock, used for code patching.
+ * Users of this lock can sleep.
+ */
+extern void kernel_text_lock(void);
+extern void kernel_text_unlock(void);
+
 #endif /* _LINUX_MEMORY_H_ */
diff -uprN linux-2.6.23.1.orig/include/linux/module.h linux-2.6.23.1/include/linux/module.h
--- linux-2.6.23.1.orig/include/linux/module.h	2007-11-28 10:17:05.101167183 +0100
+++ linux-2.6.23.1/include/linux/module.h	2007-11-28 10:13:52.339353592 +0100
@@ -15,6 +15,8 @@
 #include <linux/stringify.h>
 #include <linux/kobject.h>
 #include <linux/moduleparam.h>
+#include <linux/immediate.h>
+#include <linux/marker.h>
 #include <asm/local.h>
 
 #include <asm/module.h>
@@ -370,6 +372,14 @@ struct module
 	/* The command line arguments (may be mangled).  People like
 	   keeping pointers to this stuff */
 	char *args;
+#ifdef CONFIG_IMMEDIATE
+	const struct __immediate *immediate;
+	unsigned int num_immediate;
+#endif
+#ifdef CONFIG_MARKERS
+	struct marker *markers;
+	unsigned int num_markers;
+#endif
 };
 #ifndef MODULE_ARCH_INIT
 #define MODULE_ARCH_INIT {}
@@ -482,6 +492,13 @@ int register_module_notifier(struct noti
 int unregister_module_notifier(struct notifier_block * nb);
 
 extern void print_modules(void);
+extern void list_modules(void *call_data);
+
+extern void module_update_markers(struct module *probe_module, int *refcount);
+extern int module_get_iter_markers(struct marker_iter *iter);
+
+extern void _module_immediate_update(void);
+extern void module_immediate_update(void);
 
 #else /* !CONFIG_MODULES... */
 #define EXPORT_SYMBOL(sym)
@@ -584,6 +601,24 @@ static inline void print_modules(void)
 {
 }
 
+static inline void module_update_markers(struct module *probe_module,
+		int *refcount)
+{
+}
+
+static inline int module_get_iter_markers(struct marker_iter *iter)
+{
+	return 0;
+}
+
+static inline void _module_immediate_update(void)
+{
+}
+
+static inline void module_immediate_update(void)
+{
+}
+
 #endif /* CONFIG_MODULES */
 
 struct device_driver;
diff -uprN linux-2.6.23.1.orig/include/linux/netlink.h linux-2.6.23.1/include/linux/netlink.h
--- linux-2.6.23.1.orig/include/linux/netlink.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/linux/netlink.h	2007-11-28 10:13:52.340353384 +0100
@@ -24,6 +24,7 @@
 /* leave room for NETLINK_DM (DM Events) */
 #define NETLINK_SCSITRANSPORT	18	/* SCSI Transports */
 #define NETLINK_ECRYPTFS	19
+#define NETLINK_LTT		31 	/* Linux Trace Toolkit FIXME */
 
 #define MAX_LINKS 32		
 
diff -uprN linux-2.6.23.1.orig/include/linux/profile.h linux-2.6.23.1/include/linux/profile.h
--- linux-2.6.23.1.orig/include/linux/profile.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/linux/profile.h	2007-11-28 10:13:52.340353384 +0100
@@ -7,10 +7,11 @@
 #include <linux/init.h>
 #include <linux/cpumask.h>
 #include <linux/cache.h>
+#include <linux/immediate.h>
 
 #include <asm/errno.h>
 
-extern int prof_on __read_mostly;
+DECLARE_IMMEDIATE(char, prof_on) __read_mostly;
 
 #define CPU_PROFILING	1
 #define SCHED_PROFILING	2
@@ -38,7 +39,7 @@ static inline void profile_hit(int type,
 	/*
 	 * Speedup for the common (no profiling enabled) case:
 	 */
-	if (unlikely(prof_on == type))
+	if (unlikely(immediate_read(prof_on) == type))
 		profile_hits(type, ip, 1);
 }
 
diff -uprN linux-2.6.23.1.orig/include/linux/sched.h linux-2.6.23.1/include/linux/sched.h
--- linux-2.6.23.1.orig/include/linux/sched.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/linux/sched.h	2007-11-28 10:13:52.342352967 +0100
@@ -1880,6 +1880,9 @@ static inline void inc_syscw(struct task
 }
 #endif
 
+extern void clear_kernel_trace_flag_all_tasks(void);
+extern void set_kernel_trace_flag_all_tasks(void);
+
 #endif /* __KERNEL__ */
 
 #endif
diff -uprN linux-2.6.23.1.orig/include/linux/seq_file.h linux-2.6.23.1/include/linux/seq_file.h
--- linux-2.6.23.1.orig/include/linux/seq_file.h	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/include/linux/seq_file.h	2007-11-28 10:13:52.342352967 +0100
@@ -61,5 +61,25 @@ extern struct list_head *seq_list_start_
 extern struct list_head *seq_list_next(void *v, struct list_head *head,
 		loff_t *ppos);
 
+/*
+ * Helpers for iteration over a list sorted by ascending head pointer address.
+ * To be used in contexts where preemption cannot be disabled to insure to
+ * continue iteration on a modified list starting at the same location where it
+ * stopped, or at a following location. It insures that the lost information
+ * will only be in elements added/removed from the list between iterations.
+ * void *pos is only used to get the next list element and may not be a valid
+ * list_head anymore when given to seq_sorted_list_start() or
+ * seq_sorted_list_start_head().
+ */
+extern struct list_head *seq_sorted_list_start(struct list_head *head,
+		loff_t *ppos);
+extern struct list_head *seq_sorted_list_start_head(struct list_head *head,
+		loff_t *ppos);
+/*
+ * next must be called with an existing p node
+ */
+extern struct list_head *seq_sorted_list_next(void *p, struct list_head *head,
+		loff_t *ppos);
+
 #endif
 #endif
diff -uprN linux-2.6.23.1.orig/init/Kconfig linux-2.6.23.1/init/Kconfig
--- linux-2.6.23.1.orig/init/Kconfig	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/init/Kconfig	2007-11-28 10:13:52.343352758 +0100
@@ -350,6 +350,17 @@ config CC_OPTIMIZE_FOR_SIZE
 config SYSCTL
 	bool
 
+config IMMEDIATE
+	default y if !DISABLE_IMMEDIATE
+	depends on X86_32 || X86_64 || PPC || PPC64
+	bool
+	help
+	  Immediate values are used as read-mostly variables that are rarely
+	  updated. They use code patching to modify the values inscribed in the
+	  instruction stream. It provides a way to save precious cache lines
+	  that would otherwise have to be used by these variables. They can be
+	  disabled through the EMBEDDED menu.
+
 menuconfig EMBEDDED
 	bool "Configure standard kernel features (for small systems)"
 	help
@@ -571,6 +582,16 @@ config SLOB
 
 endchoice
 
+config DISABLE_IMMEDIATE
+	default y if EMBEDDED
+	bool "Disable immediate values" if EMBEDDED
+	depends on X86_32 || X86_64 || PPC || PPC64
+	help
+	  Disable code patching based immediate values for embedded systems. It
+	  consumes slightly more memory and requires to modify the instruction
+	  stream each time a variable is updated. Should really be disabled for
+	  embedded systems with read-only text.
+
 endmenu		# General setup
 
 config RT_MUTEXES
diff -uprN linux-2.6.23.1.orig/init/main.c linux-2.6.23.1/init/main.c
--- linux-2.6.23.1.orig/init/main.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/init/main.c	2007-11-28 10:13:52.344352550 +0100
@@ -55,6 +55,7 @@
 #include <linux/pid_namespace.h>
 #include <linux/device.h>
 #include <linux/kthread.h>
+#include <linux/immediate.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -523,6 +524,7 @@ asmlinkage void __init start_kernel(void
 	 */
 	unwind_init();
 	lockdep_init();
+	immediate_update_early();
 
 	local_irq_disable();
 	early_boot_irqs_off();
diff -uprN linux-2.6.23.1.orig/ipc/msg.c linux-2.6.23.1/ipc/msg.c
--- linux-2.6.23.1.orig/ipc/msg.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/ipc/msg.c	2007-11-28 10:13:52.344352550 +0100
@@ -286,6 +286,7 @@ asmlinkage long sys_msgget(key_t key, in
 	}
 	mutex_unlock(&msg_ids(ns).mutex);
 
+	trace_mark(ipc_msg_create, "id %d flags %d", ret, msgflg);
 	return ret;
 }
 
diff -uprN linux-2.6.23.1.orig/ipc/sem.c linux-2.6.23.1/ipc/sem.c
--- linux-2.6.23.1.orig/ipc/sem.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/ipc/sem.c	2007-11-28 10:13:52.345352342 +0100
@@ -293,6 +293,7 @@ asmlinkage long sys_semget (key_t key, i
 	}
 
 	mutex_unlock(&sem_ids(ns).mutex);
+	trace_mark(ipc_sem_create, "id %d flags %d", err, semflg);
 	return err;
 }
 
diff -uprN linux-2.6.23.1.orig/ipc/shm.c linux-2.6.23.1/ipc/shm.c
--- linux-2.6.23.1.orig/ipc/shm.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/ipc/shm.c	2007-11-28 10:13:52.346352133 +0100
@@ -451,7 +451,7 @@ asmlinkage long sys_shmget (key_t key, s
 		shm_unlock(shp);
 	}
 	mutex_unlock(&shm_ids(ns).mutex);
-
+	trace_mark(ipc_shm_create, "id %d flags %d", err, shmflg);
 	return err;
 }
 
diff -uprN linux-2.6.23.1.orig/kernel/Kconfig.instrumentation linux-2.6.23.1/kernel/Kconfig.instrumentation
--- linux-2.6.23.1.orig/kernel/Kconfig.instrumentation	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/kernel/Kconfig.instrumentation	2007-11-28 10:13:52.350351299 +0100
@@ -0,0 +1,33 @@
+menu "Instrumentation Support"
+
+config KPROBES
+	bool "Kprobes (EXPERIMENTAL)"
+	depends on EXPERIMENTAL
+	help
+	  Kprobes allows you to trap at almost any kernel address and
+	  execute a callback function.  register_kprobe() establishes
+	  a probepoint and specifies the callback.  Kprobes is useful
+	  for kernel debugging, non-intrusive instrumentation and testing.
+	  If in doubt, say "N".
+
+config KPTRACE
+	tristate "KPTrace (EXPERIMENTAL)"
+	select KPROBES
+	select DEBUG_FS
+	select RELAY
+	select SYSFS
+	select KALLSYMS
+	help
+	  KPTrace is a Kprobes-based trace tool. It provides a full set
+	  of pre-defined tracepoints, plus a simple way to add and remove
+	  tracepoints at runtime.
+
+config MARKERS
+	bool "Activate markers"
+	help
+	  Place an empty function call at each marker site. Can be
+	  dynamically changed for a probe function.
+
+source "ltt/Kconfig"
+
+endmenu
diff -uprN linux-2.6.23.1.orig/kernel/Makefile linux-2.6.23.1/kernel/Makefile
--- linux-2.6.23.1.orig/kernel/Makefile	2007-11-28 10:17:05.089169684 +0100
+++ linux-2.6.23.1/kernel/Makefile	2007-11-28 10:13:52.354350466 +0100
@@ -52,6 +52,8 @@ obj-$(CONFIG_RELAY) += relay.o
 obj-$(CONFIG_SYSCTL) += utsname_sysctl.o
 obj-$(CONFIG_TASK_DELAY_ACCT) += delayacct.o
 obj-$(CONFIG_TASKSTATS) += taskstats.o tsacct.o
+obj-$(CONFIG_IMMEDIATE) += immediate.o
+obj-$(CONFIG_MARKERS) += marker.o
 
 ifneq ($(CONFIG_SCHED_NO_NO_OMIT_FRAME_POINTER),y)
 # According to Alan Modra <alan@linuxcare.com.au>, the -fno-omit-frame-pointer is
diff -uprN linux-2.6.23.1.orig/kernel/exit.c linux-2.6.23.1/kernel/exit.c
--- linux-2.6.23.1.orig/kernel/exit.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/exit.c	2007-11-28 10:13:52.347351925 +0100
@@ -178,6 +178,7 @@ repeat:
 	write_unlock_irq(&tasklist_lock);
 	proc_flush_task(p);
 	release_thread(p);
+	trace_mark(kernel_process_free, "pid %d", p->pid);
 	call_rcu(&p->rcu, delayed_put_task_struct);
 
 	p = leader;
@@ -973,6 +974,8 @@ fastcall NORET_TYPE void do_exit(long co
 
 	if (group_dead)
 		acct_process();
+	trace_mark(kernel_process_exit, "pid %d", tsk->pid);
+
 	exit_sem(tsk);
 	__exit_files(tsk);
 	__exit_fs(tsk);
@@ -1516,6 +1519,8 @@ static long do_wait(pid_t pid, int optio
 	int flag, retval;
 	int allowed, denied;
 
+	trace_mark(kernel_process_wait, "pid %d", pid);
+
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
 	/*
diff -uprN linux-2.6.23.1.orig/kernel/fork.c linux-2.6.23.1/kernel/fork.c
--- linux-2.6.23.1.orig/kernel/fork.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/fork.c	2007-11-28 10:13:52.348351716 +0100
@@ -69,6 +69,7 @@ int max_threads;		/* tunable limit on nr
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
 __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
+EXPORT_SYMBOL(tasklist_lock);
 
 int nr_processes(void)
 {
@@ -1206,6 +1207,15 @@ static struct task_struct *copy_process(
 			!cpu_online(task_cpu(p))))
 		set_task_cpu(p, smp_processor_id());
 
+	/*
+	 * The state of the parent's TIF_KTRACE flag may have changed
+	 * since it was copied in dup_task_struct() so we re-copy it here.
+	 */
+	if (test_thread_flag(TIF_KERNEL_TRACE))
+		set_tsk_thread_flag(p, TIF_KERNEL_TRACE);
+	else
+		clear_tsk_thread_flag(p, TIF_KERNEL_TRACE);
+
 	/* CLONE_PARENT re-uses the old parent */
 	if (clone_flags & (CLONE_PARENT|CLONE_THREAD))
 		p->real_parent = current->real_parent;
@@ -1389,6 +1399,10 @@ long do_fork(unsigned long clone_flags,
 	if (!IS_ERR(p)) {
 		struct completion vfork;
 
+		trace_mark(kernel_process_fork,
+			"parent_pid %d child_pid %d child_tgid %d",
+			current->pid, p->pid, p->tgid);
+
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
 			init_completion(&vfork);
diff -uprN linux-2.6.23.1.orig/kernel/immediate.c linux-2.6.23.1/kernel/immediate.c
--- linux-2.6.23.1.orig/kernel/immediate.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/kernel/immediate.c	2007-11-28 10:13:52.349351508 +0100
@@ -0,0 +1,92 @@
+/*
+ * Copyright (C) 2007 Mathieu Desnoyers
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/immediate.h>
+#include <linux/memory.h>
+
+extern const struct __immediate __start___immediate[];
+extern const struct __immediate __stop___immediate[];
+
+/*
+ * immediate_mutex nests inside module_mutex. immediate_mutex protects builtin
+ * immediates and module immediates.
+ */
+static DEFINE_MUTEX(immediate_mutex);
+
+/**
+ * immediate_update_range - Update immediate values in a range
+ * @begin: pointer to the beginning of the range
+ * @end: pointer to the end of the range
+ *
+ * Sets a range of immediates to a enabled state : set the enable bit.
+ */
+void immediate_update_range(const struct __immediate *begin,
+		const struct __immediate *end)
+{
+	const struct __immediate *iter;
+	int ret;
+
+	for (iter = begin; iter < end; iter++) {
+		mutex_lock(&immediate_mutex);
+		kernel_text_lock();
+		ret = arch_immediate_update(iter);
+		kernel_text_unlock();
+		if (ret)
+			printk(KERN_WARNING "Invalid immediate value. "
+					    "Variable at %p, "
+					    "instruction at %p, size %lu\n",
+					    (void *)iter->immediate,
+					    (void *)iter->var, iter->size);
+		mutex_unlock(&immediate_mutex);
+	}
+}
+EXPORT_SYMBOL_GPL(immediate_update_range);
+
+/**
+ * immediate_update - update all immediate values in the kernel
+ * @lock: should a module_mutex be taken ?
+ *
+ * Iterate on the kernel core and modules to update the immediate values.
+ */
+void core_immediate_update(void)
+{
+	/* Core kernel immediates */
+	immediate_update_range(__start___immediate, __stop___immediate);
+}
+EXPORT_SYMBOL_GPL(core_immediate_update);
+
+static void __init immediate_update_early_range(const struct __immediate *begin,
+		const struct __immediate *end)
+{
+	const struct __immediate *iter;
+
+	for (iter = begin; iter < end; iter++)
+		arch_immediate_update_early(iter);
+}
+
+/**
+ * immediate_update_early - Update immediate values at boot time
+ *
+ * Update the immediate values to the state of the variables they refer to. It
+ * is done before SMP is active, at the very beginning of start_kernel().
+ */
+void __init immediate_update_early(void)
+{
+	immediate_update_early_range(__start___immediate, __stop___immediate);
+}
diff -uprN linux-2.6.23.1.orig/kernel/irq/handle.c linux-2.6.23.1/kernel/irq/handle.c
--- linux-2.6.23.1.orig/kernel/irq/handle.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/irq/handle.c	2007-11-28 10:13:52.349351508 +0100
@@ -59,6 +59,7 @@ struct irq_desc irq_desc[NR_IRQS] __cach
 #endif
 	}
 };
+EXPORT_SYMBOL(irq_desc);
 
 /*
  * What should we do if we get a hw irq event on an illegal vector?
@@ -130,6 +131,10 @@ irqreturn_t handle_IRQ_event(unsigned in
 {
 	irqreturn_t ret, retval = IRQ_NONE;
 	unsigned int status = 0;
+	struct pt_regs *regs = get_irq_regs();
+
+	trace_mark(kernel_irq_entry, "irq_id %u kernel_mode %u", irq,
+		(regs)?(!user_mode(regs)):(1));
 
 	handle_dynamic_tick(action);
 
@@ -148,6 +153,8 @@ irqreturn_t handle_IRQ_event(unsigned in
 		add_interrupt_randomness(irq);
 	local_irq_disable();
 
+	trace_mark(kernel_irq_exit, MARK_NOARGS);
+
 	return retval;
 }
 
diff -uprN linux-2.6.23.1.orig/kernel/itimer.c linux-2.6.23.1/kernel/itimer.c
--- linux-2.6.23.1.orig/kernel/itimer.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/itimer.c	2007-11-28 10:13:52.350351299 +0100
@@ -132,6 +132,8 @@ enum hrtimer_restart it_real_fn(struct h
 	struct signal_struct *sig =
 	    container_of(timer, struct signal_struct, real_timer);
 
+	trace_mark(kernel_timer_itimer_expired, "pid %d", sig->tsk->pid);
+
 	send_group_sig_info(SIGALRM, SEND_SIG_PRIV, sig->tsk);
 
 	return HRTIMER_NORESTART;
@@ -157,6 +159,15 @@ int do_setitimer(int which, struct itime
 	    !timeval_valid(&value->it_interval))
 		return -EINVAL;
 
+	trace_mark(kernel_timer_itimer_set,
+			"which %d interval_sec %ld interval_usec %ld "
+			"value_sec %ld value_usec %ld",
+			which,
+			value->it_interval.tv_sec,
+			value->it_interval.tv_usec,
+			value->it_value.tv_sec,
+			value->it_value.tv_usec);
+
 	switch (which) {
 	case ITIMER_REAL:
 again:
diff -uprN linux-2.6.23.1.orig/kernel/kprobes.c linux-2.6.23.1/kernel/kprobes.c
--- linux-2.6.23.1.orig/kernel/kprobes.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/kprobes.c	2007-11-28 10:13:52.351351091 +0100
@@ -43,6 +43,7 @@
 #include <linux/seq_file.h>
 #include <linux/debugfs.h>
 #include <linux/kdebug.h>
+#include <linux/memory.h>
 
 #include <asm-generic/sections.h>
 #include <asm/cacheflush.h>
@@ -69,7 +70,7 @@ static atomic_t kprobe_count;
 /* NOTE: change this value only with kprobe_mutex held */
 static bool kprobe_enabled;
 
-DEFINE_MUTEX(kprobe_mutex);		/* Protects kprobe_table */
+static DEFINE_MUTEX(kprobe_mutex);	/* Protects kprobe_table */
 DEFINE_SPINLOCK(kretprobe_lock);	/* Protects kretprobe_inst_table */
 static DEFINE_PER_CPU(struct kprobe *, kprobe_instance) = NULL;
 
@@ -101,6 +102,10 @@ enum kprobe_slot_state {
 	SLOT_USED = 2,
 };
 
+/*
+ * Protects the kprobe_insn_pages list. Can nest into kprobe_mutex.
+ */
+static DEFINE_MUTEX(kprobe_insn_mutex);
 static struct hlist_head kprobe_insn_pages;
 static int kprobe_garbage_slots;
 static int collect_garbage_slots(void);
@@ -137,7 +142,9 @@ kprobe_opcode_t __kprobes *get_insn_slot
 {
 	struct kprobe_insn_page *kip;
 	struct hlist_node *pos;
+	kprobe_opcode_t *ret;
 
+	mutex_lock(&kprobe_insn_mutex);
  retry:
 	hlist_for_each_entry(kip, pos, &kprobe_insn_pages, hlist) {
 		if (kip->nused < INSNS_PER_PAGE) {
@@ -146,7 +153,8 @@ kprobe_opcode_t __kprobes *get_insn_slot
 				if (kip->slot_used[i] == SLOT_CLEAN) {
 					kip->slot_used[i] = SLOT_USED;
 					kip->nused++;
-					return kip->insns + (i * MAX_INSN_SIZE);
+					ret = kip->insns + (i * MAX_INSN_SIZE);
+					goto end;
 				}
 			}
 			/* Surprise!  No unused slots.  Fix kip->nused. */
@@ -160,8 +168,10 @@ kprobe_opcode_t __kprobes *get_insn_slot
 	}
 	/* All out of space.  Need to allocate a new page. Use slot 0. */
 	kip = kmalloc(sizeof(struct kprobe_insn_page), GFP_KERNEL);
-	if (!kip)
-		return NULL;
+	if (!kip) {
+		ret = NULL;
+		goto end;
+	}
 
 	/*
 	 * Use module_alloc so this page is within +/- 2GB of where the
@@ -171,7 +181,8 @@ kprobe_opcode_t __kprobes *get_insn_slot
 	kip->insns = module_alloc(PAGE_SIZE);
 	if (!kip->insns) {
 		kfree(kip);
-		return NULL;
+		ret = NULL;
+		goto end;
 	}
 	INIT_HLIST_NODE(&kip->hlist);
 	hlist_add_head(&kip->hlist, &kprobe_insn_pages);
@@ -179,7 +190,10 @@ kprobe_opcode_t __kprobes *get_insn_slot
 	kip->slot_used[0] = SLOT_USED;
 	kip->nused = 1;
 	kip->ngarbage = 0;
-	return kip->insns;
+	ret = kip->insns;
+end:
+	mutex_unlock(&kprobe_insn_mutex);
+	return ret;
 }
 
 /* Return 1 if all garbages are collected, otherwise 0. */
@@ -213,7 +227,7 @@ static int __kprobes collect_garbage_slo
 	struct kprobe_insn_page *kip;
 	struct hlist_node *pos, *next;
 
-	/* Ensure no-one is preepmted on the garbages */
+	/* Ensure no-one is preempted on the garbages */
 	if (check_safety() != 0)
 		return -EAGAIN;
 
@@ -237,6 +251,7 @@ void __kprobes free_insn_slot(kprobe_opc
 	struct kprobe_insn_page *kip;
 	struct hlist_node *pos;
 
+	mutex_lock(&kprobe_insn_mutex);
 	hlist_for_each_entry(kip, pos, &kprobe_insn_pages, hlist) {
 		if (kip->insns <= slot &&
 		    slot < kip->insns + (INSNS_PER_PAGE * MAX_INSN_SIZE)) {
@@ -253,6 +268,7 @@ void __kprobes free_insn_slot(kprobe_opc
 
 	if (dirty && ++kprobe_garbage_slots > INSNS_PER_PAGE)
 		collect_garbage_slots();
+	mutex_unlock(&kprobe_insn_mutex);
 }
 #endif
 
@@ -561,9 +577,10 @@ static int __kprobes __register_kprobe(s
 		goto out;
 	}
 
+	kernel_text_lock();
 	ret = arch_prepare_kprobe(p);
 	if (ret)
-		goto out;
+		goto out_unlock_text;
 
 	INIT_HLIST_NODE(&p->hlist);
 	hlist_add_head_rcu(&p->hlist,
@@ -576,6 +593,8 @@ static int __kprobes __register_kprobe(s
 
 		arch_arm_kprobe(p);
 	}
+out_unlock_text:
+	kernel_text_unlock();
 out:
 	mutex_unlock(&kprobe_mutex);
 
@@ -618,8 +637,11 @@ valid_p:
 		 * enabled - otherwise, the breakpoint would already have
 		 * been removed. We save on flushing icache.
 		 */
-		if (kprobe_enabled)
+		if (kprobe_enabled) {
+			kernel_text_lock();
 			arch_disarm_kprobe(p);
+			kernel_text_unlock();
+		}
 		hlist_del_rcu(&old_p->hlist);
 		cleanup_p = 1;
 	} else {
@@ -722,7 +744,6 @@ static int __kprobes pre_handler_kretpro
 		ri->rp = rp;
 		ri->task = current;
 		arch_prepare_kretprobe(ri, regs);
-
 		/* XXX(hch): why is there no hlist_move_head? */
 		hlist_del(&ri->uflist);
 		hlist_add_head(&ri->uflist, &ri->rp->used_instances);
@@ -930,8 +951,10 @@ static void __kprobes enable_all_kprobes
 
 	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
 		head = &kprobe_table[i];
+		kernel_text_lock();
 		hlist_for_each_entry_rcu(p, node, head, hlist)
 			arch_arm_kprobe(p);
+		kernel_text_unlock();
 	}
 
 	kprobe_enabled = true;
@@ -959,10 +982,12 @@ static void __kprobes disable_all_kprobe
 	printk(KERN_INFO "Kprobes globally disabled\n");
 	for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
 		head = &kprobe_table[i];
+		kernel_text_lock();
 		hlist_for_each_entry_rcu(p, node, head, hlist) {
 			if (!arch_trampoline_kprobe(p))
 				arch_disarm_kprobe(p);
 		}
+		kernel_text_unlock();
 	}
 
 	mutex_unlock(&kprobe_mutex);
diff -uprN linux-2.6.23.1.orig/kernel/kthread.c linux-2.6.23.1/kernel/kthread.c
--- linux-2.6.23.1.orig/kernel/kthread.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/kthread.c	2007-11-28 10:13:52.352350882 +0100
@@ -195,6 +195,8 @@ int kthread_stop(struct task_struct *k)
 	/* It could exit after stop_info.k set, but before wake_up_process. */
 	get_task_struct(k);
 
+	trace_mark(kernel_kthread_stop, "pid %d", k->pid);
+
 	/* Must init completion *before* thread sees kthread_stop_info.k */
 	init_completion(&kthread_stop_info.done);
 	smp_wmb();
@@ -210,6 +212,8 @@ int kthread_stop(struct task_struct *k)
 	ret = kthread_stop_info.err;
 	mutex_unlock(&kthread_stop_lock);
 
+	trace_mark(kernel_kthread_stop_ret, "ret %d", ret);
+
 	return ret;
 }
 EXPORT_SYMBOL(kthread_stop);
diff -uprN linux-2.6.23.1.orig/kernel/lockdep.c linux-2.6.23.1/kernel/lockdep.c
--- linux-2.6.23.1.orig/kernel/lockdep.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/lockdep.c	2007-11-28 10:13:52.353350674 +0100
@@ -2014,6 +2014,9 @@ void trace_hardirqs_on(void)
 	struct task_struct *curr = current;
 	unsigned long ip;
 
+	_trace_mark(locking_hardirqs_on, "ip #p%lu",
+		(unsigned long) __builtin_return_address(0));
+
 	if (unlikely(!debug_locks || current->lockdep_recursion))
 		return;
 
@@ -2061,6 +2064,9 @@ void trace_hardirqs_off(void)
 {
 	struct task_struct *curr = current;
 
+	_trace_mark(locking_hardirqs_off, "ip #p%lu",
+		(unsigned long) __builtin_return_address(0));
+
 	if (unlikely(!debug_locks || current->lockdep_recursion))
 		return;
 
@@ -2088,6 +2094,9 @@ void trace_softirqs_on(unsigned long ip)
 {
 	struct task_struct *curr = current;
 
+	_trace_mark(locking_softirqs_on, "ip #p%lu",
+		(unsigned long) __builtin_return_address(0));
+
 	if (unlikely(!debug_locks))
 		return;
 
@@ -2122,6 +2131,9 @@ void trace_softirqs_off(unsigned long ip
 {
 	struct task_struct *curr = current;
 
+	_trace_mark(locking_softirqs_off, "ip #p%lu",
+		(unsigned long) __builtin_return_address(0));
+
 	if (unlikely(!debug_locks))
 		return;
 
@@ -2358,6 +2370,10 @@ static int __lock_acquire(struct lockdep
 	int chain_head = 0;
 	u64 chain_key;
 
+	_trace_mark(locking_lock_acquire,
+		"ip #p%lu subclass %u lock %p trylock %d",
+		ip, subclass, lock, trylock);
+
 	if (!prove_locking)
 		check = 1;
 
@@ -2631,6 +2647,9 @@ __lock_release(struct lockdep_map *lock,
 {
 	struct task_struct *curr = current;
 
+	_trace_mark(locking_lock_release, "ip #p%lu lock %p nested %d",
+		ip, lock, nested);
+
 	if (!check_unlock(curr, lock, ip))
 		return;
 
diff -uprN linux-2.6.23.1.orig/kernel/marker.c linux-2.6.23.1/kernel/marker.c
--- linux-2.6.23.1.orig/kernel/marker.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/kernel/marker.c	2007-11-28 10:13:52.355350257 +0100
@@ -0,0 +1,586 @@
+/*
+ * Copyright (C) 2007 Mathieu Desnoyers
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/types.h>
+#include <linux/jhash.h>
+#include <linux/list.h>
+#include <linux/rcupdate.h>
+#include <linux/marker.h>
+#include <linux/err.h>
+#include <linux/immediate.h>
+
+extern struct marker __start___markers[];
+extern struct marker __stop___markers[];
+
+/*
+ * module_mutex nests inside markers_mutex. Markers mutex protects the builtin
+ * and module markers, the hash table and deferred_sync.
+ */
+static DEFINE_MUTEX(markers_mutex);
+
+/*
+ * Marker deferred synchronization.
+ * Upon marker probe_unregister, we delay call to synchronize_sched() to
+ * accelerate mass unregistration (only when there is no more reference to a
+ * given module do we call synchronize_sched()). However, we need to make sure
+ * every critical region has ended before we re-arm a marker that has been
+ * unregistered and then registered back with a different probe data.
+ */
+static int deferred_sync;
+
+/*
+ * Marker hash table, containing the active markers.
+ * Protected by module_mutex.
+ */
+#define MARKER_HASH_BITS 6
+#define MARKER_TABLE_SIZE (1 << MARKER_HASH_BITS)
+
+struct marker_entry {
+	struct hlist_node hlist;
+	char *format;
+	marker_probe_func *probe;
+	void *private;
+	int refcount;	/* Number of times armed. 0 if disarmed. */
+	char name[0];	/* Contains name'\0'format'\0' */
+};
+
+static struct hlist_head marker_table[MARKER_TABLE_SIZE];
+
+/**
+ * __mark_empty_function - Empty probe callback
+ * @mdata: pointer of type const struct marker
+ * @fmt: format string
+ * @...: variable argument list
+ *
+ * Empty callback provided as a probe to the markers. By providing this to a
+ * disabled marker, we make sure the  execution flow is always valid even
+ * though the function pointer change and the marker enabling are two distinct
+ * operations that modifies the execution flow of preemptible code.
+ */
+void __mark_empty_function(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+}
+EXPORT_SYMBOL_GPL(__mark_empty_function);
+
+/*
+ * Get marker if the marker is present in the marker hash table.
+ * Must be called with markers_mutex held.
+ * Returns NULL if not present.
+ */
+static struct marker_entry *get_marker(const char *name)
+{
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct marker_entry *e;
+	u32 hash = jhash(name, strlen(name), 0);
+
+	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
+	hlist_for_each_entry(e, node, head, hlist) {
+		if (!strcmp(name, e->name))
+			return e;
+	}
+	return NULL;
+}
+
+/*
+ * Add the marker to the marker hash table. Must be called with markers_mutex
+ * held.
+ */
+static int add_marker(const char *name, const char *format,
+	marker_probe_func *probe, void *private)
+{
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct marker_entry *e;
+	size_t name_len = strlen(name) + 1;
+	size_t format_len = 0;
+	u32 hash = jhash(name, name_len-1, 0);
+
+	if (format)
+		format_len = strlen(format) + 1;
+	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
+	hlist_for_each_entry(e, node, head, hlist) {
+		if (!strcmp(name, e->name)) {
+			printk(KERN_NOTICE
+				"Marker %s busy, probe %p already installed\n",
+				name, e->probe);
+			return -EBUSY;	/* Already there */
+		}
+	}
+	/*
+	 * Using kmalloc here to allocate a variable length element. Could
+	 * cause some memory fragmentation if overused.
+	 */
+	e = kmalloc(sizeof(struct marker_entry) + name_len + format_len,
+			GFP_KERNEL);
+	if (!e)
+		return -ENOMEM;
+	memcpy(&e->name[0], name, name_len);
+	if (format) {
+		e->format = &e->name[name_len];
+		memcpy(e->format, format, format_len);
+		trace_mark(core_marker_format, "name %s format %s",
+				e->name, e->format);
+	} else
+		e->format = NULL;
+	e->probe = probe;
+	e->private = private;
+	e->refcount = 0;
+	hlist_add_head(&e->hlist, head);
+	return 0;
+}
+
+/*
+ * Remove the marker from the marker hash table. Must be called with mutex_lock
+ * held.
+ */
+static void *remove_marker(const char *name)
+{
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct marker_entry *e;
+	int found = 0;
+	size_t len = strlen(name) + 1;
+	void *private = NULL;
+	u32 hash = jhash(name, len-1, 0);
+
+	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
+	hlist_for_each_entry(e, node, head, hlist) {
+		if (!strcmp(name, e->name)) {
+			found = 1;
+			break;
+		}
+	}
+	if (found) {
+		private = e->private;
+		hlist_del(&e->hlist);
+		kfree(e);
+	}
+	return private;
+}
+
+/*
+ * Set the mark_entry format to the format found in the element.
+ */
+static int marker_set_format(struct marker_entry **entry, const char *format)
+{
+	struct marker_entry *e;
+	size_t name_len = strlen((*entry)->name) + 1;
+	size_t format_len = strlen(format) + 1;
+
+	e = kmalloc(sizeof(struct marker_entry) + name_len + format_len,
+			GFP_KERNEL);
+	if (!e)
+		return -ENOMEM;
+	memcpy(&e->name[0], (*entry)->name, name_len);
+	e->format = &e->name[name_len];
+	memcpy(e->format, format, format_len);
+	e->probe = (*entry)->probe;
+	e->private = (*entry)->private;
+	e->refcount = (*entry)->refcount;
+	hlist_add_before(&e->hlist, &(*entry)->hlist);
+	hlist_del(&(*entry)->hlist);
+	kfree(*entry);
+	*entry = e;
+	trace_mark(core_marker_format, "name %s format %s",
+			e->name, e->format);
+	return 0;
+}
+
+/*
+ * Sets the probe callback corresponding to one marker.
+ */
+static int set_marker(struct marker_entry **entry, struct marker *elem)
+{
+	int ret;
+	WARN_ON(strcmp((*entry)->name, elem->name) != 0);
+
+	if ((*entry)->format) {
+		if (strcmp((*entry)->format, elem->format) != 0) {
+			printk(KERN_NOTICE
+				"Format mismatch for probe %s "
+				"(%s), marker (%s)\n",
+				(*entry)->name,
+				(*entry)->format,
+				elem->format);
+			return -EPERM;
+		}
+	} else {
+		ret = marker_set_format(entry, elem->format);
+		if (ret)
+			return ret;
+	}
+	elem->call = (*entry)->probe;
+	elem->private = (*entry)->private;
+	_immediate_set(elem->state, 1);
+	return 0;
+}
+
+/*
+ * Disable a marker and its probe callback.
+ * Note: only after a synchronize_sched() issued after setting elem->call to the
+ * empty function insures that the original callback is not used anymore. This
+ * insured by preemption disabling around the call site.
+ */
+static void disable_marker(struct marker *elem)
+{
+	_immediate_set(elem->state, 0);
+	elem->call = __mark_empty_function;
+	/*
+	 * Leave the private data and id there, because removal is racy and
+	 * should be done only after a synchronize_sched(). These are never used
+	 * until the next initialization anyway.
+	 */
+}
+
+/**
+ * marker_update_probe_range - Update a probe range
+ * @begin: beginning of the range
+ * @end: end of the range
+ * @probe_module: module address of the probe being updated
+ * @refcount: number of references left to the given probe_module (out)
+ *
+ * Updates the probe callback corresponding to a range of markers.
+ * Must be called with markers_mutex held.
+ */
+void marker_update_probe_range(struct marker *begin,
+	struct marker *end, struct module *probe_module,
+	int *refcount)
+{
+	struct marker *iter;
+	struct marker_entry *mark_entry;
+
+	for (iter = begin; iter < end; iter++) {
+		mark_entry = get_marker(iter->name);
+		if (mark_entry && mark_entry->refcount) {
+			set_marker(&mark_entry, iter);
+			/*
+			 * ignore error, continue
+			 */
+			if (probe_module)
+				if (probe_module ==
+			__module_text_address((unsigned long)mark_entry->probe))
+					(*refcount)++;
+		} else {
+			disable_marker(iter);
+		}
+	}
+}
+
+/*
+ * Update probes, removing the faulty probes.
+ * Issues a synchronize_sched() when no reference to the module passed
+ * as parameter is found in the probes so the probe module can be
+ * safely unloaded from now on.
+ *
+ * must hold markers_mutex
+ */
+static void __marker_update_probes(struct module *probe_module)
+{
+	int refcount = 0;
+
+	/* Core kernel markers */
+	marker_update_probe_range(__start___markers,
+			__stop___markers, probe_module, &refcount);
+	/* Markers in modules. */
+	module_update_markers(probe_module, &refcount);
+	if (probe_module && refcount == 0) {
+		synchronize_sched();
+		deferred_sync = 0;
+	}
+}
+
+/**
+ * marker_probe_register -  Connect a probe to a marker
+ * @name: marker name
+ * @format: format string
+ * @probe: probe handler
+ * @private: probe private data
+ *
+ * private data must be a valid allocated memory address, or NULL.
+ * Returns 0 if ok, error value on error.
+ */
+int marker_probe_register(const char *name, const char *format,
+			marker_probe_func *probe, void *private)
+{
+	struct marker_entry *entry;
+	int ret = 0;
+
+	mutex_lock(&markers_mutex);
+	entry = get_marker(name);
+	if (entry && entry->refcount) {
+		ret = -EBUSY;
+		goto end;
+	}
+	if (deferred_sync) {
+		synchronize_sched();
+		deferred_sync = 0;
+	}
+	ret = add_marker(name, format, probe, private);
+	if (ret)
+		goto end;
+	__marker_update_probes(NULL);
+end:
+	mutex_unlock(&markers_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(marker_probe_register);
+
+/**
+ * marker_probe_unregister -  Disconnect a probe from a marker
+ * @name: marker name
+ *
+ * Returns the private data given to marker_probe_register, or an ERR_PTR().
+ */
+void *marker_probe_unregister(const char *name)
+{
+	struct module *probe_module;
+	struct marker_entry *entry;
+	void *private;
+
+	mutex_lock(&markers_mutex);
+	entry = get_marker(name);
+	if (!entry) {
+		private = ERR_PTR(-ENOENT);
+		goto end;
+	}
+	entry->refcount = 0;
+	/* In what module is the probe handler ? */
+	probe_module = __module_text_address((unsigned long)entry->probe);
+	private = remove_marker(name);
+	deferred_sync = 1;
+	__marker_update_probes(probe_module);
+end:
+	mutex_unlock(&markers_mutex);
+	return private;
+}
+EXPORT_SYMBOL_GPL(marker_probe_unregister);
+
+/**
+ * marker_probe_unregister_private_data -  Disconnect a probe from a marker
+ * @private: probe private data
+ *
+ * Unregister a marker by providing the registered private data.
+ * Returns the private data given to marker_probe_register, or an ERR_PTR().
+ */
+void *marker_probe_unregister_private_data(void *private)
+{
+	struct module *probe_module;
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct marker_entry *entry;
+	int found = 0;
+	unsigned int i;
+
+	mutex_lock(&markers_mutex);
+	for (i = 0; i < MARKER_TABLE_SIZE; i++) {
+		head = &marker_table[i];
+		hlist_for_each_entry(entry, node, head, hlist) {
+			if (entry->private == private) {
+				found = 1;
+				goto iter_end;
+			}
+		}
+	}
+iter_end:
+	if (!found) {
+		private = ERR_PTR(-ENOENT);
+		goto end;
+	}
+	entry->refcount = 0;
+	/* In what module is the probe handler ? */
+	probe_module = __module_text_address((unsigned long)entry->probe);
+	private = remove_marker(entry->name);
+	deferred_sync = 1;
+	__marker_update_probes(probe_module);
+end:
+	mutex_unlock(&markers_mutex);
+	return private;
+}
+EXPORT_SYMBOL_GPL(marker_probe_unregister_private_data);
+
+/**
+ * marker_arm - Arm a marker
+ * @name: marker name
+ *
+ * Activate a marker. It keeps a reference count of the number of
+ * arming/disarming done.
+ * Returns 0 if ok, error value on error.
+ */
+int marker_arm(const char *name)
+{
+	struct marker_entry *entry;
+	int ret = 0;
+
+	mutex_lock(&markers_mutex);
+	entry = get_marker(name);
+	if (!entry) {
+		ret = -ENOENT;
+		goto end;
+	}
+	/*
+	 * Only need to update probes when refcount passes from 0 to 1.
+	 */
+	if (entry->refcount++)
+		goto end;
+end:
+	__marker_update_probes(NULL);
+	mutex_unlock(&markers_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(marker_arm);
+
+/**
+ * marker_disarm - Disarm a marker
+ * @name: marker name
+ *
+ * Disarm a marker. It keeps a reference count of the number of arming/disarming
+ * done.
+ * Returns 0 if ok, error value on error.
+ */
+int marker_disarm(const char *name)
+{
+	struct marker_entry *entry;
+	int ret = 0;
+
+	mutex_lock(&markers_mutex);
+	entry = get_marker(name);
+	if (!entry) {
+		ret = -ENOENT;
+		goto end;
+	}
+	/*
+	 * Only permit decrement refcount if higher than 0.
+	 * Do probe update only on 1 -> 0 transition.
+	 */
+	if (entry->refcount) {
+		if (--entry->refcount)
+			goto end;
+	} else {
+		ret = -EPERM;
+		goto end;
+	}
+end:
+	__marker_update_probes(NULL);
+	mutex_unlock(&markers_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(marker_disarm);
+
+/**
+ * marker_get_private_data - Get a marker's probe private data
+ * @name: marker name
+ *
+ * Returns the private data pointer, or an ERR_PTR.
+ * The private data pointer should _only_ be dereferenced if the caller is the
+ * owner of the data, or its content could vanish. This is mostly used to
+ * confirm that a caller is the owner of a registered probe.
+ */
+void *marker_get_private_data(const char *name)
+{
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct marker_entry *e;
+	size_t name_len = strlen(name) + 1;
+	u32 hash = jhash(name, name_len-1, 0);
+	int found = 0;
+
+	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
+	hlist_for_each_entry(e, node, head, hlist) {
+		if (!strcmp(name, e->name)) {
+			found = 1;
+			return e->private;
+		}
+	}
+	return ERR_PTR(-ENOENT);
+}
+EXPORT_SYMBOL_GPL(marker_get_private_data);
+
+/**
+ * marker_get_iter_range - Get a next marker iterator given a range.
+ * @marker: current markers (in), next marker (out)
+ * @begin: beginning of the range
+ * @end: end of the range
+ *
+ * Returns whether a next marker has been found (1) or not (0).
+ * Will return the first marker in the range if the input marker is NULL.
+ */
+int marker_get_iter_range(struct marker **marker, struct marker *begin,
+	struct marker *end)
+{
+	if (!*marker && begin != end) {
+		*marker = begin;
+		return 1;
+	}
+	if (*marker >= begin && *marker < end)
+		return 1;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(marker_get_iter_range);
+
+static void marker_get_iter(struct marker_iter *iter)
+{
+	int found = 0;
+
+	/* Core kernel markers */
+	if (!iter->module) {
+		found = marker_get_iter_range(&iter->marker,
+				__start___markers, __stop___markers);
+		if (found)
+			goto end;
+	}
+	/* Markers in modules. */
+	found = module_get_iter_markers(iter);
+end:
+	if (!found)
+		marker_iter_reset(iter);
+}
+
+void marker_iter_start(struct marker_iter *iter)
+{
+	mutex_lock(&markers_mutex);
+	marker_get_iter(iter);
+}
+EXPORT_SYMBOL_GPL(marker_iter_start);
+
+void marker_iter_next(struct marker_iter *iter)
+{
+	iter->marker++;
+	/*
+	 * iter->marker may be invalid because we blindly incremented it.
+	 * Make sure it is valid by marshalling on the markers, getting the
+	 * markers from following modules if necessary.
+	 */
+	marker_get_iter(iter);
+}
+EXPORT_SYMBOL_GPL(marker_iter_next);
+
+void marker_iter_stop(struct marker_iter *iter)
+{
+	mutex_unlock(&markers_mutex);
+}
+EXPORT_SYMBOL_GPL(marker_iter_stop);
+
+void marker_iter_reset(struct marker_iter *iter)
+{
+	iter->module = NULL;
+	iter->marker = NULL;
+}
+EXPORT_SYMBOL_GPL(marker_iter_reset);
diff -uprN linux-2.6.23.1.orig/kernel/module.c linux-2.6.23.1/kernel/module.c
--- linux-2.6.23.1.orig/kernel/module.c	2007-11-28 10:17:05.103166766 +0100
+++ linux-2.6.23.1/kernel/module.c	2007-11-28 10:13:52.356350049 +0100
@@ -32,6 +32,7 @@
 #include <linux/cpu.h>
 #include <linux/moduleparam.h>
 #include <linux/errno.h>
+#include <linux/immediate.h>
 #include <linux/err.h>
 #include <linux/vermagic.h>
 #include <linux/notifier.h>
@@ -62,7 +63,8 @@ extern int module_sysfs_initialized;
 #define INIT_OFFSET_MASK (1UL << (BITS_PER_LONG-1))
 
 /* List of modules, protected by module_mutex or preempt_disable
- * (add/delete uses stop_machine). */
+ * (add/delete uses stop_machine). Sorted by ascending list node address.
+ */
 static DEFINE_MUTEX(module_mutex);
 static LIST_HEAD(modules);
 static DECLARE_MUTEX(notify_mutex);
@@ -1210,6 +1212,8 @@ static int __unlink_module(void *_mod)
 /* Free a module, remove from lists, etc (must hold module_mutex). */
 static void free_module(struct module *mod)
 {
+	trace_mark(kernel_module_free, "name %s", mod->name);
+
 	/* Delete from various lists */
 	stop_machine_run(__unlink_module, mod, NR_CPUS);
 	remove_sect_attrs(mod);
@@ -1618,6 +1622,9 @@ static struct module *load_module(void _
 	unsigned int unusedcrcindex;
 	unsigned int unusedgplindex;
 	unsigned int unusedgplcrcindex;
+	unsigned int immediateindex;
+	unsigned int markersindex;
+	unsigned int markersstringsindex;
 	struct module *mod;
 	long err = 0;
 	void *percpu = NULL, *ptr = NULL; /* Stops spurious gcc warning */
@@ -1714,6 +1721,7 @@ static struct module *load_module(void _
 #ifdef ARCH_UNWIND_SECTION_NAME
 	unwindex = find_sec(hdr, sechdrs, secstrings, ARCH_UNWIND_SECTION_NAME);
 #endif
+	immediateindex = find_sec(hdr, sechdrs, secstrings, "__immediate");
 
 	/* Don't keep modinfo section */
 	sechdrs[infoindex].sh_flags &= ~(unsigned long)SHF_ALLOC;
@@ -1864,6 +1872,11 @@ static struct module *load_module(void _
 	mod->gpl_future_syms = (void *)sechdrs[gplfutureindex].sh_addr;
 	if (gplfuturecrcindex)
 		mod->gpl_future_crcs = (void *)sechdrs[gplfuturecrcindex].sh_addr;
+#ifdef CONFIG_IMMEDIATE
+	mod->immediate = (void *)sechdrs[immediateindex].sh_addr;
+	mod->num_immediate =
+		sechdrs[immediateindex].sh_size / sizeof(*mod->immediate);
+#endif
 
 	mod->unused_syms = (void *)sechdrs[unusedindex].sh_addr;
 	if (unusedcrcindex)
@@ -1883,6 +1896,9 @@ static struct module *load_module(void _
 		add_taint_module(mod, TAINT_FORCED_MODULE);
 	}
 #endif
+	markersindex = find_sec(hdr, sechdrs, secstrings, "__markers");
+ 	markersstringsindex = find_sec(hdr, sechdrs, secstrings,
+					"__markers_strings");
 
 	/* Now do relocations. */
 	for (i = 1; i < hdr->e_shnum; i++) {
@@ -1905,6 +1921,11 @@ static struct module *load_module(void _
 		if (err < 0)
 			goto cleanup;
 	}
+#ifdef CONFIG_MARKERS
+	mod->markers = (void *)sechdrs[markersindex].sh_addr;
+	mod->num_markers =
+		sechdrs[markersindex].sh_size / sizeof(*mod->markers);
+#endif
 
         /* Find duplicate symbols */
 	err = verify_export_symbols(mod);
@@ -1928,6 +1949,16 @@ static struct module *load_module(void _
 	 if (err < 0)
 		 goto nomodsectinfo;
 #endif
+	if (!mod->taints) {
+#ifdef CONFIG_IMMEDIATE
+	immediate_update_range(mod->immediate,
+		mod->immediate + mod->num_immediate);
+#endif
+#ifdef CONFIG_MARKERS
+		marker_update_probe_range(mod->markers,
+			mod->markers + mod->num_markers, NULL, NULL);
+#endif
+	}
 
 	err = module_finalize(hdr, sechdrs, mod);
 	if (err < 0)
@@ -1983,6 +2014,8 @@ static struct module *load_module(void _
 	/* Get rid of temporary copy */
 	vfree(hdr);
 
+	trace_mark(kernel_module_load, "name %s", mod->name);
+
 	/* Done! */
 	return mod;
 
@@ -2016,10 +2049,24 @@ nomodsectinfo:
 /*
  * link the module with the whole machine is stopped with interrupts off
  * - this defends against kallsyms not taking locks
+ * We sort the modules by struct module pointer address to permit correct
+ * iteration over modules of, at least, kallsyms for preemptible operations,
+ * such as read(). Sorting by struct module pointer address is equivalent to
+ * sort by list node address.
  */
 static int __link_module(void *_mod)
 {
-	struct module *mod = _mod;
+	struct module *mod = _mod, *iter;
+
+	list_for_each_entry_reverse(iter, &modules, list) {
+		BUG_ON(iter == mod);	/* Should never be in the list twice */
+		if (iter < mod) {
+			/* We belong to the location right after iter. */
+			list_add(&mod->list, &iter->list);
+			return 0;
+		}
+	}
+	/* We should be added at the head of the list */
 	list_add(&mod->list, &modules);
 	return 0;
 }
@@ -2289,12 +2336,12 @@ unsigned long module_kallsyms_lookup_nam
 static void *m_start(struct seq_file *m, loff_t *pos)
 {
 	mutex_lock(&module_mutex);
-	return seq_list_start(&modules, *pos);
+	return seq_sorted_list_start(&modules, pos);
 }
 
 static void *m_next(struct seq_file *m, void *p, loff_t *pos)
 {
-	return seq_list_next(p, &modules, pos);
+	return seq_sorted_list_next(p, &modules, pos);
 }
 
 static void m_stop(struct seq_file *m, void *p)
@@ -2361,6 +2408,27 @@ const struct seq_operations modules_op =
 	.show	= m_show
 };
 
+void list_modules(void *call_data)
+{
+	/* Enumerate loaded modules */
+	struct list_head	*i;
+	struct module		*mod;
+	unsigned long refcount = 0;
+
+	mutex_lock(&module_mutex);
+	list_for_each(i, &modules) {
+		mod = list_entry(i, struct module, list);
+#ifdef CONFIG_MODULE_UNLOAD
+		refcount = local_read(&mod->ref[0].count);
+#endif
+		__trace_mark(0, list_module, call_data,
+				"name %s state %d refcount %lu",
+				mod->name, mod->state, refcount);
+	}
+	mutex_unlock(&module_mutex);
+}
+EXPORT_SYMBOL_GPL(list_modules);
+
 /* Given an address, look for it in the module exception tables. */
 const struct exception_table_entry *search_module_extables(unsigned long addr)
 {
@@ -2534,3 +2602,85 @@ EXPORT_SYMBOL(module_remove_driver);
 void struct_module(struct module *mod) { return; }
 EXPORT_SYMBOL(struct_module);
 #endif
+
+#ifdef CONFIG_MARKERS
+void module_update_markers(struct module *probe_module, int *refcount)
+{
+	struct module *mod;
+
+	mutex_lock(&module_mutex);
+	list_for_each_entry(mod, &modules, list)
+		if (!mod->taints)
+			marker_update_probe_range(mod->markers,
+				mod->markers + mod->num_markers,
+				probe_module, refcount);
+	mutex_unlock(&module_mutex);
+}
+
+/*
+ * Returns 0 if current not found.
+ * Returns 1 if current found.
+ */
+int module_get_iter_markers(struct marker_iter *iter)
+{
+	struct module *iter_mod;
+	int found = 0;
+
+	mutex_lock(&module_mutex);
+	list_for_each_entry(iter_mod, &modules, list) {
+		if (!iter_mod->taints) {
+			/*
+			 * Sorted module list
+			 */
+			if (iter_mod < iter->module)
+				continue;
+			else if (iter_mod > iter->module)
+				iter->marker = NULL;
+			found = marker_get_iter_range(&iter->marker,
+				iter_mod->markers,
+				iter_mod->markers + iter_mod->num_markers);
+			if (found) {
+				iter->module = iter_mod;
+				break;
+			}
+		}
+	}
+	mutex_unlock(&module_mutex);
+	return found;
+}
+#endif
+
+#ifdef CONFIG_IMMEDIATE
+/**
+ * _module_immediate_update - update all immediate values in the kernel
+ *
+ * Iterate on the kernel core and modules to update the immediate values.
+ * Module_mutex must be held be the caller.
+ */
+void _module_immediate_update(void)
+{
+	struct module *mod;
+
+	list_for_each_entry(mod, &modules, list) {
+		if (mod->taints)
+			continue;
+		immediate_update_range(mod->immediate,
+			mod->immediate + mod->num_immediate);
+	}
+}
+EXPORT_SYMBOL_GPL(_module_immediate_update);
+
+/**
+ * module_immediate_update - update all immediate values in the kernel
+ *
+ * Iterate on the kernel core and modules to update the immediate values.
+ * Takes module_mutex.
+ */
+void module_immediate_update(void)
+{
+	mutex_lock(&module_mutex);
+	_module_immediate_update();
+	mutex_unlock(&module_mutex);
+}
+EXPORT_SYMBOL_GPL(module_immediate_update);
+#endif
diff -uprN linux-2.6.23.1.orig/kernel/printk.c linux-2.6.23.1/kernel/printk.c
--- linux-2.6.23.1.orig/kernel/printk.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/printk.c	2007-11-28 10:13:52.358349632 +0100
@@ -510,6 +510,7 @@ asmlinkage int printk(const char *fmt, .
 	int r;
 
 	va_start(args, fmt);
+	trace_mark(kernel_printk, "ip %p", __builtin_return_address(0));
 	r = vprintk(fmt, args);
 	va_end(args);
 
@@ -542,6 +543,31 @@ asmlinkage int vprintk(const char *fmt, 
 	/* Emit the output into the temporary buffer */
 	printed_len = vscnprintf(printk_buf, sizeof(printk_buf), fmt, args);
 
+	if (printed_len > 0) {
+		unsigned int loglevel;
+		int mark_len;
+		char *mark_buf;
+		char saved_char;
+
+		if (printk_buf[0] == '<' && printk_buf[1] >= '0' &&
+		   printk_buf[1] <= '7' && printk_buf[2] == '>') {
+			loglevel = printk_buf[1] - '0';
+			mark_buf = &printk_buf[3];
+			mark_len = printed_len - 3;
+		} else {
+			loglevel = default_message_loglevel;
+			mark_buf = printk_buf;
+			mark_len = printed_len;
+		}
+		if (mark_buf[mark_len - 1] == '\n')
+			mark_len--;
+		saved_char = mark_buf[mark_len];
+		mark_buf[mark_len] = '\0';
+		_trace_mark(kernel_vprintk, "loglevel %c string %s ip %p",
+			loglevel, mark_buf, __builtin_return_address(0));
+		mark_buf[mark_len] = saved_char;
+	}
+
 	/*
 	 * Copy the output into log_buf.  If the caller didn't provide
 	 * appropriate log level tags, we insert them here
diff -uprN linux-2.6.23.1.orig/kernel/profile.c linux-2.6.23.1/kernel/profile.c
--- linux-2.6.23.1.orig/kernel/profile.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/profile.c	2007-11-28 10:13:52.358349632 +0100
@@ -42,8 +42,8 @@ int (*timer_hook)(struct pt_regs *) __re
 static atomic_t *prof_buffer;
 static unsigned long prof_len, prof_shift;
 
-int prof_on __read_mostly;
-EXPORT_SYMBOL_GPL(prof_on);
+DEFINE_IMMEDIATE(char, prof_on) __read_mostly;
+EXPORT_IMMEDIATE_SYMBOL_GPL(prof_on);
 
 static cpumask_t prof_cpu_mask = CPU_MASK_ALL;
 #ifdef CONFIG_SMP
@@ -60,7 +60,7 @@ static int __init profile_setup(char * s
 	int par;
 
 	if (!strncmp(str, sleepstr, strlen(sleepstr))) {
-		prof_on = SLEEP_PROFILING;
+		immediate_set_early(prof_on, SLEEP_PROFILING);
 		if (str[strlen(sleepstr)] == ',')
 			str += strlen(sleepstr) + 1;
 		if (get_option(&str, &par))
@@ -69,7 +69,7 @@ static int __init profile_setup(char * s
 			"kernel sleep profiling enabled (shift: %ld)\n",
 			prof_shift);
 	} else if (!strncmp(str, schedstr, strlen(schedstr))) {
-		prof_on = SCHED_PROFILING;
+		immediate_set_early(prof_on, SCHED_PROFILING);
 		if (str[strlen(schedstr)] == ',')
 			str += strlen(schedstr) + 1;
 		if (get_option(&str, &par))
@@ -78,7 +78,7 @@ static int __init profile_setup(char * s
 			"kernel schedule profiling enabled (shift: %ld)\n",
 			prof_shift);
 	} else if (!strncmp(str, kvmstr, strlen(kvmstr))) {
-		prof_on = KVM_PROFILING;
+		immediate_set_early(prof_on, KVM_PROFILING);
 		if (str[strlen(kvmstr)] == ',')
 			str += strlen(kvmstr) + 1;
 		if (get_option(&str, &par))
@@ -88,7 +88,7 @@ static int __init profile_setup(char * s
 			prof_shift);
 	} else if (get_option(&str, &par)) {
 		prof_shift = par;
-		prof_on = CPU_PROFILING;
+		immediate_set_early(prof_on, CPU_PROFILING);
 		printk(KERN_INFO "kernel profiling enabled (shift: %ld)\n",
 			prof_shift);
 	}
@@ -99,7 +99,7 @@ __setup("profile=", profile_setup);
 
 void __init profile_init(void)
 {
-	if (!prof_on) 
+	if (!_immediate_read(prof_on))
 		return;
  
 	/* only text is profiled */
@@ -288,7 +288,7 @@ void profile_hits(int type, void *__pc, 
 	int i, j, cpu;
 	struct profile_hit *hits;
 
-	if (prof_on != type || !prof_buffer)
+	if (!prof_buffer)
 		return;
 	pc = min((pc - (unsigned long)_stext) >> prof_shift, prof_len - 1);
 	i = primary = (pc & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
@@ -398,7 +398,7 @@ void profile_hits(int type, void *__pc, 
 {
 	unsigned long pc;
 
-	if (prof_on != type || !prof_buffer)
+	if (!prof_buffer)
 		return;
 	pc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;
 	atomic_add(nr_hits, &prof_buffer[min(pc, prof_len - 1)]);
@@ -555,7 +555,7 @@ static int __init create_hash_tables(voi
 	}
 	return 0;
 out_cleanup:
-	prof_on = 0;
+	immediate_set_early(prof_on, 0);
 	smp_mb();
 	on_each_cpu(profile_nop, NULL, 0, 1);
 	for_each_online_cpu(cpu) {
@@ -582,7 +582,7 @@ static int __init create_proc_profile(vo
 {
 	struct proc_dir_entry *entry;
 
-	if (!prof_on)
+	if (!_immediate_read(prof_on))
 		return 0;
 	if (create_hash_tables())
 		return -1;
diff -uprN linux-2.6.23.1.orig/kernel/relay.c linux-2.6.23.1/kernel/relay.c
--- linux-2.6.23.1.orig/kernel/relay.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/relay.c	2007-11-28 10:13:52.359349423 +0100
@@ -1202,8 +1202,7 @@ EXPORT_SYMBOL_GPL(relay_file_operations)
 
 static __init int relay_init(void)
 {
-
-	hotcpu_notifier(relay_hotcpu_callback, 0);
+	hotcpu_notifier(relay_hotcpu_callback, 5);
 	return 0;
 }
 
diff -uprN linux-2.6.23.1.orig/kernel/sched.c linux-2.6.23.1/kernel/sched.c
--- linux-2.6.23.1.orig/kernel/sched.c	2007-11-28 10:17:05.095168433 +0100
+++ linux-2.6.23.1/kernel/sched.c	2007-11-28 10:13:52.362348798 +0100
@@ -1436,6 +1436,8 @@ static int try_to_wake_up(struct task_st
 #endif
 
 	rq = task_rq_lock(p, &flags);
+	trace_mark(kernel_sched_try_wakeup, "pid %d state %ld",
+		p->pid, p->state);
 	old_state = p->state;
 	if (!(old_state & state))
 		goto out;
@@ -1677,6 +1679,8 @@ void fastcall wake_up_new_task(struct ta
 	int this_cpu;
 
 	rq = task_rq_lock(p, &flags);
+	trace_mark(kernel_sched_wakeup_new_task, "pid %d state %ld",
+		p->pid, p->state);
 	BUG_ON(p->state != TASK_RUNNING);
 	this_cpu = smp_processor_id(); /* parent's CPU */
 	update_rq_clock(rq);
@@ -1864,6 +1868,9 @@ context_switch(struct rq *rq, struct tas
 	struct mm_struct *mm, *oldmm;
 
 	prepare_task_switch(rq, prev, next);
+	trace_mark(kernel_sched_schedule,
+		"prev_pid %d next_pid %d prev_state %ld",
+		prev->pid, next->pid, prev->state);
 	mm = next->mm;
 	oldmm = prev->active_mm;
 	/*
@@ -2118,6 +2125,8 @@ static void sched_migrate_task(struct ta
 	    || unlikely(cpu_is_offline(dest_cpu)))
 		goto out;
 
+	trace_mark(kernel_sched_migrate_task, "pid %d state %ld dest_cpu %d",
+		p->pid, p->state, dest_cpu);
 	/* force the process onto the specified CPU */
 	if (migrate_task(p, dest_cpu, &req)) {
 		/* Need to wait for migration thread (might exit: take ref). */
@@ -6743,3 +6752,45 @@ void set_curr_task(int cpu, struct task_
 }
 
 #endif
+
+/**
+ * clear_kernel_trace_flag_all_tasks - clears all TIF_KERNEL_TRACE thread flags.
+ *
+ * This function iterates on all threads in the system to clear their
+ * TIF_KERNEL_TRACE flag. Setting the TIF_KERNEL_TRACE flag with the
+ * tasklist_lock held in copy_process() makes sure that once we finish clearing
+ * the thread flags, all threads have their flags cleared.
+ */
+void clear_kernel_trace_flag_all_tasks(void)
+{
+	struct task_struct *p;
+	struct task_struct *t;
+
+	read_lock(&tasklist_lock);
+	do_each_thread(p, t) {
+		clear_tsk_thread_flag(t, TIF_KERNEL_TRACE);
+	} while_each_thread(p, t);
+	read_unlock(&tasklist_lock);
+}
+EXPORT_SYMBOL_GPL(clear_kernel_trace_flag_all_tasks);
+
+/**
+ * set_kernel_trace_flag_all_tasks - sets all TIF_KERNEL_TRACE thread flags.
+ *
+ * This function iterates on all threads in the system to set their
+ * TIF_KERNEL_TRACE flag. Setting the TIF_KERNEL_TRACE flag with the
+ * tasklist_lock held in copy_process() makes sure that once we finish setting
+ * the thread flags, all threads have their flags set.
+ */
+void set_kernel_trace_flag_all_tasks(void)
+{
+	struct task_struct *p;
+	struct task_struct *t;
+
+	read_lock(&tasklist_lock);
+	do_each_thread(p, t) {
+		set_tsk_thread_flag(t, TIF_KERNEL_TRACE);
+	} while_each_thread(p, t);
+	read_unlock(&tasklist_lock);
+}
+EXPORT_SYMBOL_GPL(set_kernel_trace_flag_all_tasks);
diff -uprN linux-2.6.23.1.orig/kernel/signal.c linux-2.6.23.1/kernel/signal.c
--- linux-2.6.23.1.orig/kernel/signal.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/signal.c	2007-11-28 10:13:52.364348381 +0100
@@ -664,6 +664,8 @@ static int send_signal(int sig, struct s
 	struct sigqueue * q = NULL;
 	int ret = 0;
 
+	trace_mark(kernel_send_signal, "pid %d signal %d", t->pid, sig);
+
 	/*
 	 * Deliver the signal to listening signalfds. This must be called
 	 * with the sighand lock held.
diff -uprN linux-2.6.23.1.orig/kernel/softirq.c linux-2.6.23.1/kernel/softirq.c
--- linux-2.6.23.1.orig/kernel/softirq.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/softirq.c	2007-11-28 10:13:52.366347964 +0100
@@ -229,7 +229,15 @@ restart:
 
 	do {
 		if (pending & 1) {
+			trace_mark(kernel_softirq_entry, "softirq_id %lu",
+				((unsigned long)h
+					- (unsigned long)softirq_vec)
+					/ sizeof(*h));
 			h->action(h);
+			trace_mark(kernel_softirq_exit, "softirq_id %lu",
+				((unsigned long)h
+					- (unsigned long)softirq_vec)
+					/ sizeof(*h));
 			rcu_bh_qsctr_inc(cpu);
 		}
 		h++;
@@ -317,6 +325,8 @@ void irq_exit(void)
  */
 inline fastcall void raise_softirq_irqoff(unsigned int nr)
 {
+	trace_mark(kernel_softirq_raise, "softirq %u", nr);
+
 	__raise_softirq_irqoff(nr);
 
 	/*
@@ -404,7 +414,13 @@ static void tasklet_action(struct softir
 			if (!atomic_read(&t->count)) {
 				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
 					BUG();
+				trace_mark(kernel_tasklet_low_entry,
+						"func %p data %lu",
+						t->func, t->data);
 				t->func(t->data);
+				trace_mark(kernel_tasklet_low_exit,
+						"func %p data %lu",
+						t->func, t->data);
 				tasklet_unlock(t);
 				continue;
 			}
@@ -437,7 +453,13 @@ static void tasklet_hi_action(struct sof
 			if (!atomic_read(&t->count)) {
 				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
 					BUG();
+				trace_mark(kernel_tasklet_high_entry,
+						"func %p data %lu",
+						t->func, t->data);
 				t->func(t->data);
+				trace_mark(kernel_tasklet_high_exit,
+						"func %p data %lu",
+						t->func, t->data);
 				tasklet_unlock(t);
 				continue;
 			}
diff -uprN linux-2.6.23.1.orig/kernel/timer.c linux-2.6.23.1/kernel/timer.c
--- linux-2.6.23.1.orig/kernel/timer.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/kernel/timer.c	2007-11-28 10:13:52.367347756 +0100
@@ -36,12 +36,14 @@
 #include <linux/delay.h>
 #include <linux/tick.h>
 #include <linux/kallsyms.h>
+#include <linux/ltt.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
 #include <asm/div64.h>
 #include <asm/timex.h>
 #include <asm/io.h>
+#include <asm/irq_regs.h>
 
 u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;
 
@@ -289,6 +291,8 @@ static void internal_add_timer(tvec_base
 		i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
 		vec = base->tv5.vec + i;
 	}
+	trace_mark(kernel_timer_set, "expires %lu function %p data %lu",
+		expires, timer->function, timer->data);
 	/*
 	 * Timers are FIFO:
 	 */
@@ -920,6 +924,12 @@ void do_timer(unsigned long ticks)
 {
 	jiffies_64 += ticks;
 	update_times(ticks);
+	ltt_add_timestamp(ticks);
+	trace_mark(kernel_timer_update_time,
+		"jiffies #8u%llu xtime_sec %ld xtime_nsec %ld "
+		"walltomonotonic_sec %ld walltomonotonic_nsec %ld",
+		jiffies_64, xtime.tv_sec, xtime.tv_nsec,
+		wall_to_monotonic.tv_sec, wall_to_monotonic.tv_nsec);
 }
 
 #ifdef __ARCH_WANT_SYS_ALARM
@@ -1001,7 +1011,9 @@ asmlinkage long sys_getegid(void)
 
 static void process_timeout(unsigned long __data)
 {
-	wake_up_process((struct task_struct *)__data);
+	struct task_struct *task = (struct task_struct *)__data;
+	trace_mark(kernel_timer_timeout, "pid %d", task->pid);
+	wake_up_process(task);
 }
 
 /**
diff -uprN linux-2.6.23.1.orig/ltt/Kconfig linux-2.6.23.1/ltt/Kconfig
--- linux-2.6.23.1.orig/ltt/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/Kconfig	2007-11-28 10:13:52.368347547 +0100
@@ -0,0 +1,207 @@
+config LTT_TIMESTAMP
+	bool "LTTng fine-grained timestamping"
+	default y
+	help
+	  Allow fine-grained timestamps to be taken from tracing applications.
+
+config LTT
+	bool "Linux Trace Toolkit Instrumentation Support"
+	depends on EXPERIMENTAL
+	depends on MARKERS
+	depends on LTT_TIMESTAMP
+	default n
+	help
+	  It is possible for the kernel to log important events to a trace
+	  facility. Doing so enables the use of the generated traces in order
+	  to reconstruct the dynamic behavior of the kernel, and hence the
+	  whole system.
+
+	  The tracing process contains 4 parts :
+	      1) The logging of events by key parts of the kernel.
+	      2) The tracer that keeps the events in a data buffer (uses
+	         relay).
+	      3) A trace daemon that interacts with the tracer and is
+	         notified every time there is a certain quantity of data to
+	         read from the tracer.
+	      4) A trace event data decoder that reads the accumulated data
+	         and formats it in a human-readable format.
+
+	  If you say Y, the first component will be built into the kernel.
+
+	  For more information on kernel tracing, the trace daemon or the event
+	  decoder, please check the following address :
+	       http://www.opersys.com/ltt
+	  See also the experimental page of the project :
+	       http://ltt.polymtl.ca
+
+config LTT_RELAY
+	tristate "Linux Trace Toolkit Relay+DebugFS Support"
+	select RELAY
+	select DEBUG_FS
+	depends on LTT_TRACER
+	default y
+	help
+	  Support using relay and debugfs to log the data obtained through LTT.
+
+	  If you don't have special hardware, you almost certainly want
+	  to say Y here.
+
+config LTT_SERIALIZE
+	tristate "Linux Trace Toolkit Serializer"
+	default y
+	depends on LTT
+	help
+	  Library for serializing information from format string and argument
+	  list to the trace buffers.
+
+config LTT_MARKER_CONTROL
+	tristate "Linux Trace Toolkit Marker Control"
+	depends on LTT_SERIALIZE
+	default y
+	help
+	  Interface (/proc/ltt) to enable markers.
+
+config LTT_TRACER
+	tristate "Linux Trace Toolkit Tracer"
+	depends on LTT
+	depends on LTT_MARKER_CONTROL
+	default y
+	help
+	  If you enable this option, the Linux Trace Toolkit Tracer will be
+	  either built in the kernel or as module.
+
+	  Critical parts of the kernel will call upon the kernel tracing
+	  function. The data is then recorded by the tracer if a trace daemon
+	  is running in user-space and has issued a "start" command.
+
+	  For more information on kernel tracing, the trace daemon or the event
+	  decoder, please check the following address :
+	       http://www.opersys.com/ltt
+	  See also the experimental page of the project :
+	       http://ltt.polymtl.ca
+
+config LTT_ALIGNMENT
+	bool "Align Linux Trace Toolkit Traces"
+	depends on LTT
+	default y
+	help
+	  This option enables dynamic alignment of data in buffers. The
+	  alignment is made on the smallest size between architecture size
+	  and the size of the value to be written.
+
+	  Dynamically calculating the offset of the data has a performance cost,
+	  but it is more efficient on some architectures (especially 64 bits) to
+	  align data than to write it unaligned.
+
+config LTT_VMCORE
+	bool "Support trace extraction from crash dump"
+	depends on LTT
+	default y
+	help
+	  If you enable this option, the Linux Trace Toolkit Tracer will
+	  support extacting ltt log from vmcore, which can be generated with
+	  kdump or LKCD tools.
+
+	  Special crash extension should be used to extract ltt buffers.
+
+config LTT_HEARTBEAT
+	bool "Write heartbeat event to shrink traces (EXPERIMENTAL)"
+	depends on EXPERIMENTAL
+	depends on LTT
+	default n
+	help
+	  The Linux Trace Toolkit Heartbeat Timer fires at an interval small
+	  enough to guarantee that the 32 bits truncated TSC won't overflow
+	  between two timer executions. An event is written in the active
+	  traces each time the timer is executed.
+
+	  Note : it is currently broken with CPU hotplug and does not support
+	  trace stop / restart correctly.
+
+config LTT_NETLINK_CONTROL
+	tristate "Linux Trace Toolkit Netlink Controller"
+	depends on LTT_TRACER
+	depends on NET
+	default m
+	help
+	  If you enable this option, the Linux Trace Toolkit Netlink Controller
+	  will be either built in the kernel or as module.
+
+config LTT_STATEDUMP
+	tristate "Linux Trace Toolkit State Dump"
+	depends on LTT_TRACER
+	default m
+	help
+	  If you enable this option, the Linux Trace Toolkit State Dump will
+	  be either built in the kernel or as module.
+
+	  This module saves the state of the running kernel at trace start
+	  into the trace buffers along with the ongoing tracing information.
+
+menu "Probes"
+	depends on LTT_MARKER_CONTROL
+
+config LTT_PROBE_FS
+	tristate "Linux Trace Toolkit File System Probe"
+	depends on LTT_MARKER_CONTROL
+	depends on CRC32
+	default m
+	help
+	  Activate per facility LTT probes. This is the dynamic mechanism
+	  where the data gathering is. Probes connect to markers when their
+	  module is loaded.
+
+	  Probe file system events. It instruments the file operations and
+	  dumps the beginning of data sent through read and write system calls.
+
+config LTT_PROBE_STACK
+	tristate "Sample process or kernel stacks (EXPERIMENTAL)"
+	depends on LTT_MARKER_CONTROL
+	depends on X86 || X86_64
+	depends on CRC32
+	default n
+	help
+	  Get complete process and/or kernel stack (architecture specific)
+
+config LTT_PROCESS_STACK
+	bool "Get complete process stack"
+	depends on LTT_MARKER_CONTROL
+	depends on X86
+	depends on CRC32
+	depends on UNWIND_INFO || !X86_64
+	depends on STACK_UNWIND || !X86_64
+	default n
+	help
+	  Get complete process stack.
+
+	  It has three limitations : it only considers functions in the loaded
+	  executable (not libraries) to be actual function pointers.
+	  Furthermore, it will dump the stack of many threads at once for
+	  multithreaded processes (it is protected from races between threads
+	  though). It can also believe that arbitrary data on the stack
+	  "looks like" a function pointer.
+
+config LTT_PROCESS_MAX_FUNCTION_STACK
+	int "Maximum number of longs on the stack between functions"
+	depends on LTT_PROCESS_STACK
+	default 100
+	help
+	  Maximum threshold over which, if we do not find a function pointer on
+	  the process stack, we stop dumping the pointers on the stack.
+
+config LTT_PROCESS_MAX_STACK_LEN
+	int "Maximum number of longs on the stack to read"
+	depends on LTT_PROCESS_STACK
+	default 250
+	help
+	  Maximum threshold of stack size over which we stop looking for
+	  function pointers.
+
+config LTT_KERNEL_STACK
+	bool "Get complete kernel stack"
+	depends on LTT_PROBE_STACK
+	default n
+	help
+	  Get complete kernel stack
+
+endmenu
diff -uprN linux-2.6.23.1.orig/ltt/Makefile linux-2.6.23.1/ltt/Makefile
--- linux-2.6.23.1.orig/ltt/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/Makefile	2007-11-28 10:13:52.378345463 +0100
@@ -0,0 +1,14 @@
+#
+# Makefile for the LTT objects.
+#
+obj-$(CONFIG_LTT)			+= ltt-timestamp.o
+obj-$(CONFIG_LTT)			+= ltt-core.o
+obj-$(CONFIG_LTT)			+= probes/
+obj-$(CONFIG_LTT_HEARTBEAT)		+= ltt-heartbeat.o
+obj-$(CONFIG_LTT_TRACER)		+= ltt-tracer.o
+obj-$(CONFIG_LTT_RELAY)		+= ltt-relay.o
+obj-$(CONFIG_LTT_NETLINK_CONTROL)	+= ltt-control.o
+obj-$(CONFIG_LTT_SERIALIZE)		+= ltt-serialize.o
+obj-$(CONFIG_LTT_STATEDUMP)		+= ltt-statedump.o
+obj-$(CONFIG_LTT_TEST_TSC)		+= ltt-test-tsc.o
+obj-$(CONFIG_LTT_MARKER_CONTROL)	+= ltt-marker-control.o
diff -uprN linux-2.6.23.1.orig/ltt/ltt-control.c linux-2.6.23.1/ltt/ltt-control.c
--- linux-2.6.23.1.orig/ltt/ltt-control.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-control.c	2007-11-28 10:13:52.369347339 +0100
@@ -0,0 +1,120 @@
+/*
+ * LTT control module over a netlink socket.
+ *
+ * Inspired from Relay Apps, by Tom Zanussi and iptables
+ *
+ * Copyright 2005 -
+ * 	Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+#include <linux/netlink.h>
+#include <linux/inet.h>
+#include <linux/ip.h>
+#include <linux/security.h>
+#include <linux/skbuff.h>
+#include <linux/types.h>
+#include <net/sock.h>
+#include "ltt-control.h"
+
+#define LTTCTLM_BASE	0x10
+#define LTTCTLM_CONTROL	(LTTCTLM_BASE + 1)	/* LTT control message */
+
+static struct sock *socket;
+
+void ltt_control_input(struct sock *sk, int len)
+{
+	struct sk_buff *skb;
+	struct nlmsghdr *nlh = NULL;
+	u8 *payload = NULL;
+	lttctl_peer_msg_t *msg;
+	int err;
+
+	printk(KERN_DEBUG "ltt-control ltt_control_input\n");
+
+	while ((skb = skb_dequeue(&sk->sk_receive_queue)) != NULL) {
+
+		nlh = (struct nlmsghdr *)skb->data;
+		if (security_netlink_recv(skb, CAP_SYS_PTRACE)) {
+			netlink_ack(skb, nlh, EPERM);
+			kfree_skb(skb);
+			continue;
+		}
+		/* process netlink message pointed by skb->data */
+		err = EINVAL;
+		payload = NLMSG_DATA(nlh);
+		/*
+		 * process netlink message with header pointed by
+		 * nlh and payload pointed by payload
+		 */
+		if (nlh->nlmsg_len !=
+				sizeof(lttctl_peer_msg_t) +
+				sizeof(struct nlmsghdr)) {
+			printk(KERN_ALERT
+				"ltt-control bad message length %d vs. %zu\n",
+				nlh->nlmsg_len, sizeof(lttctl_peer_msg_t) +
+				sizeof(struct nlmsghdr));
+			netlink_ack(skb, nlh, EINVAL);
+			kfree_skb(skb);
+			continue;
+		}
+		msg = (lttctl_peer_msg_t *)payload;
+
+		switch (msg->op) {
+		case OP_CREATE:
+			err = ltt_control(LTT_CONTROL_CREATE_TRACE,
+					msg->trace_name,
+					msg->trace_type, msg->args);
+			break;
+		case OP_DESTROY:
+			err = ltt_control(LTT_CONTROL_DESTROY_TRACE,
+					msg->trace_name,
+					msg->trace_type, msg->args);
+			break;
+		case OP_START:
+			err = ltt_control(LTT_CONTROL_START,
+					msg->trace_name,
+					msg->trace_type, msg->args);
+			break;
+		case OP_STOP:
+			err = ltt_control(LTT_CONTROL_STOP,
+					msg->trace_name,
+					msg->trace_type, msg->args);
+			break;
+		default:
+			err = EBADRQC;
+			printk(KERN_INFO "ltt-control invalid operation\n");
+		}
+		netlink_ack(skb, nlh, err);
+		kfree_skb(skb);
+	}
+}
+
+
+static int ltt_control_init(void)
+{
+	printk(KERN_INFO "ltt-control init\n");
+	socket = netlink_kernel_create(NETLINK_LTT, 1,
+			ltt_control_input, NULL, THIS_MODULE);
+	if (socket == NULL)
+		return -EPERM;
+	else
+		return 0;
+}
+
+static void ltt_control_exit(void)
+{
+	printk(KERN_INFO "ltt-control exit\n");
+	sock_release(socket->sk_socket);
+}
+
+
+module_init(ltt_control_init)
+module_exit(ltt_control_exit)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Controller");
+
diff -uprN linux-2.6.23.1.orig/ltt/ltt-control.h linux-2.6.23.1/ltt/ltt-control.h
--- linux-2.6.23.1.orig/ltt/ltt-control.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-control.h	2007-11-28 10:13:52.369347339 +0100
@@ -0,0 +1,30 @@
+/*
+ * LTT control module over a netlink socket.
+ *
+ * Inspired from Relay Apps, by Tom Zanussi and iptables
+ *
+ * Copyright 2005 -
+ * 	Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#ifndef _LTT_CONTROL_H
+#define _LTT_CONTROL_H
+
+enum trace_op {
+	OP_CREATE,
+	OP_DESTROY,
+	OP_START,
+	OP_STOP,
+	OP_ALIGN,
+	OP_NONE
+};
+
+typedef struct lttctl_peer_msg {
+	char trace_name[NAME_MAX];
+	char trace_type[NAME_MAX];
+	enum trace_op op;
+	union ltt_control_args args;
+} lttctl_peer_msg_t;
+
+#endif /* _LTT_CONTROL_H */
+
diff -uprN linux-2.6.23.1.orig/ltt/ltt-core.c linux-2.6.23.1/ltt/ltt-core.c
--- linux-2.6.23.1.orig/ltt/ltt-core.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-core.c	2007-11-28 10:13:52.369347339 +0100
@@ -0,0 +1,55 @@
+/*
+ * LTT core in-kernel infrastructure.
+ *
+ * Copyright 2006 - Mathieu Desnoyers mathieu.desnoyers@polymtl.ca
+ *
+ * Distributed under the GPL license
+ */
+
+#include <linux/ltt-core.h>
+#include <linux/module.h>
+
+/* Traces structures */
+struct ltt_traces ltt_traces = {
+	.head = LIST_HEAD_INIT(ltt_traces.head),
+};
+EXPORT_SYMBOL(ltt_traces);
+
+/* Traces list writer locking */
+static DEFINE_MUTEX(ltt_traces_mutex);
+
+void ltt_lock_traces(void)
+{
+	mutex_lock(&ltt_traces_mutex);
+}
+EXPORT_SYMBOL_GPL(ltt_lock_traces);
+
+void ltt_unlock_traces(void)
+{
+	mutex_unlock(&ltt_traces_mutex);
+}
+EXPORT_SYMBOL_GPL(ltt_unlock_traces);
+
+unsigned int ltt_nesting[NR_CPUS];
+EXPORT_SYMBOL(ltt_nesting);
+
+int ltt_run_filter_default(void *trace, uint16_t eID)
+{
+	return 1;
+}
+
+/* This function pointer is protected by a trace activation check */
+ltt_run_filter_functor ltt_run_filter = ltt_run_filter_default;
+EXPORT_SYMBOL_GPL(ltt_run_filter);
+
+void ltt_filter_register(ltt_run_filter_functor func)
+{
+	ltt_run_filter = func;
+}
+EXPORT_SYMBOL_GPL(ltt_filter_register);
+
+void ltt_filter_unregister(void)
+{
+	ltt_run_filter = ltt_run_filter_default;
+}
+EXPORT_SYMBOL_GPL(ltt_filter_unregister);
diff -uprN linux-2.6.23.1.orig/ltt/ltt-heartbeat.c linux-2.6.23.1/ltt/ltt-heartbeat.c
--- linux-2.6.23.1.orig/ltt/ltt-heartbeat.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-heartbeat.c	2007-11-28 10:13:52.370347131 +0100
@@ -0,0 +1,294 @@
+/*
+ * (C) Copyright	2006 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * notes : heartbeat timer cannot be used for early tracing in the boot process,
+ * as it depends on timer interrupts.
+ *
+ * The timer needs to be only on one CPU to support hotplug.
+ * We have the choice between schedule_delayed_work_on and an IPI to get each
+ * CPU to write the heartbeat. IPI has been chosen because it is considered
+ * faster than passing through the timer to get the work scheduled on all the
+ * CPUs.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/ltt-core.h>
+#include <linux/workqueue.h>
+#include <linux/cpu.h>
+#include <linux/timex.h>
+#include <linux/bitops.h>
+#include <linux/marker.h>
+#include <linux/ltt-tracer.h>
+
+#define BITS_OF_COMPACT_DATA		11
+
+/* Expected minimum probe duration, in cycles */
+#define MIN_PROBE_DURATION		400
+/* Expected maximum interrupt latency in ms : 15ms, *2 for security */
+#define EXPECTED_INTERRUPT_LATENCY	30
+
+static struct timer_list heartbeat_timer;
+static unsigned int precalc_heartbeat_expire;
+
+int ltt_compact_data_shift;
+EXPORT_SYMBOL_GPL(ltt_compact_data_shift);
+
+int ltt_tsc_lsb_truncate;	/* Number of LSB removed from compact tsc */
+EXPORT_SYMBOL_GPL(ltt_tsc_lsb_truncate);
+int ltt_tscbits;		/* 32 - tsc_lsb_truncate - tsc_msb_cutoff */
+EXPORT_SYMBOL_GPL(ltt_tscbits);
+
+static void heartbeat_ipi(void *info)
+{
+	struct ltt_probe_private_data call_data;
+
+	call_data.trace = NULL;
+	call_data.force = 0;
+	call_data.id = MARKER_ID_HEARTBEAT_32;
+	/* Log a heartbeat event for each trace, each tracefile */
+	for (call_data.channel = ltt_channel_index_begin();
+			call_data.channel < ltt_channel_index_end();
+			call_data.channel += ltt_channel_index_size())
+		__trace_mark(0, core_heartbeat_32, &call_data,
+				MARK_NOARGS);
+}
+
+static void heartbeat_ipi_full(void *info)
+{
+	struct ltt_probe_private_data call_data;
+
+	call_data.trace = info;
+	call_data.force = 1;
+	call_data.id = MARKER_ID_HEARTBEAT_64;
+	call_data.cpu = -1;
+	/* Log a heartbeat event for each trace, each tracefile */
+	for (call_data.channel = ltt_channel_index_begin();
+			call_data.channel < ltt_channel_index_end();
+			call_data.channel += ltt_channel_index_size())
+		__trace_mark(0, core_heartbeat_64, &call_data,
+				"timestamp #8u%llu", ltt_get_timestamp64());
+}
+
+/*
+ * Write the full TSC in the traces. To be called when we cannot assume that the
+ * heartbeat events will keep a trace buffer synchronized. (missing timer
+ * events, tracing starts, tracing restarts).
+ */
+void ltt_write_full_tsc(struct ltt_trace_struct *trace)
+{
+	on_each_cpu(heartbeat_ipi_full, trace, 1, 1);
+}
+EXPORT_SYMBOL_GPL(ltt_write_full_tsc);
+
+/* We need to be in process context to do an IPI */
+static void heartbeat_work(struct work_struct *work)
+{
+	on_each_cpu(heartbeat_ipi, NULL, 1, 0);
+}
+
+static DECLARE_WORK(hb_work, heartbeat_work);
+
+/*
+ * heartbeat_timer : - Timer function generating heartbeat.
+ * @data: unused
+ *
+ * Guarantees at least 1 execution of heartbeat before low word of TSC wraps.
+ */
+static void heartbeat_timer_fct(unsigned long data)
+{
+	PREPARE_WORK(&hb_work, heartbeat_work);
+	schedule_work(&hb_work);
+
+	mod_timer(&heartbeat_timer, jiffies + precalc_heartbeat_expire);
+}
+
+/*
+ * init_heartbeat_timer: - Start timer generating heartbeat events.
+ */
+static void init_heartbeat_timer(void)
+{
+	int tsc_msb_cutoff;
+	int data_bits = BITS_OF_COMPACT_DATA;
+	int max_tsc_msb_cutoff;
+	unsigned long mask;
+
+	if (loops_per_jiffy > 0) {
+		printk(KERN_DEBUG "LTT : ltt-heartbeat init\n");
+		printk(KERN_DEBUG "Requested number of bits %d\n", data_bits);
+
+		ltt_tsc_lsb_truncate = max(0,
+			(int)get_count_order(MIN_PROBE_DURATION)-1);
+		max_tsc_msb_cutoff =
+			32 - 1 - get_count_order(((1UL << 1)
+			+ (EXPECTED_INTERRUPT_LATENCY*HZ/1000)
+			+ LTT_PERCPU_TIMER_INTERVAL + 1)
+			* (loops_per_jiffy << 1));
+		printk(KERN_DEBUG "Available number of bits %d\n",
+			ltt_tsc_lsb_truncate + max_tsc_msb_cutoff);
+		if (ltt_tsc_lsb_truncate + max_tsc_msb_cutoff <
+			data_bits) {
+			printk(KERN_DEBUG "Number of bits truncated to %d\n",
+				ltt_tsc_lsb_truncate + max_tsc_msb_cutoff);
+			data_bits = ltt_tsc_lsb_truncate + max_tsc_msb_cutoff;
+		}
+
+		tsc_msb_cutoff = data_bits - ltt_tsc_lsb_truncate;
+
+		if (tsc_msb_cutoff > 0)
+			mask = (1UL<<(32-tsc_msb_cutoff))-1;
+		else
+			mask = 0xFFFFFFFFUL;
+		precalc_heartbeat_expire =
+			(mask/(loops_per_jiffy << 1)
+				- 1 - LTT_PERCPU_TIMER_INTERVAL
+				- (EXPECTED_INTERRUPT_LATENCY*HZ/1000)) >> 1;
+		WARN_ON(precalc_heartbeat_expire == 0);
+		printk(KERN_DEBUG
+			"Heartbeat timer will fire each %u jiffies.\n",
+			precalc_heartbeat_expire);
+
+		ltt_tscbits = 32 - ltt_tsc_lsb_truncate - tsc_msb_cutoff;
+		printk(KERN_DEBUG
+			"Compact TSC init : truncate %d lsb, cutoff %d msb.\n",
+			ltt_tsc_lsb_truncate, tsc_msb_cutoff);
+	} else
+		printk(KERN_WARNING
+			"LTT: no tsc for heartbeat timer "
+			"- continuing without one \n");
+}
+
+/*
+ * ltt_init_compact_markers reserves the number of bits to identify the event
+ * numbers in the compact headers. It must be called every time the compact
+ * markers are changed.
+ */
+void ltt_init_compact_markers(unsigned int num_compact_events)
+{
+	if (loops_per_jiffy > 0) {
+		ltt_compact_data_shift =
+			get_count_order(num_compact_events) + ltt_tscbits;
+		printk(KERN_DEBUG "Data shifted from %d bits\n",
+			ltt_compact_data_shift);
+
+		printk(KERN_DEBUG "%d bits used for event IDs, "
+			"%d available for data.\n",
+			get_count_order(num_compact_events),
+			32 - ltt_compact_data_shift);
+	} else
+		printk(KERN_WARNING
+			"LTT: no tsc for heartbeat timer "
+			"- continuing without one \n");
+}
+EXPORT_SYMBOL_GPL(ltt_init_compact_markers);
+
+
+static void start_heartbeat_timer(void)
+{
+	if (precalc_heartbeat_expire > 0) {
+		printk(KERN_DEBUG "LTT : ltt-heartbeat start\n");
+
+		init_timer(&heartbeat_timer);
+		heartbeat_timer.function = heartbeat_timer_fct;
+		heartbeat_timer.expires = jiffies + precalc_heartbeat_expire;
+		add_timer(&heartbeat_timer);
+	} else
+		printk(KERN_WARNING
+			"LTT: no tsc for heartbeat timer "
+			"- continuing without one \n");
+}
+
+/*
+ * stop_heartbeat_timer: - Stop timer generating heartbeat events.
+ */
+static void stop_heartbeat_timer(void)
+{
+	if (loops_per_jiffy > 0) {
+		printk(KERN_DEBUG "LTT : ltt-heartbeat stop\n");
+		del_timer(&heartbeat_timer);
+	}
+}
+
+/*
+ * 	heartbeat_hotcpu_full - CPU hotplug heartbeat
+ * 	@cpu: CPU number
+ *
+ * 	Writes an heartbeat event in each traces, in forced mode, on behalf of
+ * 	the CPU soon to be brought online. Events are written by another CPU in
+ * 	buffers not actually used by the new comer yet.
+ */
+
+static void __cpuinit heartbeat_hotcpu_full(int cpu)
+{
+	struct ltt_probe_private_data call_data;
+
+	call_data.trace = NULL;
+	call_data.force = 1;
+	call_data.id = MARKER_ID_HEARTBEAT_64;
+	call_data.cpu = cpu;
+	/* Log a heartbeat event for each trace, each tracefile */
+	for (call_data.channel = ltt_channel_index_begin();
+			call_data.channel < ltt_channel_index_end();
+			call_data.channel += ltt_channel_index_size())
+		__trace_mark(0, core_heartbeat_64, &call_data,
+				"timestamp #8u%llu", ltt_get_timestamp64());
+	/*
+	 * Make sure the data is consistent for the CPU being brought online
+	 * soon.
+	 */
+	smp_wmb();
+}
+
+/*
+ * 	heartbeat_hotcpu_callback - CPU hotplug callback
+ * 	@nb: notifier block
+ * 	@action: hotplug action to take
+ * 	@hcpu: CPU number
+ *
+ * 	Returns the success/failure of the operation. (NOTIFY_OK, NOTIFY_BAD)
+ */
+static int __cpuinit heartbeat_hotcpu_callback(struct notifier_block *nb,
+				unsigned long action,
+				void *hcpu)
+{
+	unsigned int hotcpu = (unsigned long)hcpu;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		heartbeat_hotcpu_full(hotcpu);
+		break;
+	case CPU_ONLINE:
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+int ltt_heartbeat_trigger(enum ltt_heartbeat_functor_msg msg)
+{
+	printk(KERN_DEBUG "LTT : ltt-heartbeat trigger\n");
+	switch (msg) {
+	case LTT_HEARTBEAT_START:
+		start_heartbeat_timer();
+		break;
+	case LTT_HEARTBEAT_STOP:
+		stop_heartbeat_timer();
+		break;
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_heartbeat_trigger);
+
+static int __init ltt_heartbeat_init(void)
+{
+	printk(KERN_INFO "LTT : ltt-heartbeat init\n");
+	/* lower priority than relay, lower than synthetic tsc */
+	hotcpu_notifier(heartbeat_hotcpu_callback, 2);
+	init_heartbeat_timer();
+	return 0;
+}
+
+__initcall(ltt_heartbeat_init);
diff -uprN linux-2.6.23.1.orig/ltt/ltt-marker-control.c linux-2.6.23.1/ltt/ltt-marker-control.c
--- linux-2.6.23.1.orig/ltt/ltt-marker-control.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-marker-control.c	2007-11-28 10:13:52.371346922 +0100
@@ -0,0 +1,878 @@
+/*
+ * Copyright (C) 2007 Mathieu Desnoyers
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * LTT marker control module over /proc
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/vmalloc.h>
+#include <linux/marker.h>
+#include <linux/ltt-tracer.h>
+#include <linux/uaccess.h>
+#include <linux/string.h>
+#include <linux/ctype.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+
+#define DEFAULT_CHANNEL "cpu"
+#define DEFAULT_PROBE "default"
+
+LIST_HEAD(probes_list);
+
+/*
+ * Mutex protecting the probe slab cache and probe IDs consistency.
+ * Nests inside the traces mutex.
+ */
+DEFINE_MUTEX(probes_mutex);
+
+static uint8_t mark_next_compact_id = MARKER_CORE_IDS;
+/* Next available ID. Dynamic range : 128-65535 */
+static uint16_t mark_next_id = MARKER_COMPACT_IDS;
+
+struct ltt_available_probe default_probe = {
+	.name = "default",
+	.format = NULL,
+	.probe_func = ltt_trace,
+	.callbacks[0] = ltt_serialize_data,
+};
+
+static struct kmem_cache *markers_loaded_cachep;
+static LIST_HEAD(markers_loaded_list);
+/*
+ * List sorted by name strcmp order.
+ */
+static LIST_HEAD(probes_registered_list);
+
+static struct proc_dir_entry *pentry;
+
+static struct chan_name_info {
+	enum ltt_channels channels;
+	const char *name;
+	unsigned int channel_index;
+} channel_names[] = {
+	{ LTT_CHANNEL_FACILITIES, "facilities", GET_CHANNEL_INDEX(facilities) },
+	{ LTT_CHANNEL_INTERRUPTS, "interrupts", GET_CHANNEL_INDEX(interrupts) },
+	{ LTT_CHANNEL_PROCESSES, "processes", GET_CHANNEL_INDEX(processes) },
+	{ LTT_CHANNEL_MODULES, "modules", GET_CHANNEL_INDEX(modules) },
+	{ LTT_CHANNEL_CPU, "cpu", GET_CHANNEL_INDEX(cpu) },
+	{ LTT_CHANNEL_COMPACT, "compact", GET_CHANNEL_INDEX(compact) },
+	{ LTT_CHANNEL_NETWORK, "network", GET_CHANNEL_INDEX(network) },
+};
+
+static struct id_name_info {
+	enum marker_id n_id;
+	const char *name;
+} id_names[] = {
+	{ MARKER_ID_COMPACT, "compact" },
+	{ MARKER_ID_DYNAMIC, "dynamic" },
+};
+
+static struct file_operations ltt_fops;
+
+static inline int get_channel_index_from_name(const char *name)
+{
+	struct chan_name_info *info;
+
+	if (!name)
+		name = "cpu";
+	for (info = channel_names;
+		info < channel_names + ARRAY_SIZE(channel_names); info++) {
+		if (!strcmp(name, info->name))
+			return info->channel_index;
+	}
+	return -ENOENT;
+}
+
+static inline enum marker_id get_id_from_name(const char *name)
+{
+	struct id_name_info *info;
+
+	if (!name)
+		name = "dynamic";
+	for (info = id_names; info < id_names + ARRAY_SIZE(id_names); info++) {
+		if (!strcmp(name, info->name))
+			return info->n_id;
+	}
+	return -ENOENT;
+}
+
+static inline char *skip_spaces(char *buf)
+{
+	while (*buf != '\0' && isspace(*buf))
+		buf++;
+	return buf;
+}
+
+static inline char *skip_nonspaces(char *buf)
+{
+	while (*buf != '\0' && !isspace(*buf))
+		buf++;
+	return buf;
+}
+
+static void get_marker_string(char *buf, char **start,
+		char **end)
+{
+	*start = skip_spaces(buf);
+	*end = skip_nonspaces(*start);
+	**end = '\0';
+}
+
+static inline int is_pdata_in_active_markers(void *pdata)
+{
+	struct ltt_active_marker *amark;
+
+	list_for_each_entry(amark, &markers_loaded_list, node) {
+		if ((void *)amark == pdata)
+			return 1;
+	}
+	return 0;
+}
+
+/*
+ * Defragment the markers IDs. Should only be called when the IDs are not used
+ * by anyone, typically when all probes are disarmed. Clients of the markers
+ * rely on having their code markers armed during a "session" to make sure there
+ * will be no ID compaction. (for instance, a marker ID is never reused during a
+ * trace).
+ * There is no need to synchronize the hash table entries with the section
+ * elements because none is armed.
+ */
+void probe_id_defrag(void)
+{
+	struct ltt_active_marker *amark;
+
+	mark_next_compact_id = MARKER_CORE_IDS;
+	mark_next_id = MARKER_COMPACT_IDS;
+
+	mutex_lock(&probes_mutex);
+	list_for_each_entry(amark, &markers_loaded_list, node) {
+		switch (marker_id_type(amark->id)) {
+		case MARKER_ID_COMPACT:
+			amark->id = mark_next_compact_id++;
+			break;
+		case MARKER_ID_DYNAMIC:
+			amark->id = mark_next_id++;
+			break;
+		default:
+			/* default is to keep the static ID */
+			break;
+		}
+	}
+	ltt_init_compact_markers(mark_next_compact_id);
+	mutex_unlock(&probes_mutex);
+}
+EXPORT_SYMBOL_GPL(probe_id_defrag);
+
+/*
+ * Must be called with ltt_lock_traces() taken so the number of active traces
+ * stays to 0 if we are registering a compact ID.
+ * This is required because we must call ltt_init_compact_markers() to recompute
+ * the number of reserved bits.
+ */
+static int _check_id_avail(enum marker_id id)
+{
+	/* First check if there are still IDs available */
+	switch (id) {
+	case MARKER_ID_DYNAMIC:
+		if (mark_next_id == 0)
+			return -ENOSPC;
+		break;
+	case MARKER_ID_COMPACT:
+		if (ltt_traces.num_active_traces > 0)
+			return -EBUSY;
+		if (mark_next_compact_id == 0)
+			return -ENOSPC;
+		break;
+	default:
+		/* Only allow 0-7 range for core IDs */
+		if ((uint16_t)id >= MARKER_CORE_IDS)
+			return -EPERM;
+	}
+	return 0;
+}
+
+static uint16_t assign_id(enum marker_id id)
+{
+	uint16_t new_id;
+
+	switch (id) {
+	case MARKER_ID_COMPACT:
+		new_id = mark_next_compact_id++;
+		ltt_init_compact_markers(mark_next_compact_id);
+		return new_id;
+	case MARKER_ID_DYNAMIC:
+		return mark_next_id++;
+	default:
+		return (uint16_t)id;
+	}
+}
+
+int ltt_probe_register(struct ltt_available_probe *pdata)
+{
+	int ret = 0;
+	int comparison;
+	struct ltt_available_probe *iter;
+
+	mutex_lock(&probes_mutex);
+	list_for_each_entry_reverse(iter, &probes_registered_list, node) {
+		comparison = strcmp(pdata->name, iter->name);
+		if (!comparison) {
+			ret = -EBUSY;
+			goto end;
+		} else if (comparison > 0) {
+			/* We belong to the location right after iter. */
+			list_add(&pdata->node, &iter->node);
+			goto end;
+		}
+	}
+	/* Should be added at the head of the list */
+	list_add(&pdata->node, &probes_registered_list);
+end:
+	mutex_unlock(&probes_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_probe_register);
+
+/*
+ * Called when a probe does not want to be called anymore.
+ */
+int ltt_probe_unregister(struct ltt_available_probe *pdata)
+{
+	int ret = 0;
+	struct ltt_active_marker *amark, *tmp;
+	void *retptr;
+
+	mutex_lock(&probes_mutex);
+	list_for_each_entry_safe(amark, tmp, &markers_loaded_list, node) {
+		if (amark->probe == pdata) {
+			retptr = marker_probe_unregister_private_data(amark);
+			if (IS_ERR(retptr)) {
+				ret = PTR_ERR(retptr);
+				goto end;
+			}
+			list_del(&amark->node);
+			kmem_cache_free(markers_loaded_cachep, amark);
+		}
+	}
+	list_del(&pdata->node);
+end:
+	mutex_unlock(&probes_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_probe_unregister);
+
+/*
+ * Connect marker "mname" to probe "pname".
+ */
+int ltt_marker_connect(const char *mname, const char *pname,
+	enum marker_id id, uint16_t channel, int user, int align)
+
+{
+	int ret;
+	int found = 0;
+	int comparison;
+	struct ltt_active_marker *pdata;
+	struct ltt_available_probe *iter;
+
+	/*
+	 * Do not let userspace mess with core markers.
+	 */
+	if (user && id != MARKER_ID_COMPACT && id != MARKER_ID_DYNAMIC)
+		return -EPERM;
+	if (!pname)
+		pname = DEFAULT_PROBE;
+
+	ltt_lock_traces();
+	mutex_lock(&probes_mutex);
+	ret = _check_id_avail(id);
+	if (ret)
+		goto end;
+	list_for_each_entry(iter, &probes_registered_list, node) {
+		comparison = strcmp(pname, iter->name);
+		if (!comparison)
+			found = 1;
+		if (comparison <= 0)
+			break;
+	}
+	if (!found) {
+		ret = -ENOENT;
+		goto end;
+	}
+	pdata = kmem_cache_zalloc(markers_loaded_cachep, GFP_KERNEL);
+	if (!pdata) {
+		ret = -ENOMEM;
+		goto end;
+	}
+	pdata->probe = iter;
+	pdata->id = assign_id(id);
+	pdata->channel = channel;
+	pdata->align = align;
+	/*
+	 * ID has priority over channel in case of conflict.
+	 */
+	if (id == MARKER_ID_COMPACT)
+		pdata->channel = get_channel_index_from_name("compact");
+	else if (pdata->channel == get_channel_index_from_name("compact"))
+		pdata->channel = get_channel_index_from_name(DEFAULT_CHANNEL);
+	ret = marker_probe_register(mname, NULL, ltt_trace, pdata);
+	if (ret)
+		kmem_cache_free(markers_loaded_cachep, pdata);
+	else {
+		struct ltt_probe_private_data call_data;
+
+		call_data.trace = NULL;
+		call_data.force = 0;
+		call_data.channel = GET_CHANNEL_INDEX(facilities);
+		call_data.id = MARKER_CORE_IDS;
+		__trace_mark(0, core_marker_id, &call_data,
+				"name %s id %hu "
+				"int #1u%zu long #1u%zu pointer #1u%zu "
+				"size_t #1u%zu alignment #1u%u",
+				mname, pdata->id,
+				sizeof(int), sizeof(long), sizeof(void *),
+				sizeof(size_t), ltt_get_alignment(pdata));
+		list_add(&pdata->node, &markers_loaded_list);
+	}
+end:
+	mutex_unlock(&probes_mutex);
+	ltt_unlock_traces();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_marker_connect);
+
+/*
+ * Disconnect marker "mname".
+ */
+int ltt_marker_disconnect(const char *mname, int user)
+{
+	struct ltt_active_marker *pdata;
+	int ret = 0;
+
+	mutex_lock(&probes_mutex);
+	pdata = marker_get_private_data(mname);
+	if (IS_ERR(pdata)) {
+		ret = PTR_ERR(pdata);
+		goto end;
+	} else if (!is_pdata_in_active_markers(pdata)) {
+		/*
+		 * Not registered by us.
+		 */
+		ret = -EPERM;
+		goto end;
+	} else if (user && pdata->id < MARKER_CORE_IDS) {
+		ret = -EPERM;
+		goto end;
+	}
+	pdata = marker_probe_unregister(mname);
+	if (IS_ERR(pdata)) {
+		ret = PTR_ERR(pdata);
+		goto end;
+	} else {
+		list_del(&pdata->node);
+		kmem_cache_free(markers_loaded_cachep, pdata);
+	}
+end:
+	mutex_unlock(&probes_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_marker_disconnect);
+
+/*
+ * Sets a marker id.
+ * Keep the same loaded marker element.
+ */
+static int do_set_id(const char *name, enum marker_id new_id, int user)
+{
+	struct ltt_active_marker *pdata;
+	int ret;
+
+	/*
+	 * Do not let userspace mess with core markers.
+	 */
+	if (user && new_id != MARKER_ID_COMPACT && new_id != MARKER_ID_DYNAMIC)
+		return -EPERM;
+
+	ltt_lock_traces();
+	mutex_lock(&probes_mutex);
+	pdata = marker_get_private_data(name);
+	if (IS_ERR(pdata)) {
+		ret = PTR_ERR(pdata);
+		goto end;
+	} else if (!is_pdata_in_active_markers(pdata)) {
+		/*
+		 * Not registered by us.
+		 */
+		ret = -EPERM;
+		goto end;
+	} else if (user && pdata->id < MARKER_CORE_IDS) {
+		ret = -EPERM;
+		goto end;
+	}
+	pdata = marker_probe_unregister(name);
+	if (IS_ERR(pdata)) {
+		ret = PTR_ERR(pdata);
+		goto end;
+	}
+	ret = _check_id_avail(new_id);
+	if (ret)
+		goto end;
+	pdata->id = assign_id(new_id);
+	if (new_id == MARKER_ID_COMPACT)
+		pdata->channel = get_channel_index_from_name("compact");
+	else if (pdata->channel == get_channel_index_from_name("compact"))
+		pdata->channel = get_channel_index_from_name(DEFAULT_CHANNEL);
+	ret = marker_probe_register(name, NULL, ltt_trace, pdata);
+	if (ret) {
+		list_del(&pdata->node);
+		kmem_cache_free(markers_loaded_cachep, pdata);
+		printk(KERN_ERR "Bogus update of marker %s\n", name);
+	} else {
+		struct ltt_probe_private_data call_data;
+
+		call_data.trace = NULL;
+		call_data.force = 0;
+		call_data.channel = GET_CHANNEL_INDEX(facilities);
+		call_data.id = MARKER_CORE_IDS;
+		__trace_mark(0, core_marker_id, &call_data,
+				"name %s id %hu "
+				"int #1u%zu long #1u%zu pointer #1u%zu "
+				"size_t #1u%zu alignment #1u%u",
+				name, pdata->id,
+				sizeof(int), sizeof(long), sizeof(void *),
+				sizeof(size_t), ltt_get_alignment(pdata));
+	}
+end:
+	mutex_unlock(&probes_mutex);
+	ltt_unlock_traces();
+	return ret;
+}
+
+/*
+ * Sets a marker channel.
+ * Keep the same loaded marker element.
+ */
+static int do_set_channel(const char *name, uint16_t new_channel, int user)
+{
+	struct ltt_active_marker *pdata;
+	int ret;
+
+	/*
+	 * Do not let userspace mess with core markers.
+	 */
+	if (user && get_channel_index_from_name("facilities") == new_channel)
+		return -EPERM;
+
+	ltt_lock_traces();
+	mutex_lock(&probes_mutex);
+	pdata = marker_get_private_data(name);
+	if (IS_ERR(pdata)) {
+		ret = PTR_ERR(pdata);
+		goto end;
+	} else if (!is_pdata_in_active_markers(pdata)) {
+		/*
+		 * Not registered by us.
+		 */
+		ret = -EPERM;
+		goto end;
+	} else if (user && pdata->id < MARKER_CORE_IDS) {
+		ret = -EPERM;
+		goto end;
+	}
+	pdata = marker_probe_unregister(name);
+	if (IS_ERR(pdata)) {
+		ret = PTR_ERR(pdata);
+		goto end;
+	}
+	pdata->channel = new_channel;
+	if (new_channel == get_channel_index_from_name("compact")
+			&& pdata->id >= MARKER_COMPACT_IDS) {
+		ret = _check_id_avail(MARKER_ID_COMPACT);
+		if (!ret)
+			goto end;
+		pdata->id = assign_id(MARKER_ID_COMPACT);
+	} else if (pdata->id < MARKER_COMPACT_IDS
+			&& pdata->id >= MARKER_CORE_IDS) {
+		ret = _check_id_avail(MARKER_ID_DYNAMIC);
+		if (!ret)
+			goto end;
+		pdata->id = assign_id(MARKER_ID_DYNAMIC);
+	}
+
+	ret = marker_probe_register(name, NULL, ltt_trace, pdata);
+	if (ret) {
+		list_del(&pdata->node);
+		kmem_cache_free(markers_loaded_cachep, pdata);
+		printk(KERN_ERR "Bogus update of marker %s\n", name);
+	} else {
+		struct ltt_probe_private_data call_data;
+
+		call_data.trace = NULL;
+		call_data.force = 0;
+		call_data.channel = GET_CHANNEL_INDEX(facilities);
+		call_data.id = MARKER_CORE_IDS;
+		__trace_mark(0, core_marker_id, &call_data,
+				"name %s id %hu "
+				"int #1u%zu long #1u%zu pointer #1u%zu "
+				"size_t #1u%zu alignment #1u%u",
+				name, pdata->id,
+				sizeof(int), sizeof(long), sizeof(void *),
+				sizeof(size_t), ltt_get_alignment(pdata));
+	}
+end:
+	mutex_unlock(&probes_mutex);
+	ltt_unlock_traces();
+	return ret;
+}
+
+void ltt_dump_marker_state(struct ltt_trace_struct *trace)
+{
+	struct marker_iter iter;
+	struct ltt_active_marker *pdata;
+	struct ltt_probe_private_data call_data;
+
+	call_data.trace = trace;
+	call_data.force = 0;
+	call_data.channel = 0;
+	call_data.id = MARKER_CORE_IDS;
+
+	marker_iter_reset(&iter);
+	marker_iter_start(&iter);
+	for (; iter.marker != NULL; marker_iter_next(&iter)) {
+		if (_immediate_read(iter.marker->state)) {
+			pdata = iter.marker->private;
+			if (is_pdata_in_active_markers(pdata)) {
+				__trace_mark(0, core_marker_id, &call_data,
+					"name %s id %hu "
+					"int #1u%zu long #1u%zu pointer #1u%zu "
+					"size_t #1u%zu alignment #1u%u",
+					iter.marker->name, pdata->id,
+					sizeof(int), sizeof(long),
+					sizeof(void *), sizeof(size_t),
+					ltt_get_alignment(pdata));
+				if (iter.marker->format)
+					__trace_mark(0, core_marker_format,
+						&call_data,
+						"name %s format %s",
+						iter.marker->name,
+						iter.marker->format);
+			}
+		}
+	}
+	marker_iter_stop(&iter);
+}
+EXPORT_SYMBOL_GPL(ltt_dump_marker_state);
+
+/*
+ * function handling proc entry write.
+ *
+ * connect marker_name [probe_name] [id type (compact/dynamic)] [channel]
+ * disconnect marker_name
+ *
+ * Actions can be done on a connected, unarmed marker:
+ * set_id marker_name compact/dynamic
+ * set_channel marker_name channel
+ *
+ * arm marker_name
+ * disarm marker_name
+ *
+ * note : when a core probe registers, it directly establishes the connexion.
+ * Cannot be disconnected by users.
+ */
+static ssize_t ltt_write(struct file *file, const char __user *buffer,
+			   size_t count, loff_t *offset)
+{
+	char *kbuf;
+	char *iter, *marker_action, *marker_name = NULL, *probe_name = NULL,
+			*id = NULL, *channel = NULL;
+	ssize_t ret;
+	int channel_index;
+	enum marker_id n_id;
+
+	if (!count)
+		return -EINVAL;
+
+	kbuf = vmalloc(count + 1);
+	kbuf[count] = '\0';		/* Transform into a string */
+	ret = copy_from_user(kbuf, buffer, count);
+	if (ret) {
+		ret = -EINVAL;
+		goto end;
+	}
+	get_marker_string(kbuf, &marker_action, &iter);
+	if (!marker_action || marker_action == iter) {
+		ret = -EINVAL;
+		goto end;
+	}
+	if (iter < kbuf + count) {
+		iter++;			/* skip the added '\0' */
+		get_marker_string(iter, &marker_name, &iter);
+		if (marker_name == iter)
+			marker_name = NULL;
+	}
+	if (iter < kbuf + count) {
+		iter++;			/* skip the added '\0' */
+		get_marker_string(iter, &probe_name, &iter);
+		if (probe_name == iter)
+			probe_name = NULL;
+	}
+	if (iter < kbuf + count) {
+		iter++;			/* skip the added '\0' */
+		get_marker_string(iter, &id, &iter);
+		if (id == iter)
+			id = NULL;
+	}
+	if (iter < kbuf + count) {
+		iter++;			/* skip the added '\0' */
+		get_marker_string(iter, &channel, &iter);
+		if (channel == iter)
+			channel = NULL;
+	}
+
+	printk(KERN_DEBUG "Marker control : '%s' '%s' id: %s channel: %s\n",
+			marker_action, marker_name, id, channel);
+
+	if (!marker_name) {
+		ret = -EINVAL;
+		goto end;
+	}
+
+	if (!strcmp(marker_action, "arm")) {
+		ret = marker_arm(marker_name);
+		if (ret)
+			goto end;
+	} else if (!strcmp(marker_action, "disarm")) {
+		ret = marker_disarm(marker_name);
+		if (ret)
+			goto end;
+	} else if (!strcmp(marker_action, "connect")) {
+		channel_index = get_channel_index_from_name(channel);
+		if (channel_index < 0) {
+			ret = channel_index;
+			goto end;
+		}
+		n_id = get_id_from_name(id);
+		if (n_id < 0) {
+			ret = n_id;
+			goto end;
+		}
+		ret = ltt_marker_connect(marker_name, probe_name, n_id,
+					(uint16_t)channel_index, 1, 1);
+		if (ret)
+			goto end;
+	} else if (!strcmp(marker_action, "disconnect")) {
+		ret = ltt_marker_disconnect(marker_name, 1);
+		if (ret)
+			goto end;
+	} else if (!strcmp(marker_action, "set_id")) {
+		if (!id) {
+			ret = -EINVAL;
+			goto end;
+		}
+		n_id = get_id_from_name(id);
+		if (n_id < 0) {
+			ret = n_id;
+			goto end;
+		}
+		ret = do_set_id(marker_name, n_id, 1);
+		if (ret)
+			goto end;
+	} else if (!strcmp(marker_action, "set_channel")) {
+		if (!channel) {
+			ret = -EINVAL;
+			goto end;
+		}
+		channel_index = get_channel_index_from_name(channel);
+		if (channel_index < 0) {
+			ret = channel_index;
+			goto end;
+		}
+		ret = do_set_channel(marker_name, (uint16_t)channel_index, 1);
+		if (ret)
+			goto end;
+	} else {
+		ret = -EINVAL;
+		goto end;
+	}
+	ret = count;
+end:
+	vfree(kbuf);
+	return ret;
+}
+
+static void *s_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	struct marker_iter *iter = m->private;
+
+	marker_iter_next(iter);
+	if (!iter->marker) {
+		/*
+		 * Setting the iter module to -1UL will make sure
+		 * that no module can possibly hold the current marker.
+		 */
+		iter->module = (void *)-1UL;
+		return NULL;
+	}
+	return iter->marker;
+}
+
+static void *s_start(struct seq_file *m, loff_t *pos)
+{
+	struct marker_iter *iter = m->private;
+
+	if (!*pos)
+		marker_iter_reset(iter);
+	marker_iter_start(iter);
+	if (!iter->marker) {
+		/*
+		 * Setting the iter module to -1UL will make sure
+		 * that no module can possibly hold the current marker.
+		 */
+		iter->module = (void *)-1UL;
+		return NULL;
+	}
+	return iter->marker;
+}
+
+static void s_stop(struct seq_file *m, void *p)
+{
+	marker_iter_stop(m->private);
+}
+
+static int s_show(struct seq_file *m, void *p)
+{
+	struct marker_iter *iter = m->private;
+
+	seq_printf(m, "marker: %s format: \"%s\" state: %d "
+		"probe: 0x%p pdata: 0x%p\n",
+		iter->marker->name, iter->marker->format,
+		_immediate_read(iter->marker->state), iter->marker->call,
+		iter->marker->private);
+	return 0;
+}
+
+static const struct seq_operations ltt_seq_op = {
+	.start = s_start,
+	.next = s_next,
+	.stop = s_stop,
+	.show = s_show,
+};
+
+static int ltt_open(struct inode *inode, struct file *file)
+{
+	/*
+	 * Iterator kept in m->private.
+	 * Restart iteration on all modules between reads because we do not lock
+	 * the module mutex between those.
+	 */
+	int ret;
+	struct marker_iter *iter;
+
+	iter = kzalloc(sizeof(*iter), GFP_KERNEL);
+	if (!iter)
+		return -ENOMEM;
+
+	ret = seq_open(file, &ltt_seq_op);
+	if (ret == 0)
+		((struct seq_file *)file->private_data)->private = iter;
+	else
+		kfree(iter);
+	return ret;
+}
+
+static struct file_operations ltt_fops = {
+	.write = ltt_write,
+	.open = ltt_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release_private,
+};
+
+static void disconnect_all_markers(void)
+{
+	struct ltt_active_marker *pdata, *tmp;
+
+	list_for_each_entry_safe(pdata, tmp, &markers_loaded_list, node) {
+		marker_probe_unregister_private_data(pdata);
+		list_del(&pdata->node);
+		kmem_cache_free(markers_loaded_cachep, pdata);
+	}
+}
+
+static int __init marker_control_init(void)
+{
+	int ret;
+
+	pentry = create_proc_entry("ltt", S_IWUSR, NULL);
+	if (!pentry)
+		return -EBUSY;
+	markers_loaded_cachep = KMEM_CACHE(ltt_active_marker, 0);
+
+	ret = ltt_probe_register(&default_probe);
+	BUG_ON(ret);
+	ret = ltt_marker_connect("core_marker_format", DEFAULT_PROBE,
+		MARKER_ID_SET_MARKER_FORMAT, GET_CHANNEL_INDEX(facilities),
+		0, 1);
+	BUG_ON(ret);
+	ret = ltt_marker_connect("core_marker_id", DEFAULT_PROBE,
+		MARKER_ID_SET_MARKER_ID, GET_CHANNEL_INDEX(facilities),
+		0, 1);
+	BUG_ON(ret);
+#ifdef CONFIG_LTT_HEARTBEAT
+	ltt_init_compact_markers(mark_next_compact_id);
+	ret = ltt_marker_connect("core_heartbeat_32", DEFAULT_PROBE,
+		MARKER_ID_HEARTBEAT_32, 0, 0, 1);
+	BUG_ON(ret);
+	ret = ltt_marker_connect("core_heartbeat_64", DEFAULT_PROBE,
+		MARKER_ID_HEARTBEAT_64, 0, 0, 1);
+	BUG_ON(ret);
+#endif
+	pentry->proc_fops = &ltt_fops;
+
+	return 0;
+}
+module_init(marker_control_init);
+
+static void __exit marker_control_exit(void)
+{
+	int ret;
+
+	remove_proc_entry("ltt", NULL);
+	ret = ltt_marker_disconnect("core_marker_format", 0);
+	BUG_ON(ret);
+	ret = ltt_marker_disconnect("core_marker_id", 0);
+	BUG_ON(ret);
+#ifdef CONFIG_LTT_HEARTBEAT
+	ret = ltt_marker_disconnect("core_heartbeat_32", 0);
+	BUG_ON(ret);
+	ret = ltt_marker_disconnect("core_heartbeat_64", 0);
+	BUG_ON(ret);
+#endif
+	ret = ltt_probe_unregister(&default_probe);
+	BUG_ON(ret);
+	disconnect_all_markers();
+	kmem_cache_destroy(markers_loaded_cachep);
+}
+module_exit(marker_control_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Marker Control");
diff -uprN linux-2.6.23.1.orig/ltt/ltt-relay.c linux-2.6.23.1/ltt/ltt-relay.c
--- linux-2.6.23.1.orig/ltt/ltt-relay.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-relay.c	2007-11-28 10:13:52.373346505 +0100
@@ -0,0 +1,1282 @@
+/*
+ * (C) Copyright 2005-2006 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Contains the kernel code for the Linux Trace Toolkit.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *	Karim Yaghmour (karim@opersys.com)
+ *	Tom Zanussi (zanussi@us.ibm.com)
+ *	Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  19/10/05, Complete lockless mechanism. (Mathieu Desnoyers)
+ *	27/05/05, Modular redesign and rewrite. (Mathieu Desnoyers)
+
+ * Comments :
+ * num_active_traces protects the functors. Changing the pointer is an atomic
+ * operation, but the functions can only be called when in tracing. It is then
+ * safe to unload a module in which sits a functor when no tracing is active.
+ *
+ * filter_control functor is protected by incrementing its module refcount.
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+
+static struct dentry *ltt_root_dentry;
+static struct file_operations ltt_file_operations;
+
+/*
+ * A switch is done during tracing or as a final flush after tracing (so it
+ * won't write in the new sub-buffer).
+ */
+enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan,
+		struct rchan_buf *buf,
+		unsigned int cpu,
+		unsigned n_subbufs);
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu);
+
+static void ltt_force_switch(struct rchan_buf *buf,
+		enum force_switch_mode mode);
+
+/*
+ * Trace callbacks
+ */
+static void ltt_buffer_begin_callback(struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_block_start_header *header =
+		(struct ltt_block_start_header *)
+			(buf->start + (subbuf_idx*buf->chan->subbuf_size));
+
+	header->begin.cycle_count = tsc;
+	header->begin.freq = ltt_frequency();
+	header->lost_size = 0xFFFFFFFF; /* for debugging */
+	header->buf_size = buf->chan->subbuf_size;
+	ltt_write_trace_header(channel->trace, &header->trace);
+}
+
+/*
+ * offset is assumed to never be 0 here : never deliver a completely empty
+ * subbuffer. The lost size is between 0 and subbuf_size-1.
+ */
+static void ltt_buffer_end_callback(struct rchan_buf *buf,
+		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
+{
+	struct ltt_block_start_header *header =
+		(struct ltt_block_start_header *)
+			(buf->start + (subbuf_idx*buf->chan->subbuf_size));
+
+	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
+				buf->chan);
+	header->end.cycle_count = tsc;
+	header->end.freq = ltt_frequency();
+}
+
+static int ltt_subbuf_start_callback(struct rchan_buf *buf,
+		void *subbuf,
+		void *prev_subbuf,
+		size_t prev_padding)
+{
+	return 0;
+}
+
+
+
+static void ltt_deliver(struct rchan_buf *buf, unsigned subbuf_idx,
+		void *subbuf)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &channel->buf[buf->cpu];
+
+	atomic_set(&ltt_buf->wakeup_readers, 1);
+}
+
+static void ltt_buf_mapped_callback(struct rchan_buf *buf,
+		struct file *filp)
+{
+}
+
+static void ltt_buf_unmapped_callback(struct rchan_buf *buf,
+		struct file *filp)
+{
+}
+
+static struct dentry *ltt_create_buf_file_callback(const char *filename,
+		struct dentry *parent, int mode,
+		struct rchan_buf *buf, int *is_global)
+{
+	struct ltt_channel_struct *ltt_chan;
+	int err;
+	struct dentry *dentry;
+
+	ltt_chan = buf->chan->private_data;
+	err = ltt_relay_create_buffer(ltt_chan->trace, ltt_chan,
+					buf, buf->cpu,
+					buf->chan->n_subbufs);
+	if (err)
+		return ERR_PTR(err);
+
+	dentry = debugfs_create_file(filename, mode, parent, buf,
+			&ltt_file_operations);
+	if (!dentry)
+		goto error;
+	return dentry;
+error:
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+	return NULL;
+}
+
+static int ltt_remove_buf_file_callback(struct dentry *dentry)
+{
+	struct rchan_buf *buf = dentry->d_inode->i_private;
+	struct ltt_channel_struct *ltt_chan = buf->chan->private_data;
+
+	debugfs_remove(dentry);
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+
+	return 0;
+}
+
+/*
+ * This function should not be called from NMI interrupt context
+ */
+static void ltt_buf_unfull(struct rchan_buf *buf,
+		unsigned subbuf_idx,
+		void *subbuf)
+{
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	if (waitqueue_active(&ltt_buf->write_wait))
+		schedule_work(&ltt_buf->wake_writers);
+}
+
+/**
+ *	poll file op for ltt files
+ *	@filp: the file
+ *	@wait: poll table
+ *
+ *	Poll implementation.
+ */
+static unsigned int ltt_poll(struct file *filp, poll_table *wait)
+{
+	unsigned int mask = 0;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct rchan_buf *buf = inode->i_private;
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+
+	if (filp->f_mode & FMODE_READ) {
+		poll_wait(filp, &buf->read_wait, wait);
+
+		if (atomic_long_read(&ltt_buf->active_readers) != 0) {
+			return 0;
+		} else {
+			if (SUBBUF_TRUNC(
+				local_read(&ltt_buf->offset), buf->chan)
+					- SUBBUF_TRUNC(
+				atomic_long_read(&ltt_buf->consumed), buf->chan)
+					== 0) {
+				if (buf->finalized)
+					return POLLHUP;
+				else
+					return 0;
+			} else {
+				struct rchan *rchan =
+					ltt_channel->trans_channel_data;
+				if (SUBBUF_TRUNC(local_read(&ltt_buf->offset),
+							buf->chan)
+						- SUBBUF_TRUNC(
+				atomic_long_read(&ltt_buf->consumed), buf->chan)
+						>= rchan->alloc_size)
+					return POLLPRI | POLLRDBAND;
+				else
+					return POLLIN | POLLRDNORM;
+			}
+		}
+	}
+	return mask;
+}
+
+
+/**
+ *	ioctl control on the debugfs file
+ *
+ *	@inode: the inode
+ *	@filp: the file
+ *	@cmd: the command
+ *	@arg: command arg
+ *
+ *	This ioctl implements three commands necessary for a minimal
+ *	producer/consumer implementation :
+ *	RELAY_GET_SUBBUF
+ *		Get the next sub buffer that can be read. It never blocks.
+ *	RELAY_PUT_SUBBUF
+ *		Release the currently read sub-buffer. Parameter is the last
+ *		put subbuffer (returned by GET_SUBBUF).
+ *	RELAY_GET_N_BUBBUFS
+ *		returns the number of sub buffers in the per cpu channel.
+ *	RELAY_GET_SUBBUF_SIZE
+ *		returns the size of the sub buffers.
+ */
+static int ltt_ioctl(struct inode *inode, struct file *filp,
+		unsigned int cmd, unsigned long arg)
+{
+	struct rchan_buf *buf = inode->i_private;
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	u32 __user *argp = (u32 __user *)arg;
+
+	switch (cmd) {
+	case RELAY_GET_SUBBUF:
+	{
+		long consumed_old, consumed_idx;
+		atomic_long_inc(&ltt_buf->active_readers);
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+		if (SUBBUF_OFFSET(
+			local_read(&ltt_buf->commit_count[consumed_idx]),
+				buf->chan) != 0) {
+			atomic_long_dec(&ltt_buf->active_readers);
+			return -EAGAIN;
+		}
+		if ((SUBBUF_TRUNC(local_read(&ltt_buf->offset), buf->chan)
+				- SUBBUF_TRUNC(consumed_old, buf->chan)) == 0) {
+			atomic_long_dec(&ltt_buf->active_readers);
+			return -EAGAIN;
+		}
+		/* must make sure we read the counter before reading the data
+		 * in the buffer. */
+		smp_rmb();
+		return put_user((u32)consumed_old, argp);
+		break;
+	}
+	case RELAY_PUT_SUBBUF:
+	{
+		u32 uconsumed_old;
+		int ret;
+		long consumed_new, consumed_old;
+
+		ret = get_user(uconsumed_old, argp);
+		if (ret)
+			return ret; /* will return -EFAULT */
+
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		consumed_old = consumed_old & (~0xFFFFFFFFL);
+		consumed_old = consumed_old | uconsumed_old;
+		consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+
+		spin_lock(&ltt_buf->full_lock);
+		if (atomic_long_cmpxchg(
+			&ltt_buf->consumed, consumed_old, consumed_new)
+				!= consumed_old) {
+			/* We have been pushed by the writer : the last
+			 * buffer read _is_ corrupted! It can also
+			 * happen if this is a buffer we never got. */
+			atomic_long_dec(&ltt_buf->active_readers);
+			spin_unlock(&ltt_buf->full_lock);
+			return -EIO;
+		} else {
+			/* tell the client that buffer is now unfull */
+			int index;
+			void *data;
+			index = SUBBUF_INDEX(consumed_old, buf->chan);
+			data = buf->start +
+				BUFFER_OFFSET(consumed_old, buf->chan);
+			ltt_buf_unfull(buf, index, data);
+			atomic_long_dec(&ltt_buf->active_readers);
+			spin_unlock(&ltt_buf->full_lock);
+		}
+		break;
+	}
+	case RELAY_GET_N_SUBBUFS:
+		return put_user((u32)buf->chan->n_subbufs, argp);
+		break;
+	case RELAY_GET_SUBBUF_SIZE:
+		return put_user((u32)buf->chan->subbuf_size, argp);
+		break;
+	default:
+		return -ENOIOCTLCMD;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+static long ltt_compat_ioctl(struct file *file, unsigned cmd, unsigned long arg)
+{
+	long ret = -ENOIOCTLCMD;
+
+	lock_kernel();
+	ret = ltt_ioctl(file->f_dentry->d_inode, file, cmd, arg);
+	unlock_kernel();
+
+	return ret;
+}
+#endif
+
+static void ltt_relay_print_subbuffer_errors(
+		struct ltt_channel_struct *ltt_chan,
+		long cons_off, unsigned int i)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	long cons_idx;
+
+	printk(KERN_WARNING
+		"LTT : unread channel %s offset is %ld "
+		"and cons_off : %ld (cpu %u)\n",
+		ltt_chan->channel_name,
+		local_read(&ltt_chan->buf[i].offset), cons_off, i);
+	/* Check each sub-buffer for non zero commit count */
+	cons_idx = SUBBUF_INDEX(cons_off, rchan);
+	if (SUBBUF_OFFSET(local_read(&ltt_chan->buf[i].commit_count[cons_idx]),
+				rchan))
+		printk(KERN_ALERT
+			"LTT : %s : subbuffer %lu has non zero "
+			"commit count.\n",
+			ltt_chan->channel_name, cons_idx);
+	printk(KERN_ALERT "LTT : %s : commit count : %lu, subbuf size %zd\n",
+			ltt_chan->channel_name,
+			local_read(&ltt_chan->buf[i].commit_count[cons_idx]),
+			rchan->subbuf_size);
+}
+
+static void ltt_relay_print_errors(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan, int cpu)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	long cons_off;
+
+	for (cons_off = atomic_long_read(&ltt_chan->buf[cpu].consumed);
+			(SUBBUF_TRUNC(local_read(&ltt_chan->buf[cpu].offset),
+				rchan) - cons_off) > 0;
+			cons_off = SUBBUF_ALIGN(cons_off, rchan))
+		ltt_relay_print_subbuffer_errors(ltt_chan, cons_off, cpu);
+}
+
+static void ltt_relay_print_buffer_errors(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+
+	if (local_read(&ltt_chan->buf[cpu].events_lost))
+		printk(KERN_ALERT
+			"LTT : %s : %ld events lost "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			local_read(&ltt_chan->buf[cpu].events_lost),
+			ltt_chan->channel_name, cpu);
+	if (local_read(&ltt_chan->buf[cpu].corrupted_subbuffers))
+		printk(KERN_ALERT
+			"LTT : %s : %ld corrupted subbuffers "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			local_read(
+				&ltt_chan->buf[cpu].corrupted_subbuffers),
+			ltt_chan->channel_name, cpu);
+
+	ltt_relay_print_errors(trace, ltt_chan, cpu);
+}
+
+static void ltt_relay_remove_dirs(struct ltt_trace_struct *trace)
+{
+	debugfs_remove(trace->dentry.control_root);
+	debugfs_remove(trace->dentry.trace_root);
+}
+
+static void ltt_relay_release_channel(struct kref *kref)
+{
+	struct ltt_channel_struct *ltt_chan = container_of(kref,
+			struct ltt_channel_struct, kref);
+	kfree(ltt_chan);
+}
+
+/*
+ * Create ltt buffer.
+ */
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan, struct rchan_buf *buf,
+		unsigned int cpu, unsigned n_subbufs)
+{
+	unsigned int j;
+
+	ltt_chan->buf[cpu].commit_count =
+		kmalloc(sizeof(local_t) * n_subbufs, GFP_KERNEL);
+	if (!ltt_chan->buf[cpu].commit_count)
+		return -ENOMEM;
+	kref_get(&trace->kref);
+	kref_get(&trace->ltt_transport_kref);
+	kref_get(&ltt_chan->kref);
+	local_set(&ltt_chan->buf[cpu].offset,
+		ltt_subbuf_header_len());
+	atomic_long_set(&ltt_chan->buf[cpu].consumed, 0);
+	atomic_long_set(&ltt_chan->buf[cpu].active_readers, 0);
+	for (j = 0; j < n_subbufs; j++)
+		local_set(&ltt_chan->buf[cpu].commit_count[j], 0);
+	init_waitqueue_head(&ltt_chan->buf[cpu].write_wait);
+	atomic_set(&ltt_chan->buf[cpu].wakeup_readers, 0);
+	INIT_WORK(&ltt_chan->buf[cpu].wake_writers, ltt_wakeup_writers);
+	spin_lock_init(&ltt_chan->buf[cpu].full_lock);
+
+	ltt_buffer_begin_callback(buf, trace->start_tsc, 0);
+	/* atomic_add made on local variable on data that belongs to
+	 * various CPUs : ok because tracing not started (for this cpu). */
+	local_add(ltt_subbuf_header_len(),
+		&ltt_chan->buf[cpu].commit_count[0]);
+
+	local_set(&ltt_chan->buf[cpu].events_lost, 0);
+	local_set(&ltt_chan->buf[cpu].corrupted_subbuffers, 0);
+
+	return 0;
+}
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+
+	kref_put(&ltt_chan->trace->ltt_transport_kref,
+		ltt_release_transport);
+	ltt_relay_print_buffer_errors(ltt_chan, cpu);
+	kfree(ltt_chan->buf[cpu].commit_count);
+	ltt_chan->buf[cpu].commit_count = NULL;
+	kref_put(&ltt_chan->kref, ltt_relay_release_channel);
+	kref_put(&trace->kref, ltt_release_trace);
+}
+
+/*
+ * Create channel.
+ */
+static int ltt_relay_create_channel(char *trace_name,
+		struct ltt_trace_struct *trace, struct dentry *dir,
+		char *channel_name, struct ltt_channel_struct **ltt_chan,
+		unsigned int subbuf_size, unsigned int n_subbufs,
+		int overwrite)
+{
+	char *tmpname;
+	unsigned int tmpname_len;
+	int err = 0;
+
+	tmpname = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (!tmpname)
+		return EPERM;
+	if (overwrite) {
+		strncpy(tmpname, LTT_FLIGHT_PREFIX, PATH_MAX-1);
+		strncat(tmpname, channel_name,
+			PATH_MAX-1-sizeof(LTT_FLIGHT_PREFIX));
+	} else {
+		strncpy(tmpname, channel_name, PATH_MAX-1);
+	}
+	strncat(tmpname, "_", PATH_MAX-1-strlen(tmpname));
+
+	*ltt_chan = kzalloc(sizeof(struct ltt_channel_struct), GFP_KERNEL);
+	if (!(*ltt_chan))
+		goto ltt_chan_alloc_error;
+	kref_init(&(*ltt_chan)->kref);
+
+	(*ltt_chan)->trace = trace;
+	(*ltt_chan)->buffer_begin = ltt_buffer_begin_callback;
+	(*ltt_chan)->buffer_end = ltt_buffer_end_callback;
+	(*ltt_chan)->overwrite = overwrite;
+	if (strcmp(channel_name, LTT_COMPACT_CHANNEL) == 0)
+		(*ltt_chan)->compact = 1;
+	else
+		(*ltt_chan)->compact = 0;
+	(*ltt_chan)->trans_channel_data = relay_open(tmpname,
+			dir,
+			subbuf_size,
+			n_subbufs,
+			&trace->callbacks,
+			*ltt_chan);
+	tmpname_len = strlen(tmpname);
+	if (tmpname_len > 0) {
+		/* Remove final _ for pretty printing */
+		tmpname[tmpname_len-1] = '\0';
+	}
+	if ((*ltt_chan)->trans_channel_data == NULL) {
+		printk(KERN_ERR "LTT : Can't open %s channel for trace %s\n",
+				tmpname, trace_name);
+		goto relay_open_error;
+	}
+
+	strncpy((*ltt_chan)->channel_name, tmpname, PATH_MAX-1);
+
+	err = 0;
+	goto end;
+
+relay_open_error:
+	kfree(*ltt_chan);
+	*ltt_chan = NULL;
+ltt_chan_alloc_error:
+	err = EPERM;
+end:
+	kfree(tmpname);
+	return err;
+}
+
+static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
+{
+	new_trace->dentry.trace_root = debugfs_create_dir(new_trace->trace_name,
+			ltt_root_dentry);
+	if (new_trace->dentry.trace_root == NULL) {
+		printk(KERN_ERR "LTT : Trace directory name %s already taken\n",
+				new_trace->trace_name);
+		return EEXIST;
+	}
+
+	new_trace->dentry.control_root = debugfs_create_dir(LTT_CONTROL_ROOT,
+			new_trace->dentry.trace_root);
+	if (new_trace->dentry.control_root == NULL) {
+		printk(KERN_ERR "LTT : Trace control subdirectory name "\
+				"%s/%s already taken\n",
+				new_trace->trace_name, LTT_CONTROL_ROOT);
+		debugfs_remove(new_trace->dentry.trace_root);
+		return EEXIST;
+	}
+
+	new_trace->callbacks.subbuf_start = ltt_subbuf_start_callback;
+	new_trace->callbacks.buf_mapped = ltt_buf_mapped_callback;
+	new_trace->callbacks.buf_unmapped = ltt_buf_unmapped_callback;
+	new_trace->callbacks.create_buf_file = ltt_create_buf_file_callback;
+	new_trace->callbacks.remove_buf_file = ltt_remove_buf_file_callback;
+
+	return 0;
+}
+
+/*
+ * LTTng channel flush function.
+ *
+ * Must be called when no tracing is active in the channel, because of
+ * accesses across CPUs.
+ */
+static void ltt_relay_buffer_flush(struct rchan_buf *buf)
+{
+	buf->finalized = 1;
+	ltt_force_switch(buf, FORCE_FLUSH);
+}
+
+static void ltt_relay_async_wakeup_chan(struct ltt_channel_struct *ltt_channel)
+{
+	unsigned int i;
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+
+	for_each_possible_cpu(i) {
+		if (atomic_read(&ltt_channel->buf[i].wakeup_readers) == 1) {
+			atomic_set(&ltt_channel->buf[i].wakeup_readers, 0);
+			wake_up_interruptible(&rchan->buf[i]->read_wait);
+		}
+	}
+}
+
+/*
+ * Wake writers :
+ *
+ * This must be done after the trace is removed from the RCU list so that there
+ * are no stalled writers.
+ */
+static void ltt_relay_wake_writers(struct ltt_channel_buf_struct *ltt_buf)
+{
+
+	if (waitqueue_active(&ltt_buf->write_wait))
+		schedule_work(&ltt_buf->wake_writers);
+}
+
+static void ltt_relay_finish_buffer(struct ltt_channel_struct *ltt_channel,
+		unsigned int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf;
+
+	if (rchan->buf[cpu]) {
+		ltt_buf = &ltt_channel->buf[cpu];;
+		ltt_relay_buffer_flush(rchan->buf[cpu]);
+		ltt_relay_wake_writers(ltt_buf);
+	}
+}
+
+
+static void ltt_relay_finish_channel(struct ltt_channel_struct *ltt_channel)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i) {
+		ltt_relay_finish_buffer(ltt_channel, i);
+	}
+}
+
+static void ltt_relay_remove_channel(struct ltt_channel_struct *channel)
+{
+	struct rchan *rchan = channel->trans_channel_data;
+
+	relay_close(rchan);
+	kref_put(&channel->kref, ltt_relay_release_channel);
+}
+
+struct ltt_reserve_switch_offsets {
+	long begin, end, old;
+	long begin_switch, end_switch_current, end_switch_old;
+	long commit_count, reserve_commit_diff;
+	size_t before_hdr_pad, size;
+};
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static inline int ltt_relay_try_reserve(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
+		u64 *tsc)
+{
+	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_current = 0;
+	offsets->end_switch_old = 0;
+
+	if (SUBBUF_OFFSET(offsets->begin, buf->chan) == 0) {
+		offsets->begin_switch = 1;		/* For offsets->begin */
+	} else {
+		offsets->size = ltt_get_header_size(ltt_channel,
+				buf->start + offsets->begin, data_size,
+				&offsets->before_hdr_pad) + data_size;
+		if ((SUBBUF_OFFSET(offsets->begin, buf->chan) + offsets->size)
+				> buf->chan->subbuf_size) {
+			offsets->end_switch_old = 1;	/* For offsets->old */
+			offsets->begin_switch = 1;	/* For offsets->begin */
+		}
+	}
+	if (offsets->begin_switch) {
+		if (offsets->end_switch_old)
+			offsets->begin = SUBBUF_ALIGN(offsets->begin,
+						buf->chan);
+		offsets->begin = offsets->begin + ltt_subbuf_header_len();
+		/* Test new buffer integrity */
+		offsets->reserve_commit_diff =
+			SUBBUF_OFFSET(buf->chan->subbuf_size
+			- local_read(&ltt_buf->commit_count[
+				SUBBUF_INDEX(offsets->begin, buf->chan)]),
+					buf->chan);
+		if (offsets->reserve_commit_diff == 0) {
+			/* Next buffer not corrupted. */
+			if (!ltt_channel->overwrite &&
+				(SUBBUF_TRUNC(offsets->begin, buf->chan)
+				- SUBBUF_TRUNC(
+					atomic_long_read(&ltt_buf->consumed),
+					buf->chan))
+				>= rchan->alloc_size) {
+				/*
+				 * We do not overwrite non consumed buffers
+				 * and we are full : event is lost.
+				 */
+				local_inc(&ltt_buf->events_lost);
+				return -1;
+			} else {
+				/*
+				 * next buffer not corrupted, we are either in
+				 * overwrite mode or the buffer is not full.
+				 * It's safe to write in this new subbuffer.
+				 */
+			}
+		} else {
+			/*
+			 * Next subbuffer corrupted. Force pushing reader even
+			 * in normal mode. It's safe to write in this new
+			 * subbuffer.
+			 */
+		}
+		offsets->size = ltt_get_header_size(ltt_channel,
+			buf->start + offsets->begin, data_size,
+			&offsets->before_hdr_pad) + data_size;
+		if ((SUBBUF_OFFSET(offsets->begin, buf->chan) + offsets->size)
+				> buf->chan->subbuf_size) {
+			/*
+			 * Event too big for subbuffers, report error, don't
+			 * complete the sub-buffer switch.
+			 */
+			local_inc(&ltt_buf->events_lost);
+			return -1;
+		} else {
+			/*
+			 * We just made a successful buffer switch and the event
+			 * fits in the new subbuffer. Let's write.
+			 */
+		}
+	} else {
+		/*
+		 * Event fits in the current buffer and we are not on a switch
+		 * boundary. It's safe to write.
+		 */
+	}
+	offsets->end = offsets->begin + offsets->size;
+
+	if ((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0) {
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		offsets->end_switch_current = 1;	/* For offsets->begin */
+	}
+#ifdef CONFIG_LTT_HEARTBEAT
+	if (offsets->begin_switch || offsets->end_switch_old
+			|| offsets->end_switch_current)
+		*tsc = ltt_get_timestamp64();
+	else
+		*tsc = ltt_get_timestamp32();
+#else
+	*tsc = ltt_get_timestamp64();
+#endif
+	return 0;
+}
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static inline int ltt_relay_try_switch(
+		enum force_switch_mode mode,
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets,
+		u64 *tsc)
+{
+	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->old = offsets->begin;
+	offsets->end_switch_old = 0;
+
+	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
+		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
+		offsets->end_switch_old = 1;
+	} else {
+		/* we do not have to switch : buffer is empty */
+		return -1;
+	}
+	if (mode == FORCE_ACTIVE)
+		offsets->begin += ltt_subbuf_header_len();
+	/*
+	 * Always begin_switch in FORCE_ACTIVE mode.
+	 * Test new buffer integrity
+	 */
+	offsets->reserve_commit_diff = SUBBUF_OFFSET(buf->chan->subbuf_size
+		- local_read(
+		&ltt_buf->commit_count[SUBBUF_INDEX(offsets->begin,
+					buf->chan)]), buf->chan);
+	if (offsets->reserve_commit_diff == 0) {
+		/* Next buffer not corrupted. */
+		if (mode == FORCE_ACTIVE && !ltt_channel->overwrite &&
+		(offsets->begin - atomic_long_read(&ltt_buf->consumed))
+			>= rchan->alloc_size) {
+			/*
+			 * We do not overwrite non consumed buffers and we are
+			 * full : ignore switch while tracing is active.
+			 */
+			return -1;
+		}
+	} else {
+		/*
+		 * Next subbuffer corrupted. Force pushing reader even in normal
+		 * mode
+		 */
+	}
+	offsets->end = offsets->begin;
+	*tsc = ltt_get_timestamp64();
+	return 0;
+}
+
+static inline void ltt_reserve_push_reader(
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets)
+{
+	long consumed_old, consumed_new;
+
+	do {
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		/*
+		 * If buffer is in overwrite mode, push the reader consumed
+		 * count if the write position has reached it and we are not
+		 * at the first iteration (don't push the reader farther than
+		 * the writer). This operation can be done concurrently by many
+		 * writers in the same buffer, the writer being at the farthest
+		 * write position sub-buffer index in the buffer being the one
+		 * which will win this loop.
+		 * If the buffer is not in overwrite mode, pushing the reader
+		 * only happens if a sub-buffer is corrupted.
+		 */
+		if ((SUBBUF_TRUNC(offsets->end-1, buf->chan)
+			- SUBBUF_TRUNC(consumed_old, buf->chan))
+					>= rchan->alloc_size)
+			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		else {
+			consumed_new = consumed_old;
+			break;
+		}
+	} while (atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+			consumed_new) != consumed_old);
+
+	if (consumed_old != consumed_new) {
+		/*
+		 * Reader pushed : we are the winner of the push, we can
+		 * therefore reequilibrate reserve and commit. Atomic increment
+		 * of the commit count permits other writers to play around
+		 * with this variable before us. We keep track of
+		 * corrupted_subbuffers even in overwrite mode :
+		 * we never want to write over a non completely committed
+		 * sub-buffer : possible causes : the buffer size is too low
+		 * compared to the unordered data input, or there is a writer
+		 * that died between the reserve and the commit.
+		 */
+		if (offsets->reserve_commit_diff) {
+			/*
+			 * We have to alter the sub-buffer commit count : a
+			 * sub-buffer is corrupted. We do not deliver it.
+			 */
+			local_add(
+				offsets->reserve_commit_diff,
+				&ltt_buf->commit_count[
+				SUBBUF_INDEX(offsets->begin, buf->chan)]);
+			local_inc(&ltt_buf->corrupted_subbuffers);
+		}
+	}
+}
+
+
+/*
+ * ltt_reserve_switch_old_subbuf: switch old subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ *
+ * Note : offset_old should never be 0 here.
+ */
+static inline void ltt_reserve_switch_old_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+		ltt_channel->buffer_end(buf, *tsc, offsets->old,
+			SUBBUF_INDEX((offsets->old-1), buf->chan));
+		/* Must write buffer end before incrementing commit count */
+		smp_wmb();
+		offsets->commit_count =
+			local_add_return(buf->chan->subbuf_size
+				- (SUBBUF_OFFSET(offsets->old-1, buf->chan)+1),
+				&ltt_buf->commit_count[SUBBUF_INDEX(
+						offsets->old-1, buf->chan)]);
+		if (SUBBUF_OFFSET(offsets->commit_count, buf->chan) == 0)
+			ltt_deliver(buf, SUBBUF_INDEX((offsets->old-1),
+						buf->chan), NULL);
+}
+
+/*
+ * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
+ *
+ * This code can be executed unordered : writers may already have written to the
+ * sub-buffer before this code gets executed, caution.  The commit makes sure
+ * that this code is executed before the deliver of this sub-buffer.
+ */
+static inline void ltt_reserve_switch_new_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	ltt_channel->buffer_begin(buf, *tsc, SUBBUF_INDEX(offsets->begin,
+				buf->chan));
+	/* Must write buffer end before incrementing commit count */
+	smp_wmb();
+	offsets->commit_count = local_add_return(ltt_subbuf_header_len(),
+			&ltt_buf->commit_count[
+				SUBBUF_INDEX(offsets->begin, buf->chan)]);
+	/* Check if the written buffer has to be delivered */
+	if (SUBBUF_OFFSET(offsets->commit_count, buf->chan) == 0)
+		ltt_deliver(buf, SUBBUF_INDEX(offsets->begin, buf->chan), NULL);
+}
+
+
+/*
+ * ltt_reserve_end_switch_current: finish switching current subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ */
+static inline void ltt_reserve_end_switch_current(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	ltt_channel->buffer_end(buf, *tsc, offsets->end,
+		SUBBUF_INDEX((offsets->end-1), buf->chan));
+	/* Must write buffer begin before incrementing commit count */
+	smp_wmb();
+	offsets->commit_count =
+		local_add_return(buf->chan->subbuf_size
+			- (SUBBUF_OFFSET(offsets->end-1, buf->chan)+1),
+			&ltt_buf->commit_count[SUBBUF_INDEX(
+					offsets->end-1, buf->chan)]);
+	if (SUBBUF_OFFSET(offsets->commit_count, buf->chan) == 0)
+		ltt_deliver(buf, SUBBUF_INDEX((offsets->end-1),
+			buf->chan), NULL);
+}
+
+/**
+ * ltt_relay_reserve_slot: Atomic slot reservation in a LTTng buffer.
+ * @trace : the trace structure to log to.
+ * @ltt_channel : channel structure
+ * @transport_data : data structure specific to ltt relay
+ * @data_size : size of the variable length data to log.
+ * @slot_size : pointer to total size of the slot (out)
+ * @tsc : pointer to the tsc at the slot reservation (out)
+ * @cpu : cpuid
+ *
+ * Return : NULL if not enough space, else returns the pointer
+ * 		to the beginning of the reserved slot, aligned for the
+ * 		event header.
+ * It will take care of sub-buffer switching.
+ */
+static void *ltt_relay_reserve_slot(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, u64 *tsc, int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data =
+		rchan->buf[cpu];
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+
+	struct ltt_reserve_switch_offsets offsets;
+
+	offsets.reserve_commit_diff = 0;
+	offsets.size = 0;
+
+	/*
+	 * Perform retryable operations.
+	 */
+	if (ltt_nesting[smp_processor_id()] > 4) {
+		local_inc(&ltt_buf->events_lost);
+		return NULL;
+	}
+	do {
+
+		if (ltt_relay_try_reserve(ltt_channel, ltt_buf,
+				rchan, buf, &offsets, data_size, tsc))
+			return NULL;
+	} while (local_cmpxchg(&ltt_buf->offset, offsets.old,
+			offsets.end) != offsets.old);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(ltt_buf, rchan, buf, &offsets);
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (offsets.end_switch_old)
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, tsc);
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (offsets.begin_switch)
+		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	if (offsets.end_switch_current)
+		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	*slot_size = offsets.size;
+
+	return buf->start + BUFFER_OFFSET(offsets.begin, buf->chan)
+		+ offsets.before_hdr_pad;
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ *
+ * Note, however, that as a local_cmpxchg is used for some atomic
+ * operations, this function must be called from the CPU which owns the buffer
+ * for a ACTIVE flush.
+ */
+static void ltt_force_switch(struct rchan_buf *buf, enum force_switch_mode mode)
+{
+	struct ltt_channel_struct *ltt_channel =
+			(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_reserve_switch_offsets offsets;
+	u64 tsc;
+
+	offsets.reserve_commit_diff = 0;
+	offsets.size = 0;
+
+	/*
+	 * Perform retryable operations.
+	 */
+	do {
+		if (ltt_relay_try_switch(mode, ltt_channel, ltt_buf,
+				rchan, buf, &offsets, &tsc))
+			return;
+	} while (local_cmpxchg(&ltt_buf->offset, offsets.old,
+			offsets.end) != offsets.old);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_push_reader(ltt_buf, rchan, buf, &offsets);
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (offsets.end_switch_old)
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, &tsc);
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (mode == FORCE_ACTIVE)
+		if (offsets.begin_switch)
+			ltt_reserve_switch_new_subbuf(ltt_channel,
+				ltt_buf, rchan, buf, &offsets, &tsc);
+}
+
+/*
+ * Atomic unordered slot commit. Increments the commit count in the
+ * specified sub-buffer, and delivers it if necessary.
+ *
+ * Parameters:
+ *
+ * @ltt_channel : channel structure
+ * @transport_data: transport-specific data
+ * @reserved : address following the event header.
+ * @slot_size : size of the reserved slot.
+ */
+static void ltt_relay_commit_slot(struct ltt_channel_struct *ltt_channel,
+		void **transport_data, void *reserved, size_t slot_size)
+{
+	struct rchan_buf *buf = *transport_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	unsigned int offset_end = reserved - buf->start;
+	long commit_count;
+
+	/* Must write slot data before incrementing commit count */
+	smp_wmb();
+	commit_count = local_add_return(slot_size,
+		&ltt_buf->commit_count[SUBBUF_INDEX(offset_end-1, buf->chan)]);
+	/* Check if all commits have been done */
+	if (SUBBUF_OFFSET(commit_count, buf->chan) == 0)
+		ltt_deliver(buf, SUBBUF_INDEX(offset_end-1, buf->chan), NULL);
+	/*
+	 * Update lost_size for each commit. It's needed only for extracting
+	 * ltt buffers from vmcore, after crash.
+	 */
+	ltt_write_commit_counter(buf, reserved, slot_size);
+}
+
+/*
+ * This is called with preemption disabled when user space has requested
+ * blocking mode.  If one of the active traces has free space below a
+ * specific threshold value, we reenable preemption and block.
+ */
+static int ltt_relay_user_blocking(struct ltt_trace_struct *trace,
+		unsigned int index, size_t data_size, struct user_dbg_data *dbg)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+	int cpu;
+	DECLARE_WAITQUEUE(wait, current);
+
+	channel = ltt_get_channel_from_index(trace, index);
+	rchan = channel->trans_channel_data;
+	cpu = smp_processor_id();
+	relay_buf = rchan->buf[cpu];
+	ltt_buf = &channel->buf[cpu];
+	/*
+	 * Check if data is too big for the channel : do not
+	 * block for it.
+	 */
+	if (LTT_RESERVE_CRITICAL + data_size > relay_buf->chan->subbuf_size)
+		return 0;
+
+	/*
+	 * If free space too low, we block. We restart from the
+	 * beginning after we resume (cpu id may have changed
+	 * while preemption is active).
+	 */
+	spin_lock(&ltt_buf->full_lock);
+	if (!channel->overwrite &&
+			(dbg->avail_size = (dbg->write = local_read(
+				&channel->buf[relay_buf->cpu].offset))
+			+ LTT_RESERVE_CRITICAL + data_size
+			 - SUBBUF_TRUNC((dbg->read = atomic_long_read(
+			&channel->buf[relay_buf->cpu].consumed)),
+			relay_buf->chan)) >= rchan->alloc_size) {
+		__set_current_state(TASK_INTERRUPTIBLE);
+		add_wait_queue(&ltt_buf->write_wait, &wait);
+		spin_unlock(&ltt_buf->full_lock);
+		preempt_enable();
+		schedule();
+		__set_current_state(TASK_RUNNING);
+		remove_wait_queue(&ltt_buf->write_wait, &wait);
+		if (signal_pending(current))
+			return -ERESTARTSYS;
+		preempt_disable();
+		return 1;
+	}
+	spin_unlock(&ltt_buf->full_lock);
+	return 0;
+}
+
+static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
+		unsigned int index, size_t data_size, struct user_dbg_data *dbg,
+		int cpu)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+
+	channel = ltt_get_channel_from_index(trace, index);
+	rchan = channel->trans_channel_data;
+	relay_buf = rchan->buf[cpu];
+	ltt_buf = &channel->buf[cpu];
+	printk(KERN_ERR "Error in LTT usertrace : "
+	"buffer full : event lost in blocking "
+	"mode. Increase LTT_RESERVE_CRITICAL.\n");
+	printk(KERN_ERR "LTT nesting level is %u.\n",
+		ltt_nesting[cpu]);
+	printk(KERN_ERR "LTT avail size %lu.\n",
+		dbg->avail_size);
+	printk(KERN_ERR "avai write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+	printk(KERN_ERR "LTT cur size %lu.\n",
+		(dbg->write = local_read(
+		&channel->buf[relay_buf->cpu].offset))
+	+ LTT_RESERVE_CRITICAL + data_size
+	 - SUBBUF_TRUNC((dbg->read = atomic_long_read(
+	&channel->buf[relay_buf->cpu].consumed)),
+				relay_buf->chan));
+	printk(KERN_ERR "cur write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+}
+
+static struct ltt_transport ltt_relay_transport = {
+	.name = "relay",
+	.owner = THIS_MODULE,
+	.ops = {
+		.create_dirs = ltt_relay_create_dirs,
+		.remove_dirs = ltt_relay_remove_dirs,
+		.create_channel = ltt_relay_create_channel,
+		.finish_channel = ltt_relay_finish_channel,
+		.remove_channel = ltt_relay_remove_channel,
+		.wakeup_channel = ltt_relay_async_wakeup_chan,
+		.commit_slot = ltt_relay_commit_slot,
+		.reserve_slot = ltt_relay_reserve_slot,
+		.user_blocking = ltt_relay_user_blocking,
+		.user_errors = ltt_relay_print_user_errors,
+	},
+};
+
+static int __init ltt_relay_init(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay init\n");
+	ltt_root_dentry = debugfs_create_dir(LTT_RELAY_ROOT, NULL);
+	if (ltt_root_dentry == NULL)
+		return -EEXIST;
+
+	ltt_file_operations = relay_file_operations;
+	ltt_file_operations.owner = THIS_MODULE;
+	ltt_file_operations.poll = ltt_poll;
+	ltt_file_operations.ioctl = ltt_ioctl;
+#ifdef CONFIG_COMPAT
+	ltt_file_operations.compat_ioctl = ltt_compat_ioctl;
+#endif
+
+	ltt_transport_register(&ltt_relay_transport);
+
+	return 0;
+}
+
+static void __exit ltt_relay_exit(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay exit\n");
+
+	ltt_transport_unregister(&ltt_relay_transport);
+
+	debugfs_remove(ltt_root_dentry);
+}
+
+module_init(ltt_relay_init);
+module_exit(ltt_relay_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Tracer");
diff -uprN linux-2.6.23.1.orig/ltt/ltt-serialize.c linux-2.6.23.1/ltt/ltt-serialize.c
--- linux-2.6.23.1.orig/ltt/ltt-serialize.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-serialize.c	2007-11-28 10:13:52.374346297 +0100
@@ -0,0 +1,723 @@
+/*
+ * LTTng serializing code.
+ *
+ * Copyright Mathieu Desnoyers, March 2007.
+ *
+ * Licensed under the GPLv2.
+ *
+ * See this discussion about weirdness about passing va_list and then va_list to
+ * functions. (related to array argument passing). va_list seems to be
+ * implemented as an array on x86_64, but not on i386... This is why we pass a
+ * va_list * to ltt_vtrace.
+ */
+
+#include <stdarg.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+
+enum ltt_type {
+	LTT_TYPE_SIGNED_INT,
+	LTT_TYPE_UNSIGNED_INT,
+	LTT_TYPE_STRING,
+	LTT_TYPE_COMPACT,
+	LTT_TYPE_NONE,
+};
+
+#define LTT_ATTRIBUTE_COMPACT (1<<0)
+#define LTT_ATTRIBUTE_NETWORK_BYTE_ORDER (1<<1)
+
+/*
+ * Inspired from vsnprintf
+ *
+ * The serialization format string supports the basic printf format strings.
+ * In addition, it defines new formats that can be used to serialize more
+ * complex/non portable data structures.
+ *
+ * Typical use:
+ *
+ * field_name %ctype
+ * field_name #tracetype %ctype
+ * field_name #tracetype %ctype1 %ctype2 ...
+ *
+ * A conversion is performed between format string types supported by GCC and
+ * the trace type requested. GCC type is used to perform type checking on format
+ * strings. Trace type is used to specify the exact binary representation
+ * in the trace. A mapping is done between one or more GCC types to one trace
+ * type. Sign extension, if required by the conversion, is performed following
+ * the trace type.
+ *
+ * If a gcc format is not declared with a trace format, the gcc format is
+ * also used as binary representation in the trace.
+ *
+ * Strings are supported with %s.
+ * A single tracetype (sequence) can take multiple c types as parameter.
+ *
+ * c types:
+ *
+ * see printf(3).
+ *
+ * Note: to write a uint32_t in a trace, the following expression is recommended
+ * si it can be portable:
+ *
+ * ("#4u%lu", (unsigned long)var)
+ *
+ * trace types:
+ *
+ * Serialization specific formats :
+ *
+ * Fixed size integers
+ * #1u     writes uint8_t
+ * #2u     writes uint16_t
+ * #4u     writes uint32_t
+ * #8u     writes uint64_t
+ * #1d     writes int8_t
+ * #2d     writes int16_t
+ * #4d     writes int32_t
+ * #8d     writes int64_t
+ * i.e.:
+ * #1u%lu #2u%lu #4d%lu #8d%lu #llu%hu #d%lu
+ *
+ * * Attributes:
+ *
+ * b: (for bitfield)
+ * #btracetype%ctype
+ *            will be packed, truncated, in the compact header if event is saved
+ *            in compact channel and is first arg. Else the attribute has no
+ *            effect. No sign extension is done when packing compact data.
+ * n:  (for network byte order)
+ * #ntracetype%ctype
+ *            is written in the trace in network byte order.
+ *
+ * i.e.: #cn4u%lu, #n%lu, #c%u
+ *
+ * TODO (eventually)
+ * Variable length sequence
+ * #a #tracetype1 #tracetype2 %array_ptr %elem_size %num_elems
+ *            In the trace:
+ *            #a specifies that this is a sequence
+ *            #tracetype1 is the type of elements in the sequence
+ *            #tracetype2 is the type of the element count
+ *            GCC input:
+ *            array_ptr is a pointer to an array that contains members of size
+ *            elem_size.
+ *            num_elems is the number of elements in the array.
+ * i.e.: #a #lu #lu %p %lu %u
+ *
+ * Callback
+ * #k         callback (taken from the probe data)
+ *            The following % arguments are exepected by the callback
+ *
+ * i.e.: #a #lu #lu #k %p
+ *
+ * Note: No conversion is done from floats to integers, nor from integers to
+ * floats between c types and trace types. float conversion from double to float
+ * or from float to double is also not supported.
+ *
+ * REMOVE
+ * %*b     expects sizeof(data), data
+ *         where sizeof(data) is 1, 2, 4 or 8
+ *
+ * Fixed length struct, union or array.
+ * FIXME: unable to extract those sizes statically.
+ * %*r     expects sizeof(*ptr), ptr
+ * %*.*r   expects sizeof(*ptr), __alignof__(*ptr), ptr
+ * struct and unions removed.
+ * Fixed length array:
+ * [%p]#a[len #tracetype]
+ * i.e.: [%p]#a[12 #lu]
+ *
+ * Variable length sequence
+ * %*.*:*v expects sizeof(*ptr), __alignof__(*ptr), elem_num, ptr
+ *         where elem_num is the number of elements in the sequence
+ */
+
+__attribute__((no_instrument_function))
+static inline const char *parse_trace_type(const char *fmt,
+		char *trace_size, enum ltt_type *trace_type,
+		unsigned long *attributes)
+{
+	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
+				/* 'z' support added 23/7/1999 S.H.    */
+				/* 'z' changed to 'Z' --davidm 1/25/99 */
+				/* 't' added for ptrdiff_t */
+
+	/* parse attributes. */
+repeat:
+	switch (*fmt) {
+	case 'b':
+		*attributes |= LTT_ATTRIBUTE_COMPACT;
+		++fmt;
+		goto repeat;
+	case 'n':
+		*attributes |= LTT_ATTRIBUTE_NETWORK_BYTE_ORDER;
+		++fmt;
+		goto repeat;
+	}
+
+	/* get the conversion qualifier */
+	qualifier = -1;
+	if (*fmt == 'h' || *fmt == 'l' || *fmt == 'L' ||
+	    *fmt == 'Z' || *fmt == 'z' || *fmt == 't' ||
+	    *fmt == 'S' || *fmt == '1' || *fmt == '2' ||
+	    *fmt == '4' || *fmt == 8) {
+		qualifier = *fmt;
+		++fmt;
+		if (qualifier == 'l' && *fmt == 'l') {
+			qualifier = 'L';
+			++fmt;
+		}
+	}
+
+	switch (*fmt) {
+	case 'c':
+		*trace_type = LTT_TYPE_UNSIGNED_INT;
+		*trace_size = sizeof(unsigned char);
+		goto parse_end;
+	case 's':
+		*trace_type = LTT_TYPE_STRING;
+		goto parse_end;
+	case 'p':
+		*trace_type = LTT_TYPE_UNSIGNED_INT;
+		*trace_size = sizeof(void *);
+		goto parse_end;
+	case 'd':
+	case 'i':
+		*trace_type = LTT_TYPE_SIGNED_INT;
+		break;
+	case 'o':
+	case 'u':
+	case 'x':
+	case 'X':
+		*trace_type = LTT_TYPE_UNSIGNED_INT;
+		break;
+	default:
+		if (!*fmt)
+			--fmt;
+		goto parse_end;
+	}
+	switch (qualifier) {
+	case 'L':
+		*trace_size = sizeof(long long);
+		break;
+	case 'l':
+		*trace_size = sizeof(long);
+		break;
+	case 'Z':
+	case 'z':
+		*trace_size = sizeof(size_t);
+		break;
+	case 't':
+		*trace_size = sizeof(ptrdiff_t);
+		break;
+	case 'h':
+		*trace_size = sizeof(short);
+		break;
+	case '1':
+		*trace_size = sizeof(uint8_t);
+		break;
+	case '2':
+		*trace_size = sizeof(uint16_t);
+		break;
+	case '4':
+		*trace_size = sizeof(uint32_t);
+		break;
+	case '8':
+		*trace_size = sizeof(uint64_t);
+		break;
+	default:
+		*trace_size = sizeof(int);
+	}
+
+parse_end:
+	return fmt;
+}
+
+/*
+ * Restrictions:
+ * Field width and precision are *not* supported.
+ * %n not supported.
+ */
+__attribute__((no_instrument_function))
+static inline const char *parse_c_type(const char *fmt,
+		char *c_size, enum ltt_type *c_type)
+{
+	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
+				/* 'z' support added 23/7/1999 S.H.    */
+				/* 'z' changed to 'Z' --davidm 1/25/99 */
+				/* 't' added for ptrdiff_t */
+
+	/* process flags : ignore standard print formats for now. */
+repeat:
+	switch (*fmt) {
+	case '-':
+	case '+':
+	case ' ':
+	case '#':
+	case '0':
+		++fmt;
+		goto repeat;
+	}
+
+	/* get the conversion qualifier */
+	qualifier = -1;
+	if (*fmt == 'h' || *fmt == 'l' || *fmt == 'L' ||
+	    *fmt == 'Z' || *fmt == 'z' || *fmt == 't' ||
+	    *fmt == 'S') {
+		qualifier = *fmt;
+		++fmt;
+		if (qualifier == 'l' && *fmt == 'l') {
+			qualifier = 'L';
+			++fmt;
+		}
+	}
+
+	switch (*fmt) {
+	case 'c':
+		*c_type = LTT_TYPE_UNSIGNED_INT;
+		*c_size = sizeof(unsigned char);
+		goto parse_end;
+	case 's':
+		*c_type = LTT_TYPE_STRING;
+		goto parse_end;
+	case 'p':
+		*c_type = LTT_TYPE_UNSIGNED_INT;
+		*c_size = sizeof(void *);
+		goto parse_end;
+	case 'd':
+	case 'i':
+		*c_type = LTT_TYPE_SIGNED_INT;
+		break;
+	case 'o':
+	case 'u':
+	case 'x':
+	case 'X':
+		*c_type = LTT_TYPE_UNSIGNED_INT;
+		break;
+	default:
+		if (!*fmt)
+			--fmt;
+		goto parse_end;
+	}
+	switch (qualifier) {
+	case 'L':
+		*c_size = sizeof(long long);
+		break;
+	case 'l':
+		*c_size = sizeof(long);
+		break;
+	case 'Z':
+	case 'z':
+		*c_size = sizeof(size_t);
+		break;
+	case 't':
+		*c_size = sizeof(ptrdiff_t);
+		break;
+	case 'h':
+		*c_size = sizeof(short);
+		break;
+	default:
+		*c_size = sizeof(int);
+	}
+
+parse_end:
+	return fmt;
+}
+
+
+
+__attribute__((no_instrument_function))
+static inline char *serialize_trace_data(char *buffer, char *str,
+		char trace_size, enum ltt_type trace_type,
+		char c_size, enum ltt_type c_type, int align, va_list *args)
+{
+	union {
+		unsigned long v_ulong;
+		uint64_t v_uint64;
+		struct {
+			const char *s;
+			size_t len;
+		} v_string;
+	} tmp;
+
+	/*
+	 * Be careful about sign extension here.
+	 * Sign extension is done with the destination (trace) type.
+	 */
+	switch (trace_type) {
+	case LTT_TYPE_SIGNED_INT:
+		switch (c_size) {
+		case 1:	tmp.v_ulong = (long)(int8_t)va_arg(*args, int);
+			break;
+		case 2:	tmp.v_ulong = (long)(int16_t)va_arg(*args, int);
+			break;
+		case 4:	tmp.v_ulong = (long)(int32_t)va_arg(*args, int);
+			break;
+		case 8:	tmp.v_uint64 = va_arg(*args, int64_t);
+			break;
+		default:
+			BUG();
+		}
+		break;
+	case LTT_TYPE_UNSIGNED_INT:
+		switch (c_size) {
+		case 1:	tmp.v_ulong = (unsigned long)(uint8_t)
+					va_arg(*args, unsigned int);
+			break;
+		case 2:	tmp.v_ulong = (unsigned long)(uint16_t)
+					va_arg(*args, unsigned int);
+			break;
+		case 4:	tmp.v_ulong = (unsigned long)(uint32_t)
+					va_arg(*args, unsigned int);
+			break;
+		case 8:	tmp.v_uint64 = va_arg(*args, uint64_t);
+			break;
+		default:
+			BUG();
+		}
+		break;
+	case LTT_TYPE_STRING:
+		tmp.v_string.s = va_arg(*args, const char *);
+		if ((unsigned long)tmp.v_string.s < PAGE_SIZE)
+			tmp.v_string.s = "<NULL>";
+		tmp.v_string.len = strlen(tmp.v_string.s)+1;
+		if (buffer)
+			memcpy(str, tmp.v_string.s, tmp.v_string.len);
+		str += tmp.v_string.len;
+		goto copydone;
+	default:
+		BUG();
+	}
+
+	/*
+	 * If trace_size is lower or equal to 4 bytes, there is no sign
+	 * extension to do because we are already encoded in a long. Therefore,
+	 * we can combine signed and unsigned ops. 4 bytes float also works
+	 * with this, because we do a simple copy of 4 bytes into 4 bytes
+	 * without manipulation (and we do not support conversion from integers
+	 * to floats).
+	 * It is also the case if c_size is 8 bytes, which is the largest
+	 * possible integer.
+	 */
+	if (align)
+		str += ltt_align((long)str, trace_size);
+	if (trace_size <= 4 || c_size == 8) {
+		if (buffer) {
+			switch (trace_size) {
+			case 1:	if (c_size == 8)
+					*(uint8_t *)str = (uint8_t)tmp.v_uint64;
+				else
+					*(uint8_t *)str = (uint8_t)tmp.v_ulong;
+				break;
+			case 2:	if (c_size == 8)
+					*(uint16_t *)str =
+							(uint16_t)tmp.v_uint64;
+				else
+					*(uint16_t *)str =
+							(uint16_t)tmp.v_ulong;
+				break;
+			case 4:	if (c_size == 8)
+					*(uint32_t *)str =
+							(uint32_t)tmp.v_uint64;
+				else
+					*(uint32_t *)str =
+							(uint32_t)tmp.v_ulong;
+				break;
+			case 8:	/*
+				 * c_size cannot be other than 8 here because
+				 * trace_size > 4.
+				 */
+				*(uint64_t *)str = (uint64_t)tmp.v_uint64;
+				break;
+			default:
+				BUG();
+			}
+		}
+		str += trace_size;
+		goto copydone;
+	} else {
+		/*
+		 * Perform sign extension.
+		 */
+		if (buffer) {
+			switch (trace_type) {
+			case LTT_TYPE_SIGNED_INT:
+				*(int64_t *)str = (int64_t)tmp.v_ulong;
+				break;
+			case LTT_TYPE_UNSIGNED_INT:
+				*(uint64_t *)str = (uint64_t)tmp.v_ulong;
+				break;
+			default:
+				BUG();
+			}
+		}
+		str += trace_size;
+		goto copydone;
+	}
+
+copydone:
+	return str;
+}
+
+__attribute__((no_instrument_function))
+char *ltt_serialize_data(char *buffer, char *str,
+			struct ltt_serialize_closure *closure,
+			void *serialize_private,
+			int align,
+			const char *fmt, va_list *args)
+{
+	char trace_size = 0, c_size = 0;	/*
+						 * 0 (unset), 1, 2, 4, 8 bytes.
+						 */
+	enum ltt_type trace_type = LTT_TYPE_NONE, c_type = LTT_TYPE_NONE;
+	unsigned long attributes = 0;
+
+	for (; *fmt ; ++fmt) {
+		switch (*fmt) {
+		case '#':
+			/* tracetypes (#) */
+			++fmt;			/* skip first '#' */
+			if (*fmt == '#')	/* Escaped ## */
+				break;
+			attributes = 0;
+			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
+				&attributes);
+			break;
+		case '%':
+			/* c types (%) */
+			++fmt;			/* skip first '%' */
+			if (*fmt == '%')	/* Escaped %% */
+				break;
+			fmt = parse_c_type(fmt, &c_size, &c_type);
+			/*
+			 * Output c types if no trace types has been
+			 * specified.
+			 */
+			if (!trace_size)
+				trace_size = c_size;
+			if (trace_type == LTT_TYPE_NONE)
+				trace_type = c_type;
+			if (c_type == LTT_TYPE_STRING)
+				trace_type = LTT_TYPE_STRING;
+			/* perform trace write */
+			str = serialize_trace_data(buffer, str, trace_size,
+						trace_type, c_size, c_type,
+						align, args);
+			trace_size = 0;
+			c_size = 0;
+			trace_type = LTT_TYPE_NONE;
+			c_size = LTT_TYPE_NONE;
+			attributes = 0;
+			break;
+			/* default is to skip the text, doing nothing */
+		}
+	}
+	return str;
+}
+EXPORT_SYMBOL_GPL(ltt_serialize_data);
+
+/*
+ * get_one_arg
+ * @fmt: format string
+ * @args: va args list
+ * @compact_data: compact data to fill
+ *
+ * Get one argument of the format string.
+ */
+static inline int get_compact_arg(const char **orig_fmt, va_list *args,
+		uint32_t *compact_data)
+{
+	int found = 0;
+	const char *fmt = *orig_fmt;
+	char trace_size = 0, c_size = 0;	/*
+						 * 0 (unset), 1, 2, 4, 8 bytes.
+						 */
+	enum ltt_type trace_type = LTT_TYPE_NONE, c_type = LTT_TYPE_NONE;
+	unsigned long attributes = 0;
+
+	for (; *fmt ; ++fmt) {
+		switch (*fmt) {
+		case '#':
+			/* tracetypes (#) */
+			++fmt;			/* skip first '#' */
+			if (*fmt == '#')	/* Escaped ## */
+				break;
+			attributes = 0;
+			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
+				&attributes);
+			if (attributes & LTT_ATTRIBUTE_COMPACT)
+				found = 1;
+			break;
+		case '%':
+			/* c types (%) */
+			++fmt;			/* skip first '%' */
+			if (*fmt == '%')	/* Escaped %% */
+				break;
+			fmt = parse_c_type(fmt, &c_size, &c_type);
+			if (found) {
+				if (c_size <= 4)
+					*compact_data = va_arg(*args, uint32_t);
+				else
+					*compact_data = (uint32_t)
+							va_arg(*args, uint64_t);
+			}
+			trace_size = 0;
+			c_size = 0;
+			trace_type = LTT_TYPE_NONE;
+			c_size = LTT_TYPE_NONE;
+			attributes = 0;
+			goto end;	/* Stop after 1st c type */
+			/* default is to skip the text, doing nothing */
+		}
+	}
+end:
+	if (found)
+		*orig_fmt = fmt;
+	return found;
+}
+
+/*
+ * Calculate data size
+ * Assume that the padding for alignment starts at a sizeof(void *) address.
+ */
+static __attribute__((no_instrument_function))
+size_t ltt_get_data_size(struct ltt_serialize_closure *closure,
+				void *serialize_private,
+				int align,
+				const char *fmt, va_list *args)
+{
+	ltt_serialize_cb cb = closure->callbacks[0];
+	closure->cb_idx = 0;
+	return (size_t)cb(NULL, NULL, closure, serialize_private, align,
+				fmt, args);
+}
+
+static __attribute__((no_instrument_function))
+void ltt_write_event_data(char *buffer,
+				struct ltt_serialize_closure *closure,
+				void *serialize_private,
+				int align,
+				const char *fmt, va_list *args)
+{
+	ltt_serialize_cb cb = closure->callbacks[0];
+	closure->cb_idx = 0;
+	cb(buffer, buffer, closure, serialize_private, align, fmt, args);
+}
+
+
+__attribute__((no_instrument_function))
+void ltt_vtrace(const struct marker *mdata, void *call_data,
+		const char *fmt, va_list *args)
+{
+	int align;
+	struct ltt_active_marker *pdata;
+	uint16_t eID;
+	size_t data_size, slot_size;
+	int channel_index;
+	struct ltt_channel_struct *channel;
+	struct ltt_trace_struct *trace, *dest_trace = NULL;
+	void *transport_data;
+	uint64_t tsc;
+	char *buffer;
+	va_list args_copy;
+	struct ltt_serialize_closure closure;
+	struct ltt_probe_private_data *private_data = call_data;
+	u32 compact_data = 0;
+	void *serialize_private = NULL;
+	int cpu;
+
+	pdata = (struct ltt_active_marker *)mdata->private;
+	if (unlikely(private_data && private_data->id < MARKER_CORE_IDS))
+		eID = private_data->id;
+	else
+		eID = pdata->id;
+	closure.callbacks = pdata->probe->callbacks;
+
+	/*
+	 * This test is useful for quickly exiting static tracing when no
+	 * trace is active.
+	 */
+	if (likely(ltt_traces.num_active_traces == 0
+		&& (!private_data || !private_data->force)))
+		return;
+
+	preempt_disable();
+	if (likely(!private_data || !private_data->force
+			|| private_data->cpu == -1))
+		cpu = smp_processor_id();
+	else
+		cpu = private_data->cpu;
+	ltt_nesting[smp_processor_id()]++;
+
+	if (unlikely(private_data && private_data->trace))
+		dest_trace = private_data->trace;
+	if (unlikely(private_data && private_data->channel))
+		channel_index = private_data->channel;
+	else
+		channel_index = pdata->channel;
+	if (unlikely(private_data))
+		serialize_private = private_data->serialize_private;
+	/* Skip the compact data for size calculation in compact channel */
+	if (unlikely(channel_index == GET_CHANNEL_INDEX(compact))) {
+		get_compact_arg(&fmt, args, &compact_data);
+		align = 0;	/* Force no alignment for compact channel */
+	} else
+		align = ltt_get_alignment(pdata);
+	va_copy(args_copy, *args);
+	data_size = ltt_get_data_size(&closure, serialize_private, align,
+					fmt, &args_copy);
+	va_end(args_copy);
+
+	/* Iterate on each trace */
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		if (unlikely(!trace->active
+			&& (!private_data || !private_data->force)))
+			continue;
+		if (unlikely((private_data && private_data->trace)
+				&& trace != dest_trace))
+			continue;
+		if (!ltt_run_filter(trace, eID))
+			continue;
+		channel = ltt_get_channel_from_index(trace, channel_index);
+		/* reserve space : header and data */
+		buffer = ltt_reserve_slot(trace, channel, &transport_data,
+					data_size, &slot_size, &tsc, cpu);
+		if (unlikely(!buffer))
+			continue; /* buffer full */
+
+		va_copy(args_copy, *args);
+		/* Out-of-order write : header and data */
+		if (likely(channel_index != GET_CHANNEL_INDEX(compact)))
+			buffer = ltt_write_event_header(trace, channel, buffer,
+						eID, data_size, tsc);
+		else
+			buffer = ltt_write_compact_header(trace, channel,
+						buffer, eID, data_size,
+						tsc, compact_data);
+		ltt_write_event_data(buffer, &closure, serialize_private, align,
+					fmt, &args_copy);
+		va_end(args_copy);
+		/* Out-of-order commit */
+		ltt_commit_slot(channel, &transport_data, buffer, slot_size);
+	}
+	ltt_nesting[smp_processor_id()]--;
+	preempt_enable();
+}
+EXPORT_SYMBOL_GPL(ltt_vtrace);
+
+__attribute__((no_instrument_function))
+void ltt_trace(const struct marker *mdata, void *call_data,
+	const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	ltt_vtrace(mdata, call_data, fmt, &args);
+	va_end(args);
+}
+EXPORT_SYMBOL_GPL(ltt_trace);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Serializer");
diff -uprN linux-2.6.23.1.orig/ltt/ltt-statedump.c linux-2.6.23.1/ltt/ltt-statedump.c
--- linux-2.6.23.1.orig/ltt/ltt-statedump.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-statedump.c	2007-11-28 10:13:52.375346088 +0100
@@ -0,0 +1,386 @@
+/*
+ * Linux Trace Toolkit Kernel State Dump
+ *
+ * Copyright 2005 -
+ * Jean-Hugues Deschenes <jean-hugues.deschenes@polymtl.ca>
+ *
+ * Changes:
+ *	Eric Clement:                   Add listing of network IP interface
+ *	2006, 2007 Mathieu Desnoyers	Fix kernel threads
+ *	                                Various updates
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+#include <linux/netlink.h>
+#include <linux/inet.h>
+#include <linux/ip.h>
+#include <linux/kthread.h>
+#include <linux/proc_fs.h>
+#include <linux/file.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/cpu.h>
+#include <linux/netdevice.h>
+#include <linux/inetdevice.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/marker.h>
+
+#define NB_PROC_CHUNK 20
+
+static atomic_t kernel_threads_to_run;
+static struct delayed_work cpu_work[NR_CPUS];
+static struct task_struct *work_wake_task;
+
+enum lttng_thread_type {
+	LTTNG_USER_THREAD = 0,
+	LTTNG_KERNEL_THREAD = 1,
+};
+
+enum lttng_execution_mode {
+	LTTNG_USER_MODE = 0,
+	LTTNG_SYSCALL = 1,
+	LTTNG_TRAP = 2,
+	LTTNG_IRQ = 3,
+	LTTNG_SOFTIRQ = 4,
+	LTTNG_MODE_UNKNOWN = 5,
+};
+
+enum lttng_execution_submode {
+	LTTNG_NONE = 0,
+	LTTNG_UNKNOWN = 1,
+};
+
+enum lttng_process_status {
+	LTTNG_UNNAMED = 0,
+	LTTNG_WAIT_FORK = 1,
+	LTTNG_WAIT_CPU = 2,
+	LTTNG_EXIT = 3,
+	LTTNG_ZOMBIE = 4,
+	LTTNG_WAIT = 5,
+	LTTNG_RUN = 6,
+	LTTNG_DEAD = 7,
+};
+
+#ifdef CONFIG_INET
+static void ltt_enumerate_device(struct ltt_probe_private_data *call_data,
+		struct net_device *dev)
+{
+	struct in_device *in_dev;
+	struct in_ifaddr *ifa;
+
+	if (dev->flags & IFF_UP) {
+		in_dev = in_dev_get(dev);
+		if (in_dev) {
+			for (ifa = in_dev->ifa_list;
+					ifa != NULL;
+					ifa = ifa->ifa_next)
+				__trace_mark(0, list_network_ipv4_interface,
+					call_data,
+					"name %s address #4u%lu up %d",
+					dev->name,
+					(unsigned long)ifa->ifa_address, 0);
+			in_dev_put(in_dev);
+		}
+	} else
+		__trace_mark(0, list_network_ip_interface, call_data,
+			"name %s address #4u%lu up %d", dev->name, 0UL, 0);
+}
+
+static inline int
+ltt_enumerate_network_ip_interface(struct ltt_probe_private_data *call_data)
+{
+	struct net_device *dev;
+
+	read_lock(&dev_base_lock);
+	for_each_netdev(dev)
+		ltt_enumerate_device(call_data, dev);
+	read_unlock(&dev_base_lock);
+
+	return 0;
+}
+#else /* CONFIG_INET */
+static inline int
+ltt_enumerate_network_ip_interface(struct ltt_probe_private_data *call_data)
+{
+	return 0;
+}
+#endif /* CONFIG_INET */
+
+
+static inline void
+ltt_enumerate_task_fd(struct ltt_probe_private_data *call_data,
+		struct task_struct *t, char *tmp)
+{
+	struct fdtable *fdt;
+	struct file *filp;
+	unsigned int i;
+	const unsigned char *path;
+
+	if (!t->files)
+		return;
+
+	spin_lock(&t->files->file_lock);
+	fdt = files_fdtable(t->files);
+	for (i = 0; i < fdt->max_fds; i++) {
+		filp = fcheck_files(t->files, i);
+		if (!filp)
+			continue;
+		path = d_path(filp->f_dentry,
+				filp->f_vfsmnt, tmp, PAGE_SIZE);
+		/* Make sure we give at least some info */
+		__trace_mark(0, list_file_descriptor, call_data,
+			"filename %s pid %d fd %u",
+			(IS_ERR(path))?(filp->f_dentry->d_name.name):(path),
+			t->pid, i);
+	}
+	spin_unlock(&t->files->file_lock);
+}
+
+static inline int
+ltt_enumerate_file_descriptors(struct ltt_probe_private_data *call_data)
+{
+	struct task_struct *t = &init_task;
+	char *tmp = (char *)__get_free_page(GFP_KERNEL);
+
+	/* Enumerate active file descriptors */
+	do {
+		read_lock(&tasklist_lock);
+		if (t != &init_task)
+			atomic_dec(&t->usage);
+		t = next_task(t);
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+		task_lock(t);
+		ltt_enumerate_task_fd(call_data, t, tmp);
+		task_unlock(t);
+	} while (t != &init_task);
+	free_page((unsigned long)tmp);
+	return 0;
+}
+
+static inline void
+ltt_enumerate_task_vm_maps(struct ltt_probe_private_data *call_data,
+		struct task_struct *t)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *map;
+	unsigned long ino;
+
+	/* get_task_mm does a task_lock... */
+	mm = get_task_mm(t);
+	if (!mm)
+		return;
+
+	map = mm->mmap;
+	if (map) {
+		down_read(&mm->mmap_sem);
+		while (map) {
+			if (map->vm_file)
+				ino = map->vm_file->f_dentry->d_inode->i_ino;
+			else
+				ino = 0;
+			__trace_mark(0, list_vm_map, call_data,
+					"pid %d start %lu end %lu flags %lu "
+					"pgoff %lu inode %lu",
+					t->pid,
+					map->vm_start,
+					map->vm_end,
+					map->vm_flags,
+					map->vm_pgoff << PAGE_SHIFT,
+					ino);
+			map = map->vm_next;
+		}
+		up_read(&mm->mmap_sem);
+	}
+	mmput(mm);
+}
+
+static inline int
+ltt_enumerate_vm_maps(struct ltt_probe_private_data *call_data)
+{
+	struct task_struct *t = &init_task;
+
+	do {
+		read_lock(&tasklist_lock);
+		if (t != &init_task)
+			atomic_dec(&t->usage);
+		t = next_task(t);
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+		ltt_enumerate_task_vm_maps(call_data, t);
+	} while (t != &init_task);
+	return 0;
+}
+
+static inline void list_interrupts(struct ltt_probe_private_data *call_data)
+{
+	unsigned int i;
+	unsigned long flags = 0;
+
+	/* needs irq_desc */
+	for (i = 0; i < NR_IRQS; i++) {
+		struct irqaction *action;
+		const char *irq_chip_name =
+			irq_desc[i].chip->name ? : "unnamed_irq_chip";
+
+		spin_lock_irqsave(&irq_desc[i].lock, flags);
+		for (action = irq_desc[i].action;
+				action; action = action->next)
+			__trace_mark(0, list_interrupt, call_data,
+				"name %s action %s irq_id %u",
+				irq_chip_name, action->name, i);
+		spin_unlock_irqrestore(&irq_desc[i].lock, flags);
+	}
+}
+
+static inline int
+ltt_enumerate_process_states(struct ltt_probe_private_data *call_data)
+{
+	struct task_struct *t = &init_task;
+	struct task_struct *p = t;
+	enum lttng_process_status status;
+	enum lttng_thread_type type;
+	enum lttng_execution_mode mode;
+	enum lttng_execution_submode submode;
+
+	do {
+		mode = LTTNG_MODE_UNKNOWN;
+		submode = LTTNG_UNKNOWN;
+
+		read_lock(&tasklist_lock);
+		if (t != &init_task) {
+			atomic_dec(&t->usage);
+			t = next_thread(t);
+		}
+		if (t == p) {
+			p = next_task(t);
+			t = p;
+		}
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+
+		task_lock(t);
+
+		if (t->exit_state == EXIT_ZOMBIE)
+			status = LTTNG_ZOMBIE;
+		else if (t->exit_state == EXIT_DEAD)
+			status = LTTNG_DEAD;
+		else if (t->state == TASK_RUNNING) {
+			/* Is this a forked child that has not run yet? */
+			if (list_empty(&t->run_list))
+				status = LTTNG_WAIT_FORK;
+			else
+				/*
+				 * All tasks are considered as wait_cpu;
+				 * the viewer will sort out if the task was
+				 * really running at this time.
+				 */
+				status = LTTNG_WAIT_CPU;
+		} else if (t->state &
+			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)) {
+			/* Task is waiting for something to complete */
+			status = LTTNG_WAIT;
+		} else
+			status = LTTNG_UNNAMED;
+		submode = LTTNG_NONE;
+
+		/*
+		 * Verification of t->mm is to filter out kernel threads;
+		 * Viewer will further filter out if a user-space thread was
+		 * in syscall mode or not.
+		 */
+		if (t->mm)
+			type = LTTNG_USER_THREAD;
+		else
+			type = LTTNG_KERNEL_THREAD;
+
+		__trace_mark(0, list_process_state, call_data,
+				"pid %d parent_pid %d name %s type %d mode %d "
+				"submode %d status %d tgid %d",
+				t->pid, t->parent->pid, t->comm,
+				type, mode, submode, status, t->tgid);
+		task_unlock(t);
+	} while (t != &init_task);
+
+	return 0;
+}
+
+void ltt_statedump_work_func(struct work_struct *work)
+{
+	if (atomic_dec_and_test(&kernel_threads_to_run)) {
+		/* If we are the last thread, wake up do_ltt_statedump */
+		wake_up_process(work_wake_task);
+	}
+}
+
+static int do_ltt_statedump(struct ltt_probe_private_data *call_data)
+{
+	int cpu;
+
+	printk(KERN_DEBUG "LTT state dump thread start\n");
+	ltt_enumerate_process_states(call_data);
+	ltt_enumerate_file_descriptors(call_data);
+	list_modules(call_data);
+	ltt_enumerate_vm_maps(call_data);
+	list_interrupts(call_data);
+	ltt_enumerate_network_ip_interface(call_data);
+
+	/*
+	 * Fire off a work queue on each CPU. Their sole purpose in life
+	 * is to guarantee that each CPU has been in a state where is was in
+	 * syscall mode (i.e. not in a trap, an IRQ or a soft IRQ).
+	 */
+	lock_cpu_hotplug();
+	atomic_set(&kernel_threads_to_run, num_online_cpus());
+	work_wake_task = current;
+	__set_current_state(TASK_UNINTERRUPTIBLE);
+	for_each_online_cpu(cpu) {
+		INIT_DELAYED_WORK(&cpu_work[cpu], ltt_statedump_work_func);
+		schedule_delayed_work_on(cpu, &cpu_work[cpu], 0);
+	}
+	unlock_cpu_hotplug();
+	/* Wait for all threads to run */
+	schedule();
+	BUG_ON(atomic_read(&kernel_threads_to_run) != 0);
+	/* Our work is done */
+	printk(KERN_DEBUG "LTT state dump end\n");
+	__trace_mark(0, list_statedump_end, call_data, MARK_NOARGS);
+	return 0;
+}
+
+int ltt_statedump_start(struct ltt_trace_struct *trace)
+{
+	struct ltt_probe_private_data call_data;
+	printk(KERN_DEBUG "LTT state dump begin\n");
+
+	call_data.trace = trace;
+	call_data.force = 0;
+	call_data.id = MARKER_CORE_IDS;
+	call_data.channel = 0;
+	return do_ltt_statedump(&call_data);
+}
+
+static int __init statedump_init(void)
+{
+	int ret;
+	printk(KERN_DEBUG "LTT : State dump init\n");
+	ret = ltt_module_register(LTT_FUNCTION_STATEDUMP,
+			ltt_statedump_start, THIS_MODULE);
+	return ret;
+}
+
+static void __exit statedump_exit(void)
+{
+	printk(KERN_DEBUG "LTT : State dump exit\n");
+	ltt_module_unregister(LTT_FUNCTION_STATEDUMP);
+}
+
+module_init(statedump_init)
+module_exit(statedump_exit)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Jean-Hugues Deschenes");
+MODULE_DESCRIPTION("Linux Trace Toolkit Statedump");
diff -uprN linux-2.6.23.1.orig/ltt/ltt-test-tsc.c linux-2.6.23.1/ltt/ltt-test-tsc.c
--- linux-2.6.23.1.orig/ltt/ltt-test-tsc.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-test-tsc.c	2007-11-28 10:13:52.375346088 +0100
@@ -0,0 +1,123 @@
+/*
+ * Test TSC synchronization
+ */
+#include <linux/module.h>
+#include <linux/timer.h>
+#include <linux/timex.h>
+#include <linux/jiffies.h>
+#include <linux/cpu.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+
+/* Workaround for 32 bits archs which does not define this */
+#ifdef CONFIG_X86_32
+#define get_cycles_sync get_cycles
+#endif
+
+#define MAX_CYCLES_DELTA 1000ULL
+
+static DEFINE_PER_CPU(cycles_t, tsc_count);
+static DEFINE_MUTEX(tscsync_mutex);
+
+static DEFINE_PER_CPU(int, wait_sync);
+static DEFINE_PER_CPU(int, wait_end_sync);
+
+int ltt_tsc_is_sync = 1;
+EXPORT_SYMBOL(ltt_tsc_is_sync);
+
+cycles_t ltt_last_tsc;
+EXPORT_SYMBOL(ltt_last_tsc);
+
+/*
+ * Mark it noinline so we make sure it is not unrolled.
+ * Wait until value is reached.
+ */
+static noinline void tsc_barrier(long wait_cpu, int value)
+{
+	sync_core();
+	per_cpu(wait_sync, smp_processor_id())--;
+	do {
+		barrier();
+	} while (unlikely(per_cpu(wait_sync, wait_cpu) > value));
+	__get_cpu_var(tsc_count) = get_cycles_sync();
+}
+
+/*
+ * Worker thread called on each CPU.
+ * First wait with interrupts enabled, then wait with interrupt disabled,
+ * for precision. We are already bound to one CPU.
+ */
+static void test_sync(void *arg)
+{
+	long wait_cpu = (long)arg;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	/* Make sure the instructions are in I-CACHE */
+	tsc_barrier(wait_cpu, 1);
+	tsc_barrier(wait_cpu, 0);
+	per_cpu(wait_end_sync, smp_processor_id())--;
+	do {
+		barrier();
+	} while (unlikely(per_cpu(wait_end_sync, wait_cpu) > 0));
+	local_irq_restore(flags);
+}
+
+/*
+ * Do loops (making sure no unexpected event changes the timing), keep the
+ * best one. The result of each loop is the highest tsc delta between the
+ * master CPU and the slaves.
+ */
+static int test_tsc_synchronization(void)
+{
+	long cpu, master;
+	cycles_t max_diff = 0, diff, best_loop, worse_loop = 0;
+	int i;
+
+	mutex_lock(&tscsync_mutex);
+	preempt_disable();
+	master = smp_processor_id();
+	for_each_online_cpu(cpu) {
+		if (master == cpu)
+			continue;
+		best_loop = ULLONG_MAX;
+		for (i = 0; i < 10; i++) {
+			/*
+			 * Each CPU (master and slave) must decrement the
+			 * wait_sync value twice (one for priming in cache).
+			 */
+			per_cpu(wait_sync, master) = 2;
+			per_cpu(wait_sync, cpu) = 2;
+			per_cpu(wait_end_sync, master) = 1;
+			per_cpu(wait_end_sync, cpu) = 1;
+			smp_call_function_single(cpu, test_sync,
+						(void *)master, 1, 0);
+			test_sync((void *)cpu);
+			diff = abs(per_cpu(tsc_count, cpu)
+				- per_cpu(tsc_count, master));
+			best_loop = min(best_loop, diff);
+			worse_loop = max(worse_loop, diff);
+		}
+		max_diff = max(best_loop, max_diff);
+	}
+	preempt_enable();
+	if (max_diff >= MAX_CYCLES_DELTA) {
+		printk(KERN_WARNING
+			"LTTng : Your timestamp counter is not reliable.\n"
+			"See LTTng documentation to find the "
+			"appropriate solution for your architecture.\n");
+		printk("TSC unsynchronized : %llu cycles delta is over "
+			"threshold %llu\n", max_diff, MAX_CYCLES_DELTA);
+	}
+	mutex_unlock(&tscsync_mutex);
+	return max_diff < MAX_CYCLES_DELTA;
+}
+EXPORT_SYMBOL_GPL(test_tsc_synchronization);
+
+static int __init tsc_test_init(void)
+{
+	ltt_tsc_is_sync = test_tsc_synchronization();
+	return 0;
+}
+
+__initcall(tsc_test_init);
diff -uprN linux-2.6.23.1.orig/ltt/ltt-timestamp.c linux-2.6.23.1/ltt/ltt-timestamp.c
--- linux-2.6.23.1.orig/ltt/ltt-timestamp.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-timestamp.c	2007-11-28 10:13:52.376345880 +0100
@@ -0,0 +1,198 @@
+/*
+ * (C) Copyright	2006,2007 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * notes : ltt-timestamp timer-based clock cannot be used for early tracing in
+ * the boot process, as it depends on timer interrupts.
+ *
+ * The timer needs to be only on one CPU to support hotplug.
+ * We have the choice between schedule_delayed_work_on and an IPI to get each
+ * CPU to write the heartbeat. IPI has been chosen because it is considered
+ * faster than passing through the timer to get the work scheduled on all the
+ * CPUs.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/cpu.h>
+#include <linux/timex.h>
+#include <linux/bitops.h>
+#include <linux/ltt.h>
+#include <linux/smp.h>
+#include <linux/sched.h> /* FIX for m68k local_irq_enable in on_each_cpu */
+
+atomic_t lttng_generic_clock;
+EXPORT_SYMBOL(lttng_generic_clock);
+
+/* Expected maximum interrupt latency in ms : 15ms, *2 for security */
+#define EXPECTED_INTERRUPT_LATENCY	30
+
+static struct timer_list stsc_timer;
+static unsigned int precalc_expire;
+
+/* For architectures with 32 bits TSC */
+static struct synthetic_tsc_struct {
+	u32 tsc[2][2];	/* a pair of 2 32 bits. [0] is the MSB, [1] is LSB */
+	unsigned int index;	/* Index of the current synth. tsc. */
+} ____cacheline_aligned synthetic_tsc[NR_CPUS];
+
+/* Called from IPI : either in interrupt or process context */
+static void ltt_update_synthetic_tsc(void)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	u32 tsc;
+
+	preempt_disable();
+	cpu_synth = &synthetic_tsc[smp_processor_id()];
+	tsc = ltt_get_timestamp32();		/* We deal with a 32 LSB TSC */
+
+	if (tsc < cpu_synth->tsc[cpu_synth->index][1]) {
+		unsigned int new_index = cpu_synth->index ? 0 : 1; /* 0 <-> 1 */
+		/*
+		 * Overflow
+		 * Non atomic update of the non current synthetic TSC, followed
+		 * by an atomic index change. There is no write concurrency,
+		 * so the index read/write does not need to be atomic.
+		 */
+		cpu_synth->tsc[new_index][1] = tsc; /* LSB update */
+		cpu_synth->tsc[new_index][0] =
+			cpu_synth->tsc[cpu_synth->index][0]+1; /* MSB update */
+		cpu_synth->index = new_index;	/* atomic change of index */
+	} else {
+		/*
+		 * No overflow : we can simply update the 32 LSB of the current
+		 * synthetic TSC as it's an atomic write.
+		 */
+		cpu_synth->tsc[cpu_synth->index][1] = tsc;
+	}
+	preempt_enable();
+}
+
+/* Called from buffer switch : in _any_ context (even NMI) */
+u64 ltt_read_synthetic_tsc(void)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	u64 ret;
+	unsigned int index;
+	u32 tsc;
+
+	preempt_disable();
+	cpu_synth = &synthetic_tsc[smp_processor_id()];
+	index = cpu_synth->index; /* atomic read */
+	tsc = ltt_get_timestamp32();		/* We deal with a 32 LSB TSC */
+
+	if (tsc < cpu_synth->tsc[index][1]) {
+		/* Overflow */
+		ret = ((u64)(cpu_synth->tsc[index][0]+1) << 32) | ((u64)tsc);
+	} else {
+		/* no overflow */
+		ret = ((u64)cpu_synth->tsc[index][0] << 32) | ((u64)tsc);
+	}
+	preempt_enable();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_read_synthetic_tsc);
+
+static void synthetic_tsc_ipi(void *info)
+{
+	ltt_update_synthetic_tsc();
+}
+
+/* We need to be in process context to do an IPI */
+static void synthetic_tsc_work(struct work_struct *work)
+{
+	on_each_cpu(synthetic_tsc_ipi, NULL, 1, 1);
+}
+static DECLARE_WORK(stsc_work, synthetic_tsc_work);
+
+/*
+ * stsc_timer : - Timer function synchronizing synthetic TSC.
+ * @data: unused
+ *
+ * Guarantees at least 1 execution before low word of TSC wraps.
+ */
+static void stsc_timer_fct(unsigned long data)
+{
+	PREPARE_WORK(&stsc_work, synthetic_tsc_work);
+	schedule_work(&stsc_work);
+
+	mod_timer(&stsc_timer, jiffies + precalc_expire);
+}
+
+/*
+ * precalc_stsc_interval: - Precalculates the interval between the 32 bits TSC
+ * wraparounds.
+ */
+static int __init precalc_stsc_interval(void)
+{
+	unsigned long mask;
+
+	mask = 0xFFFFFFFFUL;
+	precalc_expire =
+		(mask/((ltt_frequency() / HZ * ltt_freq_scale()) << 1)
+			- 1 - (EXPECTED_INTERRUPT_LATENCY*HZ/1000)) >> 1;
+	WARN_ON(precalc_expire == 0);
+	printk(KERN_DEBUG "Synthetic TSC timer will fire each %u jiffies.\n",
+		precalc_expire);
+	return 0;
+}
+
+/*
+ * 	hotcpu_callback - CPU hotplug callback
+ * 	@nb: notifier block
+ * 	@action: hotplug action to take
+ * 	@hcpu: CPU number
+ *
+ *	Sets the new CPU's current synthetic TSC to the same value as the
+ *	currently running CPU.
+ *
+ * 	Returns the success/failure of the operation. (NOTIFY_OK, NOTIFY_BAD)
+ */
+static int __cpuinit hotcpu_callback(struct notifier_block *nb,
+				unsigned long action,
+				void *hcpu)
+{
+	unsigned int hotcpu = (unsigned long)hcpu;
+	struct synthetic_tsc_struct *cpu_synth;
+	u64 local_count;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+		cpu_synth = &synthetic_tsc[hotcpu];
+		local_count = ltt_read_synthetic_tsc();
+		cpu_synth->tsc[0][1] = (u32)local_count; /* LSB */
+		cpu_synth->tsc[0][0] = (u32)(local_count >> 32); /* MSB */
+		cpu_synth->index = 0;
+		smp_wmb();	/* Writing in data of CPU about to come up */
+		break;
+	case CPU_ONLINE:
+		/*
+		 * FIXME : heartbeat events are currently broken with CPU
+		 * hotplug : events can be recorded before heartbeat, heartbeat
+		 * too far from trace start and are broken with trace stop/start
+		 * as well.
+		 */
+		/* As we are preemptible, make sure it runs on the right cpu */
+		smp_call_function_single(hotcpu, synthetic_tsc_ipi, NULL, 1, 0);
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+/* Called from one CPU, before any tracing starts, to init each structure */
+static int __init ltt_init_synthetic_tsc(void)
+{
+	int cpu;
+	hotcpu_notifier(hotcpu_callback, 3);
+	precalc_stsc_interval();
+	init_timer(&stsc_timer);
+	stsc_timer.function = stsc_timer_fct;
+	stsc_timer.expires = jiffies + precalc_expire;
+	add_timer(&stsc_timer);
+	return 0;
+}
+
+__initcall(ltt_init_synthetic_tsc);
diff -uprN linux-2.6.23.1.orig/ltt/ltt-tracer.c linux-2.6.23.1/ltt/ltt-tracer.c
--- linux-2.6.23.1.orig/ltt/ltt-tracer.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/ltt-tracer.c	2007-11-28 10:13:52.377345671 +0100
@@ -0,0 +1,861 @@
+/*
+ * (C) Copyright	2005-2006 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Contains the kernel code for the Linux Trace Toolkit.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *	Karim Yaghmour (karim@opersys.com)
+ *	Tom Zanussi (zanussi@us.ibm.com)
+ *	Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  22/09/06 Move to the marker/probes mechanism. (Mathieu Desnoyers)
+ *  19/10/05, Complete lockless mechanism. (Mathieu Desnoyers)
+ *	27/05/05, Modular redesign and rewrite. (Mathieu Desnoyers)
+
+ * Comments :
+ * num_active_traces protects the functors. Changing the pointer is an atomic
+ * operation, but the functions can only be called when in tracing. It is then
+ * safe to unload a module in which sits a functor when no tracing is active.
+ *
+ * filter_control functor is protected by incrementing its module refcount.
+ *
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/cpu.h>
+#include <linux/kref.h>
+#include <asm/atomic.h>
+
+static struct timer_list ltt_async_wakeup_timer;
+
+/* Default callbacks for modules */
+int ltt_filter_control_default
+	(enum ltt_filter_control_msg msg, struct ltt_trace_struct *trace)
+{
+	return 0;
+}
+
+int ltt_statedump_default(struct ltt_trace_struct *trace)
+{
+	return 0;
+}
+
+
+
+/* Callbacks for registered modules */
+
+int (*ltt_filter_control_functor)
+	(enum ltt_filter_control_msg msg, struct ltt_trace_struct *trace) =
+					ltt_filter_control_default;
+struct module *ltt_filter_control_owner;
+
+/* These function pointers are protected by a trace activation check */
+struct module *ltt_run_filter_owner;
+int (*ltt_statedump_functor)(struct ltt_trace_struct *trace) =
+					ltt_statedump_default;
+struct module *ltt_statedump_owner;
+
+/* Module registration methods */
+
+int ltt_module_register(enum ltt_module_function name, void *function,
+		struct module *owner)
+{
+	int ret = 0;
+
+	switch (name) {
+	case LTT_FUNCTION_RUN_FILTER:
+		if (ltt_run_filter_owner != NULL) {
+			ret = -EEXIST;
+			goto end;
+		}
+		ltt_filter_register((ltt_run_filter_functor)function);
+		ltt_run_filter_owner = owner;
+		break;
+	case LTT_FUNCTION_FILTER_CONTROL:
+		if (ltt_filter_control_owner != NULL) {
+			ret = -EEXIST;
+			goto end;
+		}
+		ltt_filter_control_functor =
+			(int (*)(enum ltt_filter_control_msg,
+			struct ltt_trace_struct *))function;
+		break;
+	case LTT_FUNCTION_STATEDUMP:
+		if (ltt_statedump_owner != NULL) {
+			ret = -EEXIST;
+			goto end;
+		}
+		ltt_statedump_functor =
+			(int (*)(struct ltt_trace_struct *))function;
+		ltt_statedump_owner = owner;
+		break;
+	}
+
+end:
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_module_register);
+
+void ltt_module_unregister(enum ltt_module_function name)
+{
+	switch (name) {
+	case LTT_FUNCTION_RUN_FILTER:
+		ltt_filter_unregister();
+		ltt_run_filter_owner = NULL;
+		/* Wait for preempt sections to finish */
+		synchronize_sched();
+		break;
+	case LTT_FUNCTION_FILTER_CONTROL:
+		ltt_filter_control_functor = ltt_filter_control_default;
+		ltt_filter_control_owner = NULL;
+		break;
+	case LTT_FUNCTION_STATEDUMP:
+		ltt_statedump_functor = ltt_statedump_default;
+		ltt_statedump_owner = NULL;
+		break;
+	}
+
+}
+EXPORT_SYMBOL_GPL(ltt_module_unregister);
+
+static LIST_HEAD(ltt_transport_list);
+
+void ltt_transport_register(struct ltt_transport *transport)
+{
+	ltt_lock_traces();
+	list_add_tail(&transport->node, &ltt_transport_list);
+	ltt_unlock_traces();
+}
+EXPORT_SYMBOL_GPL(ltt_transport_register);
+
+void ltt_transport_unregister(struct ltt_transport *transport)
+{
+	ltt_lock_traces();
+	list_del(&transport->node);
+	ltt_unlock_traces();
+}
+EXPORT_SYMBOL_GPL(ltt_transport_unregister);
+
+static inline int is_channel_overwrite(enum ltt_channels chan,
+	enum trace_mode mode)
+{
+	switch (mode) {
+	case LTT_TRACE_NORMAL:
+		return 0;
+	case LTT_TRACE_FLIGHT:
+		switch (chan) {
+		case LTT_CHANNEL_FACILITIES:
+			return 0;
+		default:
+			return 1;
+		}
+	case LTT_TRACE_HYBRID:
+		switch (chan) {
+		case LTT_CHANNEL_CPU:
+			return 1;
+		default:
+			return 0;
+		}
+	default:
+		return 0;
+	}
+}
+
+
+void ltt_write_trace_header(struct ltt_trace_struct *trace,
+		struct ltt_trace_header *header)
+{
+	header->magic_number = LTT_TRACER_MAGIC_NUMBER;
+	header->major_version = LTT_TRACER_VERSION_MAJOR;
+	header->minor_version = LTT_TRACER_VERSION_MINOR;
+	header->float_word_order = 0;	 /* Kernel : no floating point */
+	header->arch_type = LTT_ARCH_TYPE;
+	header->arch_size = sizeof(void *);
+	header->arch_variant = LTT_ARCH_VARIANT;
+	switch (trace->mode) {
+	case LTT_TRACE_NORMAL:
+		header->flight_recorder = 0;
+		break;
+	case LTT_TRACE_FLIGHT:
+		header->flight_recorder = 1;
+		break;
+	case LTT_TRACE_HYBRID:
+		header->flight_recorder = 2;
+		break;
+	default:
+		header->flight_recorder = 0;
+	}
+
+#ifdef CONFIG_LTT_HEARTBEAT
+	header->has_heartbeat = 1;
+	header->tsc_lsb_truncate = ltt_tsc_lsb_truncate;
+	header->tscbits = ltt_tscbits;
+	header->compact_data_shift = ltt_compact_data_shift;
+#else
+	header->has_heartbeat = 0;
+	header->tsc_lsb_truncate = 0;
+	header->tscbits = 32;
+	header->compact_data_shift = 0;
+#endif
+
+	header->alignment = ltt_get_alignment(NULL);
+	header->freq_scale = trace->freq_scale;
+	header->start_freq = trace->start_freq;
+	header->start_tsc = trace->start_tsc;
+	header->start_monotonic = trace->start_monotonic;
+	header->start_time_sec = trace->start_time.tv_sec;
+	header->start_time_usec = trace->start_time.tv_usec;
+}
+EXPORT_SYMBOL_GPL(ltt_write_trace_header);
+
+static void trace_async_wakeup(struct ltt_trace_struct *trace)
+{
+	/* Must check each channel for pending read wakeup */
+	trace->ops->wakeup_channel(trace->channel.facilities);
+	trace->ops->wakeup_channel(trace->channel.interrupts);
+	trace->ops->wakeup_channel(trace->channel.processes);
+	trace->ops->wakeup_channel(trace->channel.modules);
+	trace->ops->wakeup_channel(trace->channel.network);
+	trace->ops->wakeup_channel(trace->channel.cpu);
+#ifdef CONFIG_LTT_HEARTBEAT
+	trace->ops->wakeup_channel(trace->channel.compact);
+#endif
+}
+
+/* Timer to send async wakeups to the readers */
+static void async_wakeup(unsigned long data)
+{
+	struct ltt_trace_struct *trace;
+	preempt_disable();
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		trace_async_wakeup(trace);
+	}
+	preempt_enable();
+
+	del_timer(&ltt_async_wakeup_timer);
+	ltt_async_wakeup_timer.expires = jiffies + LTT_PERCPU_TIMER_INTERVAL;
+	add_timer(&ltt_async_wakeup_timer);
+}
+
+
+
+
+void ltt_wakeup_writers(struct work_struct *work)
+{
+	struct ltt_channel_buf_struct *ltt_buf =
+		container_of(work, struct ltt_channel_buf_struct, wake_writers);
+
+	wake_up_interruptible(&ltt_buf->write_wait);
+}
+EXPORT_SYMBOL_GPL(ltt_wakeup_writers);
+
+
+/* _ltt_trace_find :
+ * find a trace by given name.
+ *
+ * Returns a pointer to the trace structure, NULL if not found. */
+static struct ltt_trace_struct *_ltt_trace_find(char *trace_name)
+{
+	int compare;
+	struct ltt_trace_struct *trace, *found = NULL;
+
+	list_for_each_entry(trace, &ltt_traces.head, list) {
+		compare = strncmp(trace->trace_name, trace_name, NAME_MAX);
+
+		if (compare == 0) {
+			found = trace;
+			break;
+		}
+	}
+
+	return found;
+}
+
+/* This function must be called with traces semaphore held. */
+static int _ltt_trace_create(char *trace_name,	enum trace_mode mode,
+				struct ltt_trace_struct *new_trace)
+{
+	int err = EPERM;
+
+	if (_ltt_trace_find(trace_name) != NULL) {
+		printk(KERN_ERR "LTT : Trace %s already exists\n", trace_name);
+		err = EEXIST;
+		goto traces_error;
+	}
+	list_add_rcu(&new_trace->list, &ltt_traces.head);
+	synchronize_sched();
+	/* Everything went fine, finish creation */
+	return 0;
+
+	/* Error handling */
+traces_error:
+	return err;
+}
+
+
+void ltt_release_transport(struct kref *kref)
+{
+	struct ltt_trace_struct *trace = container_of(kref,
+			struct ltt_trace_struct, ltt_transport_kref);
+	trace->ops->remove_dirs(trace);
+}
+EXPORT_SYMBOL_GPL(ltt_release_transport);
+
+
+void ltt_release_trace(struct kref *kref)
+{
+	struct ltt_trace_struct *trace = container_of(kref,
+			struct ltt_trace_struct, kref);
+	kfree(trace);
+}
+EXPORT_SYMBOL_GPL(ltt_release_trace);
+
+static inline void prepare_chan_size_num(unsigned *subbuf_size,
+	unsigned *n_subbufs, unsigned default_size, unsigned default_n_subbufs)
+{
+	if (*subbuf_size == 0)
+		*subbuf_size = default_size;
+	if (*n_subbufs == 0)
+		*n_subbufs = default_n_subbufs;
+	*subbuf_size = 1 << get_count_order(*subbuf_size);
+	*n_subbufs = 1 << get_count_order(*n_subbufs);
+
+	/* Subbuf size and number must both be power of two */
+	WARN_ON(hweight32(*subbuf_size) != 1);
+	WARN_ON(hweight32(*n_subbufs) != 1);
+}
+
+static int ltt_trace_create(char *trace_name, char *trace_type,
+		enum trace_mode mode,
+		unsigned subbuf_size_low, unsigned n_subbufs_low,
+		unsigned subbuf_size_med, unsigned n_subbufs_med,
+		unsigned subbuf_size_high, unsigned n_subbufs_high)
+{
+	int err = 0;
+	struct ltt_trace_struct *new_trace, *trace;
+	unsigned long flags;
+	struct ltt_transport *tran, *transport = NULL;
+
+	prepare_chan_size_num(&subbuf_size_low, &n_subbufs_low,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW, LTT_DEFAULT_N_SUBBUFS_LOW);
+
+	prepare_chan_size_num(&subbuf_size_med, &n_subbufs_med,
+		LTT_DEFAULT_SUBBUF_SIZE_MED, LTT_DEFAULT_N_SUBBUFS_MED);
+
+	prepare_chan_size_num(&subbuf_size_high, &n_subbufs_high,
+		LTT_DEFAULT_SUBBUF_SIZE_HIGH, LTT_DEFAULT_N_SUBBUFS_HIGH);
+
+	new_trace = kzalloc(sizeof(struct ltt_trace_struct), GFP_KERNEL);
+	if (!new_trace) {
+		printk(KERN_ERR
+			"LTT : Unable to allocate memory for trace %s\n",
+			trace_name);
+		err = ENOMEM;
+		goto traces_error;
+	}
+
+	kref_init(&new_trace->kref);
+	kref_init(&new_trace->ltt_transport_kref);
+	new_trace->active = 0;
+	strncpy(new_trace->trace_name, trace_name, NAME_MAX);
+	new_trace->paused = 0;
+	new_trace->mode = mode;
+	new_trace->freq_scale = ltt_freq_scale();
+
+	ltt_lock_traces();
+	list_for_each_entry(tran, &ltt_transport_list, node) {
+		if (!strcmp(tran->name, trace_type)) {
+			transport = tran;
+			break;
+		}
+	}
+
+	if (!transport) {
+		err = EINVAL;
+		printk(KERN_ERR	"LTT : Transport %s is not present.\n",
+			trace_type);
+		ltt_unlock_traces();
+		goto trace_error;
+	}
+
+	if (!try_module_get(transport->owner)) {
+		err = ENODEV;
+		printk(KERN_ERR	"LTT : Can't lock transport module.\n");
+		ltt_unlock_traces();
+		goto trace_error;
+	}
+
+	trace = _ltt_trace_find(trace_name);
+	if (trace) {
+		printk(KERN_ERR	"LTT : Trace name %s already used.\n",
+			trace_name);
+		err = EEXIST;
+		goto trace_error;
+	}
+
+	new_trace->transport = transport;
+	new_trace->ops = &transport->ops;
+
+	err = new_trace->ops->create_dirs(new_trace);
+	if (err)
+		goto dirs_error;
+
+	local_irq_save(flags);
+	new_trace->start_freq = ltt_frequency();
+	new_trace->start_tsc = ltt_get_timestamp64();
+	do_gettimeofday(&new_trace->start_time);
+	local_irq_restore(flags);
+
+	/*
+	 * Always put the facilities channel in non-overwrite mode :
+	 * This is a very low traffic channel and it can't afford to have its
+	 * data overwritten : this data (facilities info) is necessary to be
+	 * able to read the trace.
+	 *
+	 * WARNING : The heartbeat time _shouldn't_ write events in the
+	 * facilities channel as it would make the traffic too high. This is a
+	 * problematic case with flight recorder mode. FIXME
+	 */
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_FACILITIES_CHANNEL,
+			&new_trace->channel.facilities, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_FACILITIES, mode));
+	if (err != 0)
+		goto facilities_error;
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_INTERRUPTS_CHANNEL,
+			&new_trace->channel.interrupts, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_INTERRUPTS, mode));
+	if (err != 0)
+		goto interrupts_error;
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_PROCESSES_CHANNEL,
+			&new_trace->channel.processes, subbuf_size_med,
+			n_subbufs_med,
+			is_channel_overwrite(LTT_CHANNEL_PROCESSES, mode));
+	if (err != 0)
+		goto processes_error;
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_MODULES_CHANNEL,
+			&new_trace->channel.modules, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_MODULES, mode));
+	if (err != 0)
+		goto modules_error;
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_NETWORK_CHANNEL,
+			&new_trace->channel.network, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_NETWORK, mode));
+	if (err != 0)
+		goto network_error;
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.trace_root,
+			LTT_CPU_CHANNEL,
+			&new_trace->channel.cpu, subbuf_size_high,
+			n_subbufs_high,
+			is_channel_overwrite(LTT_CHANNEL_CPU, mode));
+	if (err != 0)
+		goto cpu_error;
+#ifdef CONFIG_LTT_HEARTBEAT
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.trace_root,
+			LTT_COMPACT_CHANNEL,
+			&new_trace->channel.compact, subbuf_size_high,
+			n_subbufs_high,
+			is_channel_overwrite(LTT_CHANNEL_COMPACT, mode));
+	if (err != 0)
+		goto compact_error;
+#endif
+
+	err = _ltt_trace_create(trace_name, mode, new_trace);
+
+	if (err != 0)
+		goto lock_create_error;
+	ltt_unlock_traces();
+	return err;
+
+lock_create_error:
+#ifdef CONFIG_LTT_HEARTBEAT
+	new_trace->ops->remove_channel(new_trace->channel.compact);
+compact_error:
+#endif
+	new_trace->ops->remove_channel(new_trace->channel.cpu);
+cpu_error:
+	new_trace->ops->remove_channel(new_trace->channel.network);
+network_error:
+	new_trace->ops->remove_channel(new_trace->channel.modules);
+modules_error:
+	new_trace->ops->remove_channel(new_trace->channel.processes);
+processes_error:
+	new_trace->ops->remove_channel(new_trace->channel.interrupts);
+interrupts_error:
+	new_trace->ops->remove_channel(new_trace->channel.facilities);
+facilities_error:
+	kref_put(&new_trace->ltt_transport_kref, ltt_release_transport);
+dirs_error:
+	module_put(transport->owner);
+trace_error:
+	kref_put(&new_trace->kref, ltt_release_trace);
+	ltt_unlock_traces();
+traces_error:
+	return err;
+}
+
+/* Must be called while sure that trace is in the list. */
+static int _ltt_trace_destroy(struct ltt_trace_struct	*trace)
+{
+	int err = EPERM;
+
+	if (trace == NULL) {
+		err = ENOENT;
+		goto traces_error;
+	}
+	if (trace->active) {
+		printk(KERN_ERR
+			"LTT : Can't destroy trace %s : tracer is active\n",
+			trace->trace_name);
+		err = EBUSY;
+		goto active_error;
+	}
+	/* Everything went fine */
+	list_del_rcu(&trace->list);
+	synchronize_sched();
+	return 0;
+
+	/* error handling */
+active_error:
+traces_error:
+	return err;
+}
+
+/* Sleepable part of the destroy */
+static void __ltt_trace_destroy(struct ltt_trace_struct	*trace)
+{
+	trace->ops->finish_channel(trace->channel.facilities);
+	trace->ops->finish_channel(trace->channel.interrupts);
+	trace->ops->finish_channel(trace->channel.processes);
+	trace->ops->finish_channel(trace->channel.modules);
+	trace->ops->finish_channel(trace->channel.network);
+	trace->ops->finish_channel(trace->channel.cpu);
+#ifdef CONFIG_LTT_HEARTBEAT
+	trace->ops->finish_channel(trace->channel.compact);
+#endif
+
+	flush_scheduled_work();
+
+	if (ltt_traces.num_active_traces == 0) {
+		/*
+		 * We stop the asynchronous delivery of reader wakeup, but
+		 * we must make one last check for reader wakeups pending.
+		 */
+		del_timer(&ltt_async_wakeup_timer);
+	}
+	/*
+	 * The currently destroyed trace is not in the trace list anymore,
+	 * so it's safe to call the async wakeup ourself. It will deliver
+	 * the last subbuffers.
+	 */
+	trace_async_wakeup(trace);
+
+	trace->ops->remove_channel(trace->channel.facilities);
+	trace->ops->remove_channel(trace->channel.interrupts);
+	trace->ops->remove_channel(trace->channel.processes);
+	trace->ops->remove_channel(trace->channel.modules);
+	trace->ops->remove_channel(trace->channel.network);
+	trace->ops->remove_channel(trace->channel.cpu);
+#ifdef CONFIG_LTT_HEARTBEAT
+	trace->ops->remove_channel(trace->channel.compact);
+#endif
+
+	kref_put(&trace->ltt_transport_kref, ltt_release_transport);
+
+	module_put(trace->transport->owner);
+
+	kref_put(&trace->kref, ltt_release_trace);
+}
+
+static int ltt_trace_destroy(char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace_struct *trace;
+
+	ltt_lock_traces();
+	trace = _ltt_trace_find(trace_name);
+	err = _ltt_trace_destroy(trace);
+	if (err)
+		goto error;
+	ltt_unlock_traces();
+	__ltt_trace_destroy(trace);
+	return err;
+
+	/* Error handling */
+error:
+	ltt_unlock_traces();
+	return err;
+}
+
+/* must be called from within a traces lock. */
+static int _ltt_trace_start(struct ltt_trace_struct *trace)
+{
+	int err = 0;
+
+	if (trace == NULL) {
+		err = ENOENT;
+		goto traces_error;
+	}
+	if (trace->active)
+		printk(KERN_INFO "LTT : Tracing already active for trace %s\n",
+				trace->trace_name);
+	if (!try_module_get(ltt_run_filter_owner)) {
+		err = ENODEV;
+		printk(KERN_ERR "LTT : Can't lock filter module.\n");
+		goto get_ltt_run_filter_error;
+	}
+	if (ltt_traces.num_active_traces == 0) {
+		probe_id_defrag();
+#ifdef CONFIG_LTT_HEARTBEAT
+		if (ltt_heartbeat_trigger(LTT_HEARTBEAT_START)) {
+			err = ENODEV;
+			printk(KERN_ERR
+				"LTT : Heartbeat timer module not present.\n");
+			goto ltt_heartbeat_error;
+		}
+#endif
+		init_timer(&ltt_async_wakeup_timer);
+		ltt_async_wakeup_timer.function = async_wakeup;
+		ltt_async_wakeup_timer.expires =
+			jiffies + LTT_PERCPU_TIMER_INTERVAL;
+		add_timer(&ltt_async_wakeup_timer);
+		set_kernel_trace_flag_all_tasks();
+	}
+	/*
+	 * Write a full 64 bits TSC in each trace. Used for successive
+	 * trace stop/start.
+	 */
+	ltt_write_full_tsc(trace);
+	trace->active = 1;
+	/* Read by trace points without protection : be careful */
+	ltt_traces.num_active_traces++;
+	return err;
+
+	/* error handling */
+#ifdef CONFIG_LTT_HEARTBEAT
+ltt_heartbeat_error:
+#endif
+	module_put(ltt_run_filter_owner);
+get_ltt_run_filter_error:
+traces_error:
+	return err;
+}
+
+static int ltt_trace_start(char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace_struct *trace;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find(trace_name);
+	if (trace == NULL)
+		goto no_trace;
+	err = _ltt_trace_start(trace);
+
+	ltt_unlock_traces();
+
+	/*
+	 * Call the kernel state dump.
+	 * Events will be mixed with real kernel events, it's ok.
+	 * Notice that there is no protection on the trace : that's exactly
+	 * why we iterate on the list and check for trace equality instead of
+	 * directly using this trace handle inside the logging function.
+	 */
+
+	ltt_dump_marker_state(trace);
+
+	if (!try_module_get(ltt_statedump_owner)) {
+		err = ENODEV;
+		printk(KERN_ERR
+			"LTT : Can't lock state dump module.\n");
+	} else {
+		ltt_statedump_functor(trace);
+		module_put(ltt_statedump_owner);
+	}
+
+	return err;
+
+	/* Error handling */
+no_trace:
+	ltt_unlock_traces();
+	return err;
+}
+
+
+/* must be called from within traces lock */
+static int _ltt_trace_stop(struct ltt_trace_struct *trace)
+{
+	int err = EPERM;
+
+	if (trace == NULL) {
+		err = ENOENT;
+		goto traces_error;
+	}
+	if (!trace->active)
+		printk(KERN_INFO "LTT : Tracing not active for trace %s\n",
+				trace->trace_name);
+	if (trace->active) {
+		trace->active = 0;
+		ltt_traces.num_active_traces--;
+		synchronize_sched(); /* Wait for each tracing to be finished */
+	}
+	if (ltt_traces.num_active_traces == 0) {
+		clear_kernel_trace_flag_all_tasks();
+#ifdef CONFIG_LTT_HEARTBEAT
+	/* stop the heartbeat if we are the last active trace */
+		ltt_heartbeat_trigger(LTT_HEARTBEAT_STOP);
+#endif
+	}
+	module_put(ltt_run_filter_owner);
+	/* Everything went fine */
+	return 0;
+
+	/* Error handling */
+traces_error:
+	return err;
+}
+
+static int ltt_trace_stop(char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace_struct *trace;
+
+	ltt_lock_traces();
+	trace = _ltt_trace_find(trace_name);
+	err = _ltt_trace_stop(trace);
+	ltt_unlock_traces();
+	return err;
+}
+
+
+/* Exported functions */
+
+int ltt_control(enum ltt_control_msg msg, char *trace_name, char *trace_type,
+		union ltt_control_args args)
+{
+	int err = EPERM;
+
+	printk(KERN_ALERT "ltt_control : trace %s\n", trace_name);
+	switch (msg) {
+	case LTT_CONTROL_START:
+		printk(KERN_DEBUG "Start tracing %s\n", trace_name);
+		err = ltt_trace_start(trace_name);
+		break;
+	case LTT_CONTROL_STOP:
+		printk(KERN_DEBUG "Stop tracing %s\n", trace_name);
+		err = ltt_trace_stop(trace_name);
+		break;
+	case LTT_CONTROL_CREATE_TRACE:
+		printk(KERN_DEBUG "Creating trace %s\n", trace_name);
+		err = ltt_trace_create(trace_name, trace_type,
+			args.new_trace.mode,
+			args.new_trace.subbuf_size_low,
+			args.new_trace.n_subbufs_low,
+			args.new_trace.subbuf_size_med,
+			args.new_trace.n_subbufs_med,
+			args.new_trace.subbuf_size_high,
+			args.new_trace.n_subbufs_high);
+		break;
+	case LTT_CONTROL_DESTROY_TRACE:
+		printk(KERN_DEBUG "Destroying trace %s\n", trace_name);
+		err = ltt_trace_destroy(trace_name);
+		break;
+	}
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_control);
+
+int ltt_filter_control(enum ltt_filter_control_msg msg, char *trace_name)
+{
+	int err;
+	struct ltt_trace_struct *trace;
+
+	printk(KERN_DEBUG "ltt_filter_control : trace %s\n", trace_name);
+	ltt_lock_traces();
+	trace = _ltt_trace_find(trace_name);
+	if (trace == NULL) {
+		printk(KERN_ALERT
+			"Trace does not exist. Cannot proxy control request\n");
+		err = ENOENT;
+		goto trace_error;
+	}
+	if (!try_module_get(ltt_filter_control_owner)) {
+		err = ENODEV;
+		goto get_module_error;
+	}
+	switch (msg) {
+	case LTT_FILTER_DEFAULT_ACCEPT:
+		printk(KERN_DEBUG
+			"Proxy filter default accept %s\n", trace_name);
+		err = (*ltt_filter_control_functor)(msg, trace);
+		break;
+	case LTT_FILTER_DEFAULT_REJECT:
+		printk(KERN_DEBUG
+			"Proxy filter default reject %s\n", trace_name);
+		err = (*ltt_filter_control_functor)(msg, trace);
+		break;
+	default:
+		err = EPERM;
+	}
+	module_put(ltt_filter_control_owner);
+
+get_module_error:
+trace_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_filter_control);
+
+static void __exit ltt_exit(void)
+{
+	struct ltt_trace_struct *trace;
+
+	ltt_lock_traces();
+	/* Stop each trace and destroy it */
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		_ltt_trace_stop(trace);
+		/* _ltt_trace_destroy does a synchronize_sched() */
+		_ltt_trace_destroy(trace);
+		__ltt_trace_destroy(trace);
+	}
+	ltt_unlock_traces();
+}
+
+module_exit(ltt_exit)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Tracer");
diff -uprN linux-2.6.23.1.orig/ltt/probes/Makefile linux-2.6.23.1/ltt/probes/Makefile
--- linux-2.6.23.1.orig/ltt/probes/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/probes/Makefile	2007-11-28 10:13:52.380345046 +0100
@@ -0,0 +1,4 @@
+# LTT probes makefile
+
+obj-$(CONFIG_LTT_PROBE_STACK)	+= ltt-probe-stack_arch_$(ARCH).o
+obj-$(CONFIG_LTT_PROBE_FS)	+= ltt-probe-fs.o
diff -uprN linux-2.6.23.1.orig/ltt/probes/ltt-probe-fs.c linux-2.6.23.1/ltt/probes/ltt-probe-fs.c
--- linux-2.6.23.1.orig/ltt/probes/ltt-probe-fs.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/probes/ltt-probe-fs.c	2007-11-28 10:13:52.378345463 +0100
@@ -0,0 +1,98 @@
+/*
+ * FS probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-tracer.h>
+
+#include <asm/uaccess.h>
+
+/*
+ * Expects va args : (int elem_num, const char __user *s)
+ * Element size is implicit (sizeof(char)).
+ */
+static char *ltt_serialize_fs_data(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	void *serialize_private,
+	int align, const char *fmt, va_list *args)
+{
+	int elem_size;
+	int elem_num;
+	const char __user  *s;
+	unsigned long noncopy;
+
+	elem_num = va_arg(*args, int);
+	s = va_arg(*args, const char __user *);
+	elem_size = sizeof(*s);
+
+	if (align)
+		str += ltt_align((long)str, sizeof(int));
+	if (buffer)
+		*(int*)str = elem_num;
+	str += sizeof(int);
+
+	if (elem_num > 0) {
+		/* No alignment required for char */
+		if (buffer) {
+			noncopy = __copy_from_user_inatomic(str, s,
+					elem_num*elem_size);
+			memset(str+(elem_num*elem_size)-noncopy, 0, noncopy);
+		}
+		str += (elem_num*elem_size);
+	}
+	/* Following alignment for genevent compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+static struct ltt_available_probe probe_array[] =
+{
+	{ "fs_read_data", "%u %zd %k",
+		.probe_func = ltt_trace,
+		.callbacks[0] = ltt_serialize_data,
+		.callbacks[1] = ltt_serialize_fs_data },
+	{ "fs_write_data", "%u %zd %k",
+		.probe_func = ltt_trace,
+		.callbacks[0] = ltt_serialize_data,
+		.callbacks[1] = ltt_serialize_fs_data },
+};
+
+static int __init probe_init(void)
+{
+	int result, i;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		result = ltt_probe_register(&probe_array[i]);
+		if (result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[i].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	int i, err;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		err = ltt_probe_unregister(&probe_array[i]);
+		BUG_ON(err);
+	}
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("FS probe");
diff -uprN linux-2.6.23.1.orig/ltt/probes/ltt-probe-stack_arch_i386.c linux-2.6.23.1/ltt/probes/ltt-probe-stack_arch_i386.c
--- linux-2.6.23.1.orig/ltt/probes/ltt-probe-stack_arch_i386.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/probes/ltt-probe-stack_arch_i386.c	2007-11-28 10:13:52.379345255 +0100
@@ -0,0 +1,386 @@
+/*
+ * stack arch i386 probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ptrace.h>
+#include <linux/ltt-tracer.h>
+
+struct stack_dump_args {
+	unsigned long ebp;
+};
+
+#ifdef CONFIG_LTT_KERNEL_STACK
+/* Kernel specific trace stack dump routines, from arch/i386/kernel/traps.c */
+
+/*
+ * Event kernel_dump structures
+ * ONLY USE IT ON THE CURRENT STACK.
+ * It does not protect against other stack modifications and _will_
+ * cause corruption upon race between threads.
+ *
+ * FIXME : using __kernel_text_address here can break things if a module is
+ * loaded during tracing. The real function pointers are OK : if they are on our
+ * stack, it means that the module refcount must not be 0, but the problem comes
+ * from the "false positives" : that that appears to be a function pointer.
+ * The solution to this problem :
+ * Without frame pointers, we have one version with spinlock irqsave (never call
+ * this in NMI context, and another with a trylock, which can fail.
+ */
+
+static inline int dump_valid_stack_ptr(struct thread_info *tinfo, void *p)
+{
+	return	p > (void *)tinfo &&
+		p < (void *)tinfo + THREAD_SIZE - 3;
+}
+
+static inline unsigned long dump_context_stack(struct thread_info *tinfo,
+		unsigned long *stack, unsigned long ebp,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	unsigned long addr;
+
+#ifdef	CONFIG_FRAME_POINTER
+	while (dump_valid_stack_ptr(tinfo, (void *)ebp)) {
+		unsigned long new_ebp;
+		if (buffer != NULL) {
+			unsigned long *dest = (unsigned long*)*str;
+			addr = *(unsigned long *)(ebp + 4);
+			*dest = addr;
+		} else {
+			(*len)++;
+		}
+		(*str) += sizeof(unsigned long);
+		new_ebp = *(unsigned long *)ebp;
+		if (new_ebp <= ebp)
+			break;
+		ebp = new_ebp;
+	}
+#else
+	while (dump_valid_stack_ptr(tinfo, stack)) {
+		addr = *stack++;
+		if (__kernel_text_address(addr)) {
+			if (buffer != NULL) {
+				unsigned long *dest = (unsigned long*)*str;
+				*dest = addr;
+			} else {
+				(*len)++;
+			}
+			(*str) += sizeof(unsigned long);
+		}
+	}
+#endif
+	return ebp;
+}
+
+static void dump_trace(unsigned long * stack,
+		unsigned long ebp,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	while (1) {
+		struct thread_info *context;
+		context = (struct thread_info *)
+			((unsigned long)stack & (~(THREAD_SIZE - 1)));
+		ebp = dump_context_stack(context, stack, ebp,
+				buffer, str, len);
+		stack = (unsigned long*)context->previous_esp;
+		if (!stack)
+			break;
+	}
+}
+
+static char *ltt_serialize_kernel_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	void *serialize_private,
+	int align, const char *fmt, va_list *args)
+{
+	struct stack_dump_args *stargs;
+
+	/* Ugly and crude argument passing hack part 2. */
+	stargs = serialize_private;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(unsigned long)));
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(unsigned long));
+	dump_trace((unsigned long*)stargs, stargs->ebp, buffer, &str,
+			&closure->cb_args[0]);
+
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_kernel_stack(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	struct ltt_probe_private_data call_data;
+	struct stack_dump_args stargs;
+	va_list args;
+	struct pt_regs *regs;
+
+#ifndef CONFIG_LTT_KERNEL_STACK
+	return;
+#endif
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || user_mode_vm(regs)))
+		goto end;
+	/* Grab ebp right from our regs */
+	asm ("movl %%ebp, %0" : "=r" (stargs.ebp) : );
+	/* Ugly and crude argument passing hack part 1. */
+	call_data.channel = 0;
+	call_data.trace = NULL;
+	call_data.force = 0;
+	call_data.id = MARKER_CORE_IDS;
+	call_data.serialize_private = &stargs;
+	ltt_vtrace(mdata, &call_data, fmt, &args);
+end:
+	va_end(args);
+}
+#endif /* CONFIG_LTT_KERNEL_STACK */
+
+#ifdef CONFIG_LTT_PROCESS_STACK
+/* FIXME : how could we autodetect this.... ?!? disabled for now. */
+#if 0
+static void dump_fp_process_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t next_ebp;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+
+	if (buffer)
+		*dest = regs->eip;
+	else
+		(*len)++;
+	dest++;
+
+	/* Start at the topmost ebp */
+	iter = (uint32_t*)regs->ebp;
+
+	/* Keep on going until we reach the end of the process' stack limit or
+	 * find an invalid ebp. */
+	while (!get_user(next_ebp, iter)) {
+		/* If another thread corrupts the thread user space stack
+		 * concurrently */
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		if (next_ebp <= (uint32_t)(iter+1))
+			break;
+		if (get_user(addr, (uint32_t*)next_ebp+1))
+			break;
+		if (buffer)
+			*dest = addr;
+		else
+			(*len)++;
+		dest++;
+		iter = (uint32_t*)next_ebp;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+#endif //0
+
+static void dump_nofp_process_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+	uint32_t *prev_iter, *beg_iter;
+
+	if (buffer)
+		*dest = regs->eip;
+	else
+		(*len)++;
+	dest++;
+
+	/*
+	 * FIXME : currently detects code addresses from executable only,
+	 * not libraries.
+	 */
+	/* Start at the top of the user stack */
+	prev_iter = beg_iter = iter = (uint32_t*) regs->esp;
+
+	while (!get_user(addr, iter)) {
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		/* Does this LOOK LIKE an address in the program */
+		if (addr > current->mm->start_code &&
+			addr < current->mm->end_code) {
+			if (buffer)
+				*dest = addr;
+			else
+				(*len)++;
+			dest++;
+		}
+		prev_iter = iter;
+		iter++;
+		if (iter >
+			prev_iter + CONFIG_LTT_PROCESS_MAX_FUNCTION_STACK)
+			break;
+		if (iter > beg_iter + CONFIG_LTT_PROCESS_MAX_STACK_LEN)
+			break;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+
+static char *ltt_serialize_process_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	void *serialize_private,
+	int align, const char *fmt, va_list *args)
+{
+	struct pt_regs *regs;
+
+	/* Ugly and crude argument passing hack part 2 (revisited). */
+	regs = serialize_private;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(unsigned long)));
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(unsigned long));
+	dump_nofp_process_stack(regs, buffer, &str, &closure->cb_args[0]);
+
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_process_stack(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	struct ltt_probe_private_data call_data;
+	va_list args;
+	struct pt_regs *regs;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || !user_mode_vm(regs)))
+		goto end;
+	/* Ugly and crude argument passing hack part 1 (revisited). */
+	call_data.channel = 0;
+	call_data.trace = NULL;
+	call_data.force = 0;
+	call_data.id = MARKER_CORE_IDS;
+	call_data.serialize_private = regs;
+	ltt_vtrace(mdata, &call_data, fmt, &args);
+end:
+	va_end(args);
+}
+#endif /* CONFIG_LTT_PROCESS_STACK */
+
+static struct ltt_available_probe probe_array[] =
+{
+#ifdef CONFIG_LTT_KERNEL_STACK
+	{ "stack_arch_irq_dump_kernel_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+	},
+	{ "stack_arch_nmi_dump_kernel_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+	},
+	{ "stack_arch_syscall_dump_kernel_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+	},
+#endif /* CONFIG_LTT_KERNEL_STACK */
+#ifdef CONFIG_LTT_PROCESS_STACK
+	{ "stack_arch_irq_dump_process32_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_process_stack,
+		.callbacks[0] = ltt_serialize_process_stack,
+	},
+	{ "stack_arch_syscall_dump_process32_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_process_stack,
+		.callbacks[0] = ltt_serialize_process_stack,
+	},
+#endif /* CONFIG_LTT_PROCESS_STACK */
+};
+
+static int __init probe_init(void)
+{
+	int result, i;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		result = ltt_probe_register(&probe_array[i]);
+		if (result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[i].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	int i, err;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		err = ltt_probe_unregister(&probe_array[i]);
+		BUG_ON(err);
+	}
+}
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Stack i386 probe");
diff -uprN linux-2.6.23.1.orig/ltt/probes/ltt-probe-stack_arch_x86_64.c linux-2.6.23.1/ltt/probes/ltt-probe-stack_arch_x86_64.c
--- linux-2.6.23.1.orig/ltt/probes/ltt-probe-stack_arch_x86_64.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.23.1/ltt/probes/ltt-probe-stack_arch_x86_64.c	2007-11-28 10:13:52.380345046 +0100
@@ -0,0 +1,653 @@
+/*
+ * stack arch x86_64 probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ptrace.h>
+#include <linux/ltt-tracer.h>
+
+#include <asm/desc_defs.h>
+#include <asm/user32.h>
+
+#ifdef CONFIG_LTT_KERNEL_STACK
+/* Kernel specific trace stack dump routines, from arch/x86_64/kernel/traps.c */
+
+/*
+ * Event kernel_dump structures
+ * ONLY USE IT ON THE CURRENT STACK.
+ * It does not protect against other stack modifications and _will_
+ * cause corruption upon race between threads.
+ * FIXME : using __kernel_text_address here can break things if a module is
+ * loaded during tracing. The real function pointers are OK : if they are on our
+ * stack, it means that the module refcount must not be 0, but the problem comes
+ * from the "false positives" : that that appears to be a function pointer.
+ * The solution to this problem :
+ * Without frame pointers, we have one version with spinlock irqsave (never call
+ * this in NMI context, and another with a trylock, which can fail.
+ */
+
+/* From traps.c */
+
+static unsigned long *trace_in_exception_stack(unsigned cpu,
+					unsigned long stack,
+					unsigned *usedp, const char **idp)
+{
+	static char ids[][8] = {
+		[DEBUG_STACK - 1] = "#DB",
+		[NMI_STACK - 1] = "NMI",
+		[DOUBLEFAULT_STACK - 1] = "#DF",
+		[STACKFAULT_STACK - 1] = "#SS",
+		[MCE_STACK - 1] = "#MC",
+#if DEBUG_STKSZ > EXCEPTION_STKSZ
+		[N_EXCEPTION_STACKS ... N_EXCEPTION_STACKS + DEBUG_STKSZ / EXCEPTION_STKSZ - 2] = "#DB[?]"
+#endif
+	};
+	unsigned k;
+
+	/*
+	 * Iterate over all exception stacks, and figure out whether
+	 * 'stack' is in one of them:
+	 */
+	for (k = 0; k < N_EXCEPTION_STACKS; k++) {
+		unsigned long end = per_cpu(orig_ist, cpu).ist[k];
+		/*
+		 * Is 'stack' above this exception frame's end?
+		 * If yes then skip to the next frame.
+		 */
+		if (stack >= end)
+			continue;
+		/*
+		 * Is 'stack' above this exception frame's start address?
+		 * If yes then we found the right frame.
+		 */
+		if (stack >= end - EXCEPTION_STKSZ) {
+			/*
+			 * Make sure we only iterate through an exception
+			 * stack once. If it comes up for the second time
+			 * then there's something wrong going on - just
+			 * break out and return NULL:
+			 */
+			if (*usedp & (1U << k))
+				break;
+			*usedp |= 1U << k;
+			*idp = ids[k];
+			return (unsigned long *)end;
+		}
+		/*
+		 * If this is a debug stack, and if it has a larger size than
+		 * the usual exception stacks, then 'stack' might still
+		 * be within the lower portion of the debug stack:
+		 */
+#if DEBUG_STKSZ > EXCEPTION_STKSZ
+		if (k == DEBUG_STACK - 1 && stack >= end - DEBUG_STKSZ) {
+			unsigned j = N_EXCEPTION_STACKS - 1;
+
+			/*
+			 * Black magic. A large debug stack is composed of
+			 * multiple exception stack entries, which we
+			 * iterate through now. Dont look:
+			 */
+			do {
+				++j;
+				end -= EXCEPTION_STKSZ;
+				ids[j][4] = '1' + (j - N_EXCEPTION_STACKS);
+			} while (stack < end - EXCEPTION_STKSZ);
+			if (*usedp & (1U << j))
+				break;
+			*usedp |= 1U << j;
+			*idp = ids[j];
+			return (unsigned long *)end;
+		}
+#endif
+	}
+	return NULL;
+}
+
+/*
+ * x86-64 can have up to three kernel stacks:
+ * process stack
+ * interrupt stack
+ * severe exception (double fault, nmi, stack fault, debug, mce) hardware stack
+ */
+
+static inline int valid_stack_ptr(struct thread_info *tinfo, void *p)
+{
+	void *t = (void *)tinfo;
+        return p > t && p < t + THREAD_SIZE - 3;
+}
+
+/* Must be called with preemption disabled */
+static void dump_trace(unsigned long *stack,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	const unsigned cpu = smp_processor_id();
+	unsigned long *irqstack_end = (unsigned long *)cpu_pda(cpu)->irqstackptr;
+	unsigned used = 0;
+	struct thread_info *tinfo;
+
+#define HANDLE_STACK(cond) \
+	do while (cond) { \
+		unsigned long addr = *stack++; \
+		if (__kernel_text_address(addr)) { \
+			/* \
+			 * If the address is either in the text segment of the \
+			 * kernel, or in the region which contains vmalloc'ed \
+			 * memory, it *may* be the address of a calling \
+			 * routine; if so, print it so that someone tracing \
+			 * down the cause of the crash will be able to figure \
+			 * out the call path that was taken. \
+			 */ \
+			if (buffer != NULL) { \
+				unsigned long *dest = (unsigned long*)*str; \
+				*dest = addr; \
+			} else { \
+				(*len)++; \
+			} \
+			(*str) += sizeof(unsigned long); \
+		} \
+	} while (0)
+
+	/*
+	 * Print function call entries in all stacks, starting at the
+	 * current stack address. If the stacks consist of nested
+	 * exceptions
+	 */
+	for (;;) {
+		const char *id;
+		unsigned long *estack_end;
+		estack_end = trace_in_exception_stack(cpu, (unsigned long)stack,
+						&used, &id);
+
+		if (estack_end) {
+			HANDLE_STACK (stack < estack_end);
+			/*
+			 * We link to the next stack via the
+			 * second-to-last pointer (index -2 to end) in the
+			 * exception stack:
+			 */
+			stack = (unsigned long *) estack_end[-2];
+			continue;
+		}
+		if (irqstack_end) {
+			unsigned long *irqstack;
+			irqstack = irqstack_end -
+				(IRQSTACKSIZE - 64) / sizeof(*irqstack);
+
+			if (stack >= irqstack && stack < irqstack_end) {
+				HANDLE_STACK (stack < irqstack_end);
+				/*
+				 * We link to the next stack (which would be
+				 * the process stack normally) the last
+				 * pointer (index -1 to end) in the IRQ stack:
+				 */
+				stack = (unsigned long *) (irqstack_end[-1]);
+				irqstack_end = NULL;
+				continue;
+			}
+		}
+		break;
+	}
+
+	/*
+	 * This handles the process stack:
+	 */
+	tinfo = current_thread_info();
+	HANDLE_STACK (valid_stack_ptr(tinfo, stack));
+#undef HANDLE_STACK
+}
+
+
+static char *ltt_serialize_kernel_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	void *serialize_private,
+	int align, const char *fmt, va_list *args)
+{
+	unsigned long *stack = serialize_private;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(unsigned long)));
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(unsigned long));
+	dump_trace(stack, buffer, &str, &closure->cb_args[0]);
+
+	/* Following alignment for genevent compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_kernel_stack(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	struct ltt_probe_private_data call_data;
+	struct pt_regs *regs;
+	va_list args;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || user_mode_vm(regs)))
+		goto end;
+	call_data.channel = 0;
+	call_data.trace = NULL;
+	call_data.force = 0;
+	call_data.id = MARKER_CORE_IDS;
+	call_data.serialize_private = regs;
+	ltt_vtrace(mdata, &call_data, fmt, &args);
+end:
+	va_end(args);
+}
+#endif /* CONFIG_LTT_KERNEL_STACK */
+
+#ifdef CONFIG_LTT_PROCESS_STACK
+/* FIXME : how could we autodetect this.... ?!? disabled for now. */
+#if 0
+static void dump_fp_process32_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t next_ebp;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+
+	if (buffer)
+		*dest = PTR_LOW(regs->rip);
+	else
+		(*len)++;
+	dest++;
+
+	/* Start at the topmost ebp */
+	iter = (uint32_t*)PTR_LOW(regs->rbp);
+
+	/* Keep on going until we reach the end of the process' stack limit or
+	 * find an invalid ebp. */
+	while (!get_user(next_ebp, iter)) {
+		/* If another thread corrupts the thread user space stack
+		 * concurrently */
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		if (PTR_LOW(next_ebp) <= (unsigned long)(iter+1))
+			break;
+		if (get_user(addr, (uint32_t*)PTR_LOW(next_ebp)+1))
+			break;
+		if (buffer)
+			*dest = addr;
+		else
+			(*len)++;
+		dest++;
+		iter = (uint32_t*)PTR_LOW(next_ebp);
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+#endif //0
+
+static void dump_nofp_process32_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+	uint32_t *prev_iter, *beg_iter;
+
+	if (buffer)
+		*dest = PTR_LOW(regs->rip);
+	else
+		(*len)++;
+	dest++;
+
+	/*
+	 * FIXME : currently detects code addresses from executable only,
+	 * not libraries.
+	 */
+	/* Start at the top of the user stack */
+	prev_iter = beg_iter = iter = (uint32_t*)PTR_LOW(regs->rsp);
+
+	while (!get_user(addr, iter)) {
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		/* Does this LOOK LIKE an address in the program */
+		if (addr > current->mm->start_code &&
+			addr < current->mm->end_code) {
+			if (buffer)
+				*dest = addr;
+			else
+				(*len)++;
+			dest++;
+		}
+		prev_iter = iter;
+		iter++;
+		if (iter >
+			prev_iter + CONFIG_LTT_PROCESS_MAX_FUNCTION_STACK)
+			break;
+		if (iter > beg_iter + CONFIG_LTT_PROCESS_MAX_STACK_LEN)
+			break;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+
+#if 0
+static void dump_fp_process64_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint64_t addr;
+	uint64_t next_rbp;
+	uint64_t *iter;
+	uint64_t *dest = (uint64_t*)*str;
+
+	if (buffer)
+		*dest = regs->rip;
+	else
+		(*len)++;
+	dest++;
+
+	/* Start at the topmost rbp */
+	iter = (uint64_t*)regs->rbp;
+
+	/* Keep on going until we reach the end of the process' stack limit or
+	 * find an invalid ebp. */
+	while (!get_user(next_rbp, iter)) {
+		/* If another thread corrupts the thread user space stack
+		 * concurrently */
+		if (buffer)
+			if (dest == (uint64_t*)(*str) + *len)
+				break;
+		if (next_rbp <= (uint64_t)(iter+1))
+			break;
+		if (get_user(addr, (uint64_t*)next_rbp+1))
+			break;
+		if (buffer)
+			*dest = addr;
+		else
+			(*len)++;
+		dest++;
+		iter = (uint64_t*)next_rbp;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint64_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+#endif //0
+
+static void dump_nofp_process64_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint64_t addr;
+	uint64_t *iter;
+	uint64_t *dest = (uint64_t*)*str;
+	uint64_t *prev_iter, *beg_iter;
+
+	if (buffer)
+		*dest = regs->rip;
+	else
+		(*len)++;
+	dest++;
+
+	/*
+	 * FIXME : currently detects code addresses from executable only,
+	 * not libraries.
+	 */
+	/* Start at the top of the user stack */
+	prev_iter = beg_iter = iter = (uint64_t*) regs->rsp;
+
+	while (!get_user(addr, iter)) {
+		if (buffer)
+			if (dest == (uint64_t*)(*str) + *len)
+				break;
+		/* Does this LOOK LIKE an address in the program */
+		if (addr > current->mm->start_code &&
+			addr < current->mm->end_code) {
+			if (buffer)
+				*dest = addr;
+			else
+				(*len)++;
+			dest++;
+		}
+		prev_iter = iter;
+		iter++;
+		if (iter >
+			prev_iter + CONFIG_LTT_PROCESS_MAX_FUNCTION_STACK)
+			break;
+		if (iter > beg_iter + CONFIG_LTT_PROCESS_MAX_STACK_LEN)
+			break;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint64_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+
+static char *ltt_serialize_process32_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	void *serialize_private,
+	int align, const char *fmt, va_list *args)
+{
+	struct pt_regs *regs;
+
+	regs = serialize_private;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(uint32_t)));
+
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(uint32_t));
+	dump_nofp_process32_stack(regs, buffer, &str,
+		&closure->cb_args[0]);
+
+	/* Following alignment for genevent compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+static char *ltt_serialize_process64_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	void *serialize_private,
+	int align, const char *fmt, va_list *args)
+{
+	struct pt_regs *regs;
+
+	regs = serialize_private;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(uint64_t)));
+
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(uint64_t));
+	dump_nofp_process64_stack(regs, buffer, &str,
+		&closure->cb_args[0]);
+
+	/* Following alignment for genevent compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_process32_stack(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	struct ltt_probe_private_data call_data;
+	va_list args;
+	struct pt_regs *regs;
+
+	if (!test_thread_flag(TIF_IA32))
+		return;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || !user_mode_vm(regs)))
+		goto end;
+	call_data.channel = 0;
+	call_data.trace = NULL;
+	call_data.force = 0;
+	call_data.id = MARKER_CORE_IDS;
+	call_data.serialize_private = regs;
+	ltt_vtrace(mdata, &call_data, fmt, &args);
+end:
+	va_end(args);
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_process64_stack(const struct marker *mdata, void *private,
+	const char *fmt, ...)
+{
+	struct ltt_probe_private_data call_data;
+	va_list args;
+	struct pt_regs *regs;
+
+	if (test_thread_flag(TIF_IA32))
+		return;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || !user_mode_vm(regs)))
+		goto end;
+	call_data.channel = 0;
+	call_data.trace = NULL;
+	call_data.force = 0;
+	call_data.id = MARKER_CORE_IDS;
+	call_data.serialize_private = regs;
+	ltt_vtrace(mdata, &call_data, fmt, &args);
+end:
+	va_end(args);
+}
+#endif /* CONFIG_LTT_PROCESS_STACK */
+
+static struct ltt_available_probe probe_array[] =
+{
+#ifdef CONFIG_LTT_KERNEL_STACK
+	{ "stack_arch_irq_dump_kernel_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+	},
+	{ "stack_arch_nmi_dump_kernel_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+	},
+	{ "stack_arch_syscall_dump_kernel_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+	},
+#endif /* CONFIG_LTT_KERNEL_STACK */
+#ifdef CONFIG_LTT_PROCESS_STACK
+	{ "stack_arch_irq_dump_process32_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_process32_stack,
+		.callbacks[0] = ltt_serialize_process32_stack,
+	},
+	{ "stack_arch_syscall_dump_process32_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_process32_stack,
+		.callbacks[0] = ltt_serialize_process32_stack,
+	},
+	{ "stack_arch_irq_dump_process64_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_process64_stack,
+		.callbacks[0] = ltt_serialize_process64_stack,
+	},
+	{ "stack_arch_syscall_dump_process64_stack", MARK_NOARGS,
+		.probe_func = ltt_trace_process64_stack,
+		.callbacks[0] = ltt_serialize_process64_stack,
+	},
+#endif /* CONFIG_LTT_PROCESS_STACK */
+};
+
+static int __init probe_init(void)
+{
+	int result, i;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		result = ltt_probe_register(&probe_array[i]);
+		if (result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[i].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	int i, err;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		err = ltt_probe_unregister(&probe_array[i]);
+		BUG_ON(err);
+	}
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Stack x86_64 probe");
diff -uprN linux-2.6.23.1.orig/mm/bounce.c linux-2.6.23.1/mm/bounce.c
--- linux-2.6.23.1.orig/mm/bounce.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/mm/bounce.c	2007-11-28 10:13:52.382344629 +0100
@@ -13,7 +13,7 @@
 #include <linux/init.h>
 #include <linux/hash.h>
 #include <linux/highmem.h>
-#include <linux/blktrace_api.h>
+#include <linux/marker.h>
 #include <asm/tlbflush.h>
 
 #define POOL_SIZE	64
@@ -237,7 +237,7 @@ static void __blk_queue_bounce(struct re
 	if (!bio)
 		return;
 
-	blk_add_trace_bio(q, *bio_orig, BLK_TA_BOUNCE);
+	trace_mark(blk_bio_bounce, "%p %p", q, *bio_orig);
 
 	/*
 	 * at least one page was bounced, fill in possible non-highmem
diff -uprN linux-2.6.23.1.orig/mm/filemap.c linux-2.6.23.1/mm/filemap.c
--- linux-2.6.23.1.orig/mm/filemap.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/mm/filemap.c	2007-11-28 10:13:52.383344421 +0100
@@ -511,9 +511,13 @@ void fastcall wait_on_page_bit(struct pa
 {
 	DEFINE_WAIT_BIT(wait, &page->flags, bit_nr);
 
+	trace_mark(mm_filemap_wait_start, "address %p", page_address(page));
+
 	if (test_bit(bit_nr, &page->flags))
 		__wait_on_bit(page_waitqueue(page), &wait, sync_page,
 							TASK_UNINTERRUPTIBLE);
+
+	trace_mark(mm_filemap_wait_end, "address %p", page_address(page));
 }
 EXPORT_SYMBOL(wait_on_page_bit);
 
diff -uprN linux-2.6.23.1.orig/mm/highmem.c linux-2.6.23.1/mm/highmem.c
--- linux-2.6.23.1.orig/mm/highmem.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/mm/highmem.c	2007-11-28 10:13:52.384344212 +0100
@@ -26,7 +26,7 @@
 #include <linux/init.h>
 #include <linux/hash.h>
 #include <linux/highmem.h>
-#include <linux/blktrace_api.h>
+#include <linux/marker.h>
 #include <asm/tlbflush.h>
 
 /*
diff -uprN linux-2.6.23.1.orig/mm/memory.c linux-2.6.23.1/mm/memory.c
--- linux-2.6.23.1.orig/mm/memory.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/mm/memory.c	2007-11-28 10:13:52.385344004 +0100
@@ -50,6 +50,8 @@
 #include <linux/delayacct.h>
 #include <linux/init.h>
 #include <linux/writeback.h>
+#include <linux/kprobes.h>
+#include <linux/mutex.h>
 
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -84,6 +86,12 @@ EXPORT_SYMBOL(high_memory);
 
 int randomize_va_space __read_mostly = 1;
 
+/*
+ * mutex protecting text section modification (dynamic code patching).
+ * some users need to sleep (allocating memory...) while they hold this lock.
+ */
+static DEFINE_MUTEX(text_mutex);
+
 static int __init disable_randmaps(char *s)
 {
 	randomize_va_space = 0;
@@ -2164,6 +2172,7 @@ static int do_swap_page(struct mm_struct
 	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
 	page = lookup_swap_cache(entry);
 	if (!page) {
+		trace_mark(mm_swap_in, "address #p%lu", address);
 		grab_swap_token(); /* Contend for token _before_ read-in */
  		swapin_readahead(entry, address, vma);
  		page = read_swap_cache_async(entry, vma, address);
@@ -2636,30 +2645,43 @@ unlock:
 int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 		unsigned long address, int write_access)
 {
+	int res;
 	pgd_t *pgd;
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
 
+	trace_mark(mm_handle_fault_entry, "address %lu ip #p%ld",
+		address, KSTK_EIP(current));
 	__set_current_state(TASK_RUNNING);
 
 	count_vm_event(PGFAULT);
 
-	if (unlikely(is_vm_hugetlb_page(vma)))
-		return hugetlb_fault(mm, vma, address, write_access);
+	if (unlikely(is_vm_hugetlb_page(vma))) {
+		res = hugetlb_fault(mm, vma, address, write_access);
+		goto end;
+	}
 
 	pgd = pgd_offset(mm, address);
 	pud = pud_alloc(mm, pgd, address);
-	if (!pud)
-		return VM_FAULT_OOM;
+	if (!pud) {
+		res = VM_FAULT_OOM;
+		goto end;
+	}
 	pmd = pmd_alloc(mm, pud, address);
-	if (!pmd)
-		return VM_FAULT_OOM;
+	if (!pmd) {
+		res = VM_FAULT_OOM;
+		goto end;
+	}
 	pte = pte_alloc_map(mm, pmd, address);
-	if (!pte)
-		return VM_FAULT_OOM;
-
-	return handle_pte_fault(mm, vma, address, pte, pmd, write_access);
+	if (!pte) {
+		res = VM_FAULT_OOM;
+		goto end;
+	}
+	res = handle_pte_fault(mm, vma, address, pte, pmd, write_access);
+end:
+	trace_mark(mm_handle_fault_exit, MARK_NOARGS);
+	return res;
 }
 
 #ifndef __PAGETABLE_PUD_FOLDED
@@ -2867,3 +2889,29 @@ int access_process_vm(struct task_struct
 	return buf - old_buf;
 }
 EXPORT_SYMBOL_GPL(access_process_vm);
+
+/**
+ * kernel_text_lock     -   Take the kernel text modification lock
+ *
+ * Insures mutual write exclusion of kernel and modules text live text
+ * modification. Should be used for code patching.
+ * Users of this lock can sleep.
+ */
+void __kprobes kernel_text_lock(void)
+{
+	mutex_lock(&text_mutex);
+}
+EXPORT_SYMBOL_GPL(kernel_text_lock);
+
+/**
+ * kernel_text_unlock   -   Release the kernel text modification lock
+ *
+ * Insures mutual write exclusion of kernel and modules text live text
+ * modification. Should be used for code patching.
+ * Users of this lock can sleep.
+ */
+void __kprobes kernel_text_unlock(void)
+{
+	mutex_unlock(&text_mutex);
+}
+EXPORT_SYMBOL_GPL(kernel_text_unlock);
diff -uprN linux-2.6.23.1.orig/mm/page_alloc.c linux-2.6.23.1/mm/page_alloc.c
--- linux-2.6.23.1.orig/mm/page_alloc.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/mm/page_alloc.c	2007-11-28 10:13:52.387343587 +0100
@@ -507,6 +507,9 @@ static void __free_pages_ok(struct page 
 	int i;
 	int reserved = 0;
 
+	trace_mark(mm_page_free, "order %u address %p",
+		order, page_address(page));
+
 	for (i = 0 ; i < (1 << order) ; ++i)
 		reserved += free_pages_check(page + i);
 	if (reserved)
@@ -1414,6 +1417,8 @@ fastcall unsigned long __get_free_pages(
 	page = alloc_pages(gfp_mask, order);
 	if (!page)
 		return 0;
+	trace_mark(mm_page_alloc, "order %u address %p",
+		order, page_address(page));
 	return (unsigned long) page_address(page);
 }
 
diff -uprN linux-2.6.23.1.orig/mm/page_io.c linux-2.6.23.1/mm/page_io.c
--- linux-2.6.23.1.orig/mm/page_io.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/mm/page_io.c	2007-11-28 10:13:52.388343379 +0100
@@ -122,6 +122,7 @@ int swap_writepage(struct page *page, st
 		rw |= (1 << BIO_RW_SYNC);
 	count_vm_event(PSWPOUT);
 	set_page_writeback(page);
+	trace_mark(mm_swap_out, "address %p", page_address(page));
 	unlock_page(page);
 	submit_bio(rw, bio);
 out:
diff -uprN linux-2.6.23.1.orig/net/core/dev.c linux-2.6.23.1/net/core/dev.c
--- linux-2.6.23.1.orig/net/core/dev.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/net/core/dev.c	2007-11-28 10:13:52.390342962 +0100
@@ -1580,6 +1580,8 @@ int dev_queue_xmit(struct sk_buff *skb)
 	}
 
 gso:
+	trace_mark(net_dev_xmit, "skb %p protocol #2u%hu", skb, skb->protocol);
+
 	spin_lock_prefetch(&dev->queue_lock);
 
 	/* Disable soft irqs for various locks below. Also
@@ -1943,6 +1945,9 @@ int netif_receive_skb(struct sk_buff *sk
 
 	__get_cpu_var(netdev_rx_stat).total++;
 
+	trace_mark(net_dev_receive, "skb %p protocol #2u%hu",
+		skb, skb->protocol);
+
 	skb_reset_network_header(skb);
 	skb_reset_transport_header(skb);
 	skb->mac_len = skb->network_header - skb->mac_header;
diff -uprN linux-2.6.23.1.orig/net/ipv4/devinet.c linux-2.6.23.1/net/ipv4/devinet.c
--- linux-2.6.23.1.orig/net/ipv4/devinet.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/net/ipv4/devinet.c	2007-11-28 10:13:52.391342753 +0100
@@ -264,6 +264,8 @@ static void __inet_del_ifa(struct in_dev
 		struct in_ifaddr **ifap1 = &ifa1->ifa_next;
 
 		while ((ifa = *ifap1) != NULL) {
+			trace_mark(net_del_ifa_ipv4, "label %s",
+				ifa->ifa_label);
 			if (!(ifa->ifa_flags & IFA_F_SECONDARY) &&
 			    ifa1->ifa_scope <= ifa->ifa_scope)
 				last_prim = ifa;
@@ -370,6 +372,9 @@ static int __inet_insert_ifa(struct in_i
 			}
 			ifa->ifa_flags |= IFA_F_SECONDARY;
 		}
+		trace_mark(net_insert_ifa_ipv4, "label %s address #4u%lu",
+			ifa->ifa_label,
+			(unsigned long)ifa->ifa_address);
 	}
 
 	if (!(ifa->ifa_flags & IFA_F_SECONDARY)) {
diff -uprN linux-2.6.23.1.orig/net/socket.c linux-2.6.23.1/net/socket.c
--- linux-2.6.23.1.orig/net/socket.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/net/socket.c	2007-11-28 10:13:52.393342336 +0100
@@ -562,6 +562,11 @@ int sock_sendmsg(struct socket *sock, st
 	struct sock_iocb siocb;
 	int ret;
 
+	trace_mark(net_socket_sendmsg,
+		"sock %p family %d type %d protocol %d size %zu",
+		sock, sock->sk->sk_family, sock->sk->sk_type,
+		sock->sk->sk_protocol, size);
+
 	init_sync_kiocb(&iocb, NULL);
 	iocb.private = &siocb;
 	ret = __sock_sendmsg(&iocb, sock, msg, size);
@@ -645,7 +650,13 @@ int sock_recvmsg(struct socket *sock, st
 	struct sock_iocb siocb;
 	int ret;
 
+	trace_mark(net_socket_recvmsg,
+		"sock %p family %d type %d protocol %d size %zu",
+		sock, sock->sk->sk_family, sock->sk->sk_type,
+		sock->sk->sk_protocol, size);
+
 	init_sync_kiocb(&iocb, NULL);
+
 	iocb.private = &siocb;
 	ret = __sock_recvmsg(&iocb, sock, msg, size, flags);
 	if (-EIOCBQUEUED == ret)
@@ -1207,6 +1218,11 @@ asmlinkage long sys_socket(int family, i
 	if (retval < 0)
 		goto out_release;
 
+	trace_mark(net_socket_create,
+		"sock %p family %d type %d protocol %d fd %d",
+		sock, sock->sk->sk_family, sock->sk->sk_type,
+		sock->sk->sk_protocol, retval);
+
 out:
 	/* It may be already another descriptor 8) Not kernel problem. */
 	return retval;
@@ -2013,6 +2029,8 @@ asmlinkage long sys_socketcall(int call,
 	a0 = a[0];
 	a1 = a[1];
 
+	trace_mark(net_socket_call, "call %d a0 %lu", call, a0);
+
 	switch (call) {
 	case SYS_SOCKET:
 		err = sys_socket(a0, a1, a[2]);
diff -uprN linux-2.6.23.1.orig/scripts/kallsyms.c linux-2.6.23.1/scripts/kallsyms.c
--- linux-2.6.23.1.orig/scripts/kallsyms.c	2007-10-12 18:43:44.000000000 +0200
+++ linux-2.6.23.1/scripts/kallsyms.c	2007-11-28 10:13:52.393342336 +0100
@@ -34,7 +34,7 @@
 
 struct sym_entry {
 	unsigned long long addr;
-	unsigned int len;
+	unsigned int len, start_pos;
 	unsigned char *sym;
 };
 
@@ -202,8 +202,10 @@ static void read_map(FILE *in)
 				exit (1);
 			}
 		}
-		if (read_symbol(in, &table[table_cnt]) == 0)
+		if (read_symbol(in, &table[table_cnt]) == 0) {
+			table[table_cnt].start_pos = table_cnt;
 			table_cnt++;
+		}
 	}
 }
 
@@ -507,6 +509,35 @@ static void optimize_token_table(void)
 }
 
 
+static int compare_symbols(const void *a, const void *b)
+{
+	struct sym_entry *sa, *sb;
+	int wa, wb;
+
+	sa = (struct sym_entry *) a;
+	sb = (struct sym_entry *) b;
+
+	/* sort by address first */
+	if (sa->addr > sb->addr)
+		return 1;
+	if (sa->addr < sb->addr)
+		return -1;
+
+	/* sort by "weakness" type */
+	wa = (sa->sym[0] == 'w') || (sa->sym[0] == 'W');
+	wb = (sb->sym[0] == 'w') || (sb->sym[0] == 'W');
+	if (wa != wb)
+		return wa - wb;
+
+	/* sort by initial order, so that other symbols are left undisturbed */
+	return sa->start_pos - sb->start_pos;
+}
+
+static void sort_symbols(void)
+{
+	qsort(table, table_cnt, sizeof(struct sym_entry), compare_symbols);
+}
+
 int main(int argc, char **argv)
 {
 	if (argc >= 2) {
@@ -527,6 +558,7 @@ int main(int argc, char **argv)
 		usage();
 
 	read_map(stdin);
+	sort_symbols();
 	optimize_token_table();
 	write_src();
 
