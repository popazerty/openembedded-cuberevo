This patch adds the LTTng-0.9.5 into the STLinux kernel linux-2.6.17.14_sh4_stm23ear_0101.  
It also updates relayFS and some parts of ltt core.  

The patch instruments both the common kernel as well as the sh architecture specific code.

Signed-off by Giuseppe Cavallaro <peppe.cavallaro@st.com>

Index: linux/arch/sh/Kconfig
===================================================================
--- linux.orig/arch/sh/Kconfig	2007-08-30 11:24:25.301624000 +0100
+++ linux/arch/sh/Kconfig	2007-08-30 12:39:33.683640000 +0100
@@ -931,6 +931,14 @@
 
 source "arch/sh/oprofile/Kconfig"
 
+menu "Instrumentation Support"
+
+source "kernel/Kconfig.marker"
+
+source "ltt/Kconfig"
+
+endmenu
+
 source "arch/sh/Kconfig.debug"
 
 source "security/Kconfig"
Index: linux/arch/sh/kernel/entry.S
===================================================================
--- linux.orig/arch/sh/kernel/entry.S	2007-08-30 11:04:42.435900000 +0100
+++ linux/arch/sh/kernel/entry.S	2007-08-30 12:39:33.690641000 +0100
@@ -398,6 +398,10 @@
 	 tst	#_TIF_NEED_RESCHED, r0
 	STI()
 	! XXX setup arguments...
+#ifdef CONFIG_LTT
+	mov	r15, r4
+	mov	#0, r5			! trace entry [0]
+#endif
 	mov.l	4f, r0			! do_syscall_trace
 	jsr	@r0
 	 nop
@@ -406,6 +410,10 @@
 
 	.align	2
 syscall_trace_entry:
+#ifdef CONFIG_LTT
+	mov	r15, r4			! pass stacked regs as arg
+	mov	#1, r5			! trace entry [1]
+#endif
 	!                     	Yes it is traced.
 	! XXX setup arguments...
 	mov.l	4f, r11		! Call do_syscall_trace which notifies
Index: linux/arch/sh/kernel/process.c
===================================================================
--- linux.orig/arch/sh/kernel/process.c	2007-08-30 11:04:42.552900000 +0100
+++ linux/arch/sh/kernel/process.c	2007-08-30 12:39:33.694641000 +0100
@@ -170,6 +170,7 @@
 
 int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 {	/* Don't use this in BL=1(cli).  Or else, CPU resets! */
+	unsigned long pid;
 	struct pt_regs regs;
 
 	memset(&regs, 0, sizeof(regs));
@@ -180,7 +181,11 @@
 	regs.sr = (1 << 30);
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+	pid =  do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0,
+			&regs, 0, NULL, NULL);
+	MARK(kernel_arch_kthread_create, "%ld %p", pid, fn);
+	return pid;
+
 }
 
 /*
Index: linux/arch/sh/kernel/ptrace.c
===================================================================
--- linux.orig/arch/sh/kernel/ptrace.c	2007-08-30 11:04:31.867856000 +0100
+++ linux/arch/sh/kernel/ptrace.c	2007-08-30 12:39:33.698641000 +0100
@@ -277,10 +277,23 @@
 	return ret;
 }
 
+#ifdef CONFIG_LTT
+asmlinkage void do_syscall_trace(struct pt_regs *regs, int entryexit)
+#else
 asmlinkage void do_syscall_trace(void)
+#endif
 {
 	struct task_struct *tsk = current;
 
+#ifdef CONFIG_LTT
+	if (entryexit) {
+                MARK(kernel_arch_syscall_entry, "%d %ld", regs->regs[3],
+				instruction_pointer(regs));
+	} else {
+                MARK(kernel_arch_syscall_exit, MARK_NOARGS);
+	}
+#endif
+
 	if (!test_thread_flag(TIF_SYSCALL_TRACE) &&
 	    !test_thread_flag(TIF_SINGLESTEP))
 		return;
Index: linux/arch/sh/kernel/sys_sh.c
===================================================================
--- linux.orig/arch/sh/kernel/sys_sh.c	2007-08-30 12:38:04.230285000 +0100
+++ linux/arch/sh/kernel/sys_sh.c	2007-08-30 12:39:33.702640000 +0100
@@ -22,6 +22,7 @@
 #include <linux/file.h>
 #include <linux/utsname.h>
 #include <asm/cacheflush.h>
+
 #include <asm/uaccess.h>
 #include <asm/ipc.h>
 #include <asm/cachectl.h>
@@ -176,6 +177,8 @@
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	MARK(kernel_arch_ipc_call, "%u %d", call, first);
+
 	if (call <= SEMCTL)
 		switch (call) {
 		case SEMOP:
Index: linux/arch/sh/kernel/syscalls.S
===================================================================
--- linux.orig/arch/sh/kernel/syscalls.S	2007-08-30 11:04:31.918856000 +0100
+++ linux/arch/sh/kernel/syscalls.S	2007-08-30 12:39:33.707640000 +0100
@@ -352,3 +352,5 @@
 	.long sys_sync_file_range
 	.long sys_tee
 	.long sys_vmsplice		/* 315 */
+	.long sys_ltt_trace_generic	/* 316 */
+	.long sys_ltt_register_generic  /* 317 */
Index: linux/arch/sh/kernel/traps.c
===================================================================
--- linux.orig/arch/sh/kernel/traps.c	2007-08-30 12:39:17.056576000 +0100
+++ linux/arch/sh/kernel/traps.c	2007-08-30 12:39:33.711640000 +0100
@@ -482,6 +482,9 @@
 
 	oldfs = get_fs();
 
+        MARK(kernel_arch_trap_entry, "%ld %ld", (error_code >> 5),
+                        instruction_pointer(regs));
+
 	if (user_mode(regs)) {
  		int si_code = BUS_ADRERR;
 
@@ -505,15 +508,20 @@
 		tmp = handle_unaligned_access(instruction, regs);
 		set_fs(oldfs);
 
-		if (tmp==0)
-			return; /* sorted */
+                if (tmp==0) {
+			MARK(kernel_arch_trap_exit, MARK_NOARGS);
+                        return; /* sorted */
+                }
 
 	uspace_segv:
 		info.si_signo = SIGBUS;
 		info.si_errno = 0;
 		info.si_code = si_code;
 		info.si_addr = (void *) address;
+
 		force_sig_info(SIGBUS, &info, current);
+
+
 	} else {
 		if (regs->pc & 1)
 			die("unaligned program counter", regs, error_code);
@@ -530,6 +538,7 @@
 		handle_unaligned_access(instruction, regs);
 		set_fs(oldfs);
 	}
+	MARK(kernel_arch_trap_exit, MARK_NOARGS);
 }
 
 #ifdef CONFIG_SH_DSP
Index: linux/arch/sh/kernel/vmlinux.lds.S
===================================================================
--- linux.orig/arch/sh/kernel/vmlinux.lds.S	2007-08-30 11:04:31.986856000 +0100
+++ linux/arch/sh/kernel/vmlinux.lds.S	2007-08-30 12:39:33.715640000 +0100
@@ -39,6 +39,7 @@
 
   .data : {			/* Data */
 	*(.data)
+	EXTRA_RWDATA
 
  	 /* Align the initial ramdisk image (INITRD) on page boundaries. */
  	 . = ALIGN(4096);
Index: linux/arch/sh/mm/fault.c
===================================================================
--- linux.orig/arch/sh/mm/fault.c	2007-08-30 11:04:39.447888000 +0100
+++ linux/arch/sh/mm/fault.c	2007-08-30 12:39:33.721640000 +0100
@@ -32,6 +32,7 @@
 	struct vm_area_struct * vma;
 	unsigned long page;
 	int si_code;
+	int excep_code;
 	siginfo_t info;
 
 	/* Only enable interrupts if they were on before the fault */
@@ -75,6 +76,10 @@
 		return;
 	}
 
+        /* Get the execption code. */
+        __asm__ __volatile__("stc r2_bank, %0":"=r"(excep_code));
+        excep_code >>= 5;
+	MARK(kernel_arch_trap_entry, "%ld %ld", excep_code,instruction_pointer(regs));
 	/*
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
@@ -129,6 +134,7 @@
 	}
 
 	up_read(&mm->mmap_sem);
+	MARK(kernel_arch_trap_exit, MARK_NOARGS);
 	return;
 
 /*
@@ -145,6 +151,7 @@
  		info.si_code = si_code;
  		info.si_addr = (void *) address;
  		force_sig_info(SIGSEGV, &info, tsk);
+		MARK(kernel_arch_trap_exit, MARK_NOARGS);
 		return;
 	}
 
@@ -213,5 +220,7 @@
 	/* Kernel mode? Handle exceptions or die */
 	if (!user_mode(regs))
 		goto no_context;
+
+	MARK(kernel_arch_trap_exit, MARK_NOARGS);
 }
 
Index: linux/include/asm-sh/ltt.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/asm-sh/ltt.h	2007-08-30 12:39:33.728642000 +0100
@@ -0,0 +1,44 @@
+/*
+ * SH definitions for tracing system
+ * Author Giuseppe Cavallaro <peppe.cavallaro@st.com>
+ */
+
+#ifndef _ASM_SH_LTT_H
+#define _ASM_SH_LTT_H
+
+#include <linux/ltt-core.h>
+#include <asm/timer.h>
+#include <asm/clock.h>
+
+#define LTT_ARCH_TYPE LTT_ARCH_TYPE_SH
+#define LTT_ARCH_VARIANT LTT_ARCH_VARIANT_NONE
+
+#define LTT_HAS_TSC
+u64 ltt_heartbeat_read_synthetic_tsc(void);
+
+static inline u32 ltt_get_timestamp32(void)
+{
+	return get_cycles();
+}
+
+static inline u64 ltt_get_timestamp64(void)
+{
+	return (ltt_heartbeat_read_synthetic_tsc());
+}
+
+static inline unsigned int ltt_frequency(void)
+{
+	unsigned long rate;
+	struct clk *tmu1_clk;
+
+	tmu1_clk = clk_get("tmu1_clk");
+	rate = (clk_get_rate(tmu1_clk));
+
+	return (unsigned int)(rate);
+}
+
+static inline u32 ltt_freq_scale(void)
+{
+	return 1;
+}
+#endif
Index: linux/include/asm-sh/marker.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/asm-sh/marker.h	2007-08-30 12:39:33.734641000 +0100
@@ -0,0 +1,13 @@
+/*
+ * marker.h
+ *
+ * Code markup for dynamic and static tracing. Architecture specific
+ * optimisations.
+ *
+ * No optimisation implemented.
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ */
+
+#include <asm-generic/marker.h>
Index: linux/include/asm-sh/thread_info.h
===================================================================
--- linux.orig/include/asm-sh/thread_info.h	2007-08-30 11:05:54.454188000 +0100
+++ linux/include/asm-sh/thread_info.h	2007-08-30 12:39:33.737640000 +0100
@@ -108,6 +108,7 @@
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SINGLESTEP		4
+#define TIF_SYSCALL_AUDIT     6       /* syscall audit active */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_MEMDIE		18
@@ -116,6 +117,7 @@
 #define _TIF_NOTIFY_RESUME	(1<<TIF_NOTIFY_RESUME)
 #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
 #define _TIF_SINGLESTEP		(1<<TIF_SINGLESTEP)
+#define _TIF_SYSCALL_AUDIT    (1<<TIF_SYSCALL_AUDIT)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
Index: linux/include/asm-sh/timex.h
===================================================================
--- linux.orig/include/asm-sh/timex.h	2007-08-30 11:04:35.155872000 +0100
+++ linux/include/asm-sh/timex.h	2007-08-30 12:39:33.740641000 +0100
@@ -5,6 +5,10 @@
  */
 #ifndef __ASM_SH_TIMEX_H
 #define __ASM_SH_TIMEX_H
+#ifdef CONFIG_LTT
+#include <asm/cpu/timer.h>
+#include <asm/io.h>
+#endif
 
 #define CLOCK_TICK_RATE		(HZ * 100000UL)
 
@@ -12,6 +16,9 @@
 
 static __inline__ cycles_t get_cycles (void)
 {
+#ifdef CONFIG_LTT
+	return (0xffffffff - ctrl_inl(TMU1_TCNT));
+#endif
 	return 0;
 }
 
Index: linux/include/asm-sh/unistd.h
===================================================================
--- linux.orig/include/asm-sh/unistd.h	2007-08-30 11:04:35.169872000 +0100
+++ linux/include/asm-sh/unistd.h	2007-08-30 12:39:33.745640000 +0100
@@ -323,8 +323,10 @@
 #define __NR_sync_file_range	313
 #define __NR_tee		314
 #define __NR_vmsplice		315
+#define __NR_ltt_trace_generic	316
+#define __NR_ltt_register_generic	317
 
-#define NR_syscalls 316
+#define NR_syscalls 318
 
 /* user-visible error numbers are in the range -1 - -124: see <asm-sh/errno.h> */
 
Index: linux/include/asm-generic/marker.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/asm-generic/marker.h	2007-08-30 12:39:33.753640000 +0100
@@ -0,0 +1,37 @@
+#ifndef _ASM_GENERIC_MARKER_H
+#define _ASM_GENERIC_MARKER_H
+
+/*
+ * marker.h
+ *
+ * Code markup for dynamic and static tracing. Generic header.
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ *
+ * Note : the empty asm volatile with read constraint is used here instead of a
+ * "used" attribute to fix a gcc 4.1.x bug.
+ */
+
+/* Default flags, used by MARK() */
+#define MF_DEFAULT			(MF_LOCKDEP | MF_PRINTK)
+
+/* Fallback on the generic markers, since no optimized version is available */
+#define MARK_OPTIMIZED			MARK_GENERIC
+#define _MARK				MARK_GENERIC
+
+/* Marker with default behavior */
+#define MARK(format, args...)		_MARK(MF_DEFAULT, format, ## args)
+
+/* Architecture dependant marker information, used internally for marker
+ * activation. */
+
+#define MARK_OPTIMIZED_ENABLE_IMMEDIATE_OFFSET \
+		MARK_GENERIC_ENABLE_IMMEDIATE_OFFSET
+#define MARK_OPTIMIZED_ENABLE_TYPE	MARK_GENERIC_ENABLE_TYPE
+/* Dereference enable as lvalue from a pointer to its instruction */
+#define MARK_OPTIMIZED_ENABLE		MARK_GENERIC_ENABLE
+
+#define marker_optimized_set_enable marker_generic_set_enable
+
+#endif /* _ASM_GENERIC_MARKER_H */
Index: linux/include/asm-generic/ltt.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/asm-generic/ltt.h	2007-08-30 12:39:33.758641000 +0100
@@ -0,0 +1,12 @@
+#ifndef _ASM_GENERIC_LTT_H
+#define _ASM_GENERIC_LTT_H
+/*
+ * linux/include/asm-generic/ltt.h
+ *
+ * Copyright (C) 2005 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Architecture dependent definitions for ltt
+ * Architecture without TSC
+ */
+
+#endif
Index: linux/include/asm-generic/vmlinux.lds.h
===================================================================
--- linux.orig/include/asm-generic/vmlinux.lds.h	2006-06-18 02:49:35.000000000 +0100
+++ linux/include/asm-generic/vmlinux.lds.h	2007-08-30 12:39:33.761640000 +0100
@@ -85,11 +85,20 @@
 		*(__kcrctab_gpl_future)					\
 		VMLINUX_SYMBOL(__stop___kcrctab_gpl_future) = .;	\
 	}								\
-									\
-	/* Kernel symbol table: strings */				\
-        __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) {	\
-		*(__ksymtab_strings)					\
-	}								\
+        /* Kernel markers : pointers */                                 \
+        __markers : AT(ADDR(__markers) - LOAD_OFFSET) {                 \
+                VMLINUX_SYMBOL(__start___markers) = .;                  \
+                *(__markers)                                            \
+                VMLINUX_SYMBOL(__stop___markers) = .;                   \
+        }                                                               \
+        /* Kernel symbol table: strings */                              \
+        __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) { \
+                *(__ksymtab_strings)					\
+        }								\
+        /* Kernel markers : strings */					\
+        __markers_strings : AT(ADDR(__markers_strings) - LOAD_OFFSET) { \
+                *(__markers_strings)					\
+        }                                   				\
 	__end_rodata = .;						\
 	. = ALIGN(4096);						\
 									\
@@ -100,6 +109,10 @@
 		VMLINUX_SYMBOL(__stop___param) = .;			\
 	}
 
+#define EXTRA_RWDATA							\
+	. = ALIGN(8);							\
+	*(__markers_data)						\
+
 #define SECURITY_INIT							\
 	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
Index: linux/include/asm-generic/local.h
===================================================================
--- linux.orig/include/asm-generic/local.h	2006-06-18 02:49:35.000000000 +0100
+++ linux/include/asm-generic/local.h	2007-08-30 12:39:33.765640000 +0100
@@ -34,6 +34,19 @@
 #define local_add(i,l)	atomic_long_add((i),(&(l)->a))
 #define local_sub(i,l)	atomic_long_sub((i),(&(l)->a))
 
+#define local_sub_and_test(i, l) atomic_long_sub_and_test((i), (&(l)->a))
+#define local_dec_and_test(l) atomic_long_dec_and_test(&(l)->a)
+#define local_inc_and_test(l) atomic_long_inc_and_test(&(l)->a)
+#define local_add_negative(i, l) atomic_long_add_negative((i), (&(l)->a))
+#define local_add_return(i, l) atomic_long_add_return((i), (&(l)->a))
+#define local_sub_return(i, l) atomic_long_sub_return((i), (&(l)->a))
+#define local_inc_return(l) atomic_long_inc_return(&(l)->a)
+
+#define local_cmpxchg(l, o, n) atomic_long_cmpxchg((&(l)->a), (o), (n))
+#define local_xchg(l, n) atomic_long_xchg((&(l)->a), (n))
+#define local_add_unless(l, a, u) atomic_long_add_unless((&(l)->a), (a), (u))
+#define local_inc_not_zero(l) atomic_long_inc_not_zero(&(l)->a)
+
 /* Non-atomic variants, ie. preemption disabled and won't be touched
  * in interrupt, etc.  Some archs can optimize this case well. */
 #define __local_inc(l)		local_set((l), local_read(l) + 1)
@@ -45,19 +58,19 @@
  * much more efficient than these naive implementations.  Note they take
  * a variable (eg. mystruct.foo), not an address.
  */
-#define cpu_local_read(v)	local_read(&__get_cpu_var(v))
-#define cpu_local_set(v, i)	local_set(&__get_cpu_var(v), (i))
-#define cpu_local_inc(v)	local_inc(&__get_cpu_var(v))
-#define cpu_local_dec(v)	local_dec(&__get_cpu_var(v))
-#define cpu_local_add(i, v)	local_add((i), &__get_cpu_var(v))
-#define cpu_local_sub(i, v)	local_sub((i), &__get_cpu_var(v))
+#define cpu_local_read(l)	local_read(&__get_cpu_var(l))
+#define cpu_local_set(l, i)	local_set(&__get_cpu_var(l), (i))
+#define cpu_local_inc(l)	local_inc(&__get_cpu_var(l))
+#define cpu_local_dec(l)	local_dec(&__get_cpu_var(l))
+#define cpu_local_add(i, l)	local_add((i), &__get_cpu_var(l))
+#define cpu_local_sub(i, l)	local_sub((i), &__get_cpu_var(l))
 
 /* Non-atomic increments, ie. preemption disabled and won't be touched
  * in interrupt, etc.  Some archs can optimize this case well.
  */
-#define __cpu_local_inc(v)	__local_inc(&__get_cpu_var(v))
-#define __cpu_local_dec(v)	__local_dec(&__get_cpu_var(v))
-#define __cpu_local_add(i, v)	__local_add((i), &__get_cpu_var(v))
-#define __cpu_local_sub(i, v)	__local_sub((i), &__get_cpu_var(v))
+#define __cpu_local_inc(l)	__local_inc(&__get_cpu_var(l))
+#define __cpu_local_dec(l)	__local_dec(&__get_cpu_var(l))
+#define __cpu_local_add(i, l)	__local_add((i), &__get_cpu_var(l))
+#define __cpu_local_sub(i, l)	__local_sub((i), &__get_cpu_var(l))
 
 #endif /* _ASM_GENERIC_LOCAL_H */
Index: linux/include/asm-generic/atomic.h
===================================================================
--- linux.orig/include/asm-generic/atomic.h	2006-06-18 02:49:35.000000000 +0100
+++ linux/include/asm-generic/atomic.h	2007-08-30 12:39:33.769640000 +0100
@@ -66,7 +66,77 @@
 	atomic64_sub(i, v);
 }
 
-#else
+static inline int atomic_long_sub_and_test(long i, atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return atomic64_sub_and_test(i, v);
+}
+
+static inline int atomic_long_dec_and_test(atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return atomic64_dec_and_test(v);
+}
+
+static inline int atomic_long_inc_and_test(atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return atomic64_inc_and_test(v);
+}
+
+static inline int atomic_long_add_negative(long i, atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return atomic64_add_negative(i, v);
+}
+
+static inline long atomic_long_add_return(long i, atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return (long)atomic64_add_return(i, v);
+}
+
+static inline long atomic_long_sub_return(long i, atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return (long)atomic64_sub_return(i, v);
+}
+
+static inline long atomic_long_inc_return(atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return (long)atomic64_inc_return(v);
+}
+
+static inline long atomic_long_dec_return(atomic_long_t *l)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return (long)atomic64_dec_return(v);
+}
+
+static inline long atomic_long_add_unless(atomic_long_t *l, long a, long u)
+{
+	atomic64_t *v = (atomic64_t *)l;
+
+	return (long)atomic64_add_unless(v, a, u);
+}
+
+#define atomic_long_inc_not_zero(l) atomic64_inc_not_zero((atomic64_t *)(l))
+
+#define atomic_long_cmpxchg(l, old, new) \
+	(atomic_cmpxchg((atomic64_t *)(l), (old), (new)))
+#define atomic_long_xchg(v, new) \
+	(atomic_xchg((atomic64_t *)(l), (new)))
+
+#else  /*  BITS_PER_LONG == 64  */
 
 typedef atomic_t atomic_long_t;
 
@@ -113,5 +183,76 @@
 	atomic_sub(i, v);
 }
 
-#endif
-#endif
+static inline int atomic_long_sub_and_test(long i, atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return atomic_sub_and_test(i, v);
+}
+
+static inline int atomic_long_dec_and_test(atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return atomic_dec_and_test(v);
+}
+
+static inline int atomic_long_inc_and_test(atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return atomic_inc_and_test(v);
+}
+
+static inline int atomic_long_add_negative(long i, atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return atomic_add_negative(i, v);
+}
+
+static inline long atomic_long_add_return(long i, atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return (long)atomic_add_return(i, v);
+}
+
+static inline long atomic_long_sub_return(long i, atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return (long)atomic_sub_return(i, v);
+}
+
+static inline long atomic_long_inc_return(atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return (long)atomic_inc_return(v);
+}
+
+static inline long atomic_long_dec_return(atomic_long_t *l)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return (long)atomic_dec_return(v);
+}
+
+static inline long atomic_long_add_unless(atomic_long_t *l, long a, long u)
+{
+	atomic_t *v = (atomic_t *)l;
+
+	return (long)atomic_add_unless(v, a, u);
+}
+
+#define atomic_long_inc_not_zero(l) atomic_inc_not_zero((atomic_t *)(l))
+
+#define atomic_long_cmpxchg(l, old, new) \
+	(atomic_cmpxchg((atomic_t *)(l), (old), (new)))
+#define atomic_long_xchg(v, new) \
+	(atomic_xchg((atomic_t *)(l), (new)))
+
+#endif  /*  BITS_PER_LONG == 64  */
+
+#endif  /*  _ASM_GENERIC_ATOMIC_H  */
Index: linux/include/linux/kernel.h
===================================================================
--- linux.orig/include/linux/kernel.h	2006-06-18 02:49:35.000000000 +0100
+++ linux/include/linux/kernel.h	2007-08-30 12:39:33.774641000 +0100
@@ -13,6 +13,7 @@
 #include <linux/types.h>
 #include <linux/compiler.h>
 #include <linux/bitops.h>
+#include <linux/marker.h>
 #include <asm/byteorder.h>
 #include <asm/bug.h>
 
Index: linux/include/linux/ltt-core.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/linux/ltt-core.h	2007-08-30 12:39:33.780641000 +0100
@@ -0,0 +1,33 @@
+/*
+ * linux/include/linux/ltt-core.h
+ *
+ * Copyright (C) 2005,2006 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * This contains the core definitions for the Linux Trace Toolkit.
+ */
+
+#ifndef LTT_CORE_H
+#define LTT_CORE_H
+
+#include <linux/list.h>
+
+#ifdef CONFIG_LTT
+
+/* All modifications of ltt_traces must be done by ltt-tracer.c, while holding
+ * the semaphore. Only reading of this information can be done elsewhere, with
+ * the RCU mechanism : the preemption must be disabled while reading the
+ * list. */
+struct ltt_traces {
+	struct list_head head;		/* Traces list */
+	unsigned int num_active_traces;	/* Number of active traces */
+} ____cacheline_aligned;
+
+extern struct ltt_traces ltt_traces;
+
+
+/* Keep track of traps nesting inside LTT */
+extern volatile unsigned int ltt_nesting[];
+
+#endif //CONFIG_LTT
+
+#endif //LTT_CORE_H
Index: linux/include/linux/ltt-facilities.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/linux/ltt-facilities.h	2007-08-30 12:39:33.786641000 +0100
@@ -0,0 +1,111 @@
+/*
+ * linux/include/linux/ltt-facilities.h
+ *
+ * Copyright (C) 2005 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * This contains the definitions for the Linux Trace Toolkit facilities.
+ *
+ * Each facility must store its facility index number (returned by the register)
+ * in an exported symbol named :
+ *
+ * ltt_facility_name_checksum
+ *
+ * Where name is the name of the facility, and checksum is the text that
+ * corresponds to the checksum of the facility in hexadecimal notation.
+ */
+
+#ifndef _LTT_FACILITIES_H
+#define _LTT_FACILITIES_H
+
+#include <linux/types.h>
+#include <asm/atomic.h>
+
+#define FACNAME_LEN 32
+
+struct user_facility_info {
+	char name[FACNAME_LEN];
+	uint32_t num_events;
+	uint32_t alignment;
+	uint32_t checksum;
+	uint32_t int_size;
+	uint32_t long_size;
+	uint32_t pointer_size;
+	uint32_t size_t_size;
+};
+
+#ifdef __KERNEL__
+
+/* Is kernel tracing enabled */
+#if defined(CONFIG_LTT)
+
+#define LTT_FAC_PER_PROCESS 16
+
+#define LTT_MAX_NUM_FACILITIES	256
+#define LTT_RESERVED_FACILITIES	1
+
+enum ltt_facility_type {
+	LTT_FACILITY_TYPE_KERNEL,
+	LTT_FACILITY_TYPE_USER
+};
+
+struct ltt_facility {
+	const char *name;
+	const unsigned int num_events;
+	u32 checksum;
+	uint8_t id;
+	const char alignment;
+};
+
+struct ltt_facility_info {
+	char name[FACNAME_LEN];
+	enum ltt_facility_type type;
+	unsigned int num_events;
+	size_t alignment;
+	u32 checksum;
+	size_t int_size;
+	size_t long_size;
+	size_t pointer_size;
+	size_t size_t_size;
+	atomic_t ref;
+};
+
+struct ltt_trace_struct;
+
+void ltt_facility_ref(uint8_t facility_id);
+
+int ltt_facility_unregister(uint8_t facility_id);
+
+int ltt_facility_register(enum ltt_facility_type type,
+		const char *name,
+		const unsigned int num_events,
+		u32 checksum,
+		size_t int_size,
+		size_t long_size,
+		size_t pointer_size,
+		size_t size_t_size,
+		size_t alignment);
+
+int ltt_facility_verify(enum ltt_facility_type type,
+		const char *name,
+		const unsigned int num_events,
+		const u32 checksum,
+		size_t int_size,
+		size_t long_size,
+		size_t pointer_size,
+		size_t size_t_size,
+		size_t alignment);
+
+unsigned int ltt_facility_kernel_register(struct ltt_facility *facility);
+
+int ltt_facility_user_access_ok(uint8_t fac_id);
+
+void ltt_facility_free_unused(void);
+
+void ltt_facility_state_dump(struct ltt_trace_struct *trace);
+
+extern int ltt_compact_facility_num_events;
+
+#endif //__KERNEL__
+
+#endif /* defined(CONFIG_LTT) */
+#endif /* _LTT_FACILITIES_H */
Index: linux/include/linux/ltt-tracer.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/linux/ltt-tracer.h	2007-08-30 12:39:33.793640000 +0100
@@ -0,0 +1,714 @@
+/*
+ * include/ltt/ltt-tracer.h
+ *
+ * Copyright (C) 2005,2006 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * This contains the definitions for the Linux Trace Toolkit tracer.
+ */
+
+#ifndef _LTT_TRACER_H
+#define _LTT_TRACER_H
+
+#include <stdarg.h>
+#include <linux/types.h>
+#include <linux/limits.h>
+#include <linux/list.h>
+#include <linux/cache.h>
+#include <linux/kernel.h>
+#include <linux/timex.h>
+#include <linux/wait.h>
+#include <linux/relay.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-core.h>
+#include <linux/marker.h>
+#include <asm/semaphore.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+#include <asm/ltt.h>
+
+/* Number of bytes to log with a read/write event */
+#define LTT_LOG_RW_SIZE			32L
+
+/* Interval (in jiffies) at which the LTT per-CPU timer fires */
+#define LTT_PERCPU_TIMER_INTERVAL	1
+
+#ifdef CONFIG_LTT_ALIGNMENT
+
+/* Calculate the offset needed to align the type */
+static inline unsigned int ltt_align(size_t align_drift,
+		 size_t size_of_type)
+{
+	size_t alignment = min(sizeof(void*), size_of_type);
+	return ((alignment - align_drift) & (alignment-1));
+}
+/* Default arch alignment */
+#define LTT_ALIGN
+
+#else
+static inline unsigned int ltt_align(size_t align_drift,
+		 size_t size_of_type)
+{
+	return 0;
+}
+
+#define LTT_ALIGN __attribute__((packed))
+
+#endif //CONFIG_LTT_ALIGNMENT
+
+/* Maximum number of callbacks per marker */
+#define LTT_NR_CALLBACKS	10
+
+struct ltt_serialize_closure;
+
+/* Serialization callback '%k' */
+typedef char *(*ltt_serialize_cb)(char *buffer, char *str,
+			struct ltt_serialize_closure *closure,
+			int align,
+			const char *fmt, va_list *args);
+
+struct ltt_serialize_closure {
+	ltt_serialize_cb *callbacks;
+	long cb_args[LTT_NR_CALLBACKS];
+	unsigned int cb_idx;
+};
+
+char *ltt_serialize_data(char *buffer, char *str,
+			struct ltt_serialize_closure *closure,
+			int align,
+			const char *fmt, va_list *args);
+
+struct ltt_probe_data {
+	const char *name;
+	const char *format;
+	unsigned int channel_index;
+	int flags;
+	int align;
+	ltt_serialize_cb callbacks[LTT_NR_CALLBACKS];
+	marker_probe_func *probe_func;
+	uint8_t fID;
+	uint8_t eID;
+};
+
+/* LTT flags
+ *
+ * LF_TRACE : first arg contains trace to write into.
+ * 	      (type : struct ltt_trace_struct *)
+ * LF_CHANNEL : following arg contains channel index to write into.
+ *              (type : uint8_t)
+ * LF_COMPACT : Write to the compact channel.
+ * LF_COMPACT_DATA : One va args is taken to put (trucated) in the
+ *                   header. It is expected to be a u32
+ *                   and it should not be part of the format string.
+ * LF_FORCE : Force write in disabled traces (internal ltt use)
+ */
+
+#define _LF_BASE		_MF_NR
+#define LF_TRACE		(1 << (_LF_BASE+0))
+#define LF_CHANNEL		(1 << (_LF_BASE+1))
+#define LF_COMPACT		(1 << (_LF_BASE+2))
+#define LF_COMPACT_DATA		(1 << (_LF_BASE+3))
+#define LF_FORCE		(1 << (_LF_BASE+4))
+
+void ltt_vtrace(const struct __mark_marker_data *mdata,
+	const char *fmt, va_list args);
+void ltt_trace(const struct __mark_marker_data *mdata, const char *fmt, ...);
+
+void ltt_init_compact_facility(void);
+
+#ifdef CONFIG_LTT
+
+struct ltt_trace_struct;
+
+/* LTTng lockless logging buffer info */
+struct ltt_channel_buf_struct {
+	/* Use the relay void *start as buffer start pointer */
+	local_t offset;			/* Current offset in the buffer */
+	atomic_long_t consumed;		/* Current offset in the buffer
+					   standard atomic access (shared) */
+	atomic_long_t active_readers;	/* Active readers count
+					   standard atomic access (shared) */
+	atomic_t wakeup_readers;	/* Boolean : wakeup readers waiting ? */
+	local_t *commit_count;		/* Commit count per sub-buffer */
+	spinlock_t full_lock;		/* buffer full condition spinlock, only
+					 * for userspace tracing blocking mode
+					 * synchronisation with reader. */
+	local_t events_lost;
+	local_t corrupted_subbuffers;
+	struct timeval	current_subbuffer_start_time;
+	wait_queue_head_t write_wait;	/* Wait queue for blocking user space
+					 * writers */
+	struct work_struct wake_writers;/* Writers wake-up work struct */
+} ____cacheline_aligned;
+
+struct ltt_channel_struct {
+	char channel_name[PATH_MAX];
+	struct ltt_trace_struct	*trace;
+	struct ltt_channel_buf_struct buf[NR_CPUS];
+	int overwrite;
+	struct kref kref;
+	int compact;
+
+	void *trans_channel_data;
+
+	/*
+	 * buffer_begin - called on buffer-switch to a new sub-buffer
+	 * @buf: the channel buffer containing the new sub-buffer
+	 */
+	void (*buffer_begin) (struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx);
+	/*
+	 * buffer_end - called on buffer-switch to a new sub-buffer
+	 * @buf: the channel buffer containing the previous sub-buffer
+	 */
+	void (*buffer_end) (struct rchan_buf *buf,
+			u64 tsc, unsigned int offset, unsigned int subbuf_idx);
+};
+
+struct user_dbg_data {
+	unsigned long avail_size;
+	unsigned long write;
+	unsigned long read;
+};
+
+struct ltt_trace_ops {
+	int (*create_dirs) (struct ltt_trace_struct *new_trace);
+	void (*remove_dirs) (struct ltt_trace_struct *new_trace);
+	int (*create_channel) (char *trace_name, struct ltt_trace_struct *trace,
+				struct dentry *dir, char *channel_name,
+				struct ltt_channel_struct **ltt_chan,
+				unsigned int subbuf_size,
+				unsigned int n_subbufs, int overwrite);
+	void (*wakeup_channel) (struct ltt_channel_struct *ltt_channel);
+	void (*finish_channel) (struct ltt_channel_struct *channel);
+	void (*remove_channel) (struct ltt_channel_struct *channel);
+	void *(*reserve_slot) (struct ltt_trace_struct *trace,
+				struct ltt_channel_struct *channel,
+				void **transport_data, size_t data_size,
+				size_t *slot_size, u64 *tsc);
+	void (*commit_slot) (struct ltt_channel_struct *channel,
+				void **transport_data, void *reserved,
+				size_t slot_size);
+	int (*user_blocking) (struct ltt_trace_struct *trace,
+				unsigned int index, size_t data_size,
+				struct user_dbg_data *dbg);
+	void (*user_errors) (struct ltt_trace_struct *trace,
+				unsigned int index, size_t data_size,
+				struct user_dbg_data *dbg);
+#ifdef CONFIG_HOTPLUG_CPU
+	int (*handle_cpuhp) (struct notifier_block *nb,
+				unsigned long action, void *hcpu,
+				struct ltt_trace_struct *trace);
+#endif
+};
+
+struct ltt_transport {
+	char *name;
+	struct module *owner;
+	struct list_head node;
+	struct ltt_trace_ops ops;
+};
+
+
+enum trace_mode { LTT_TRACE_NORMAL, LTT_TRACE_FLIGHT, LTT_TRACE_HYBRID };
+
+/* Per-trace information - each trace/flight recorder represented by one */
+struct ltt_trace_struct {
+	struct list_head list;
+	int active;
+	char trace_name[NAME_MAX];
+	int paused;
+	enum trace_mode mode;
+	struct ltt_transport *transport;
+	struct ltt_trace_ops *ops;
+	struct kref ltt_transport_kref;
+	u32 freq_scale;
+	u64 start_freq;
+	u64 start_tsc;
+	unsigned long long start_monotonic;
+	struct timeval		start_time;
+	struct {
+		struct dentry			*trace_root;
+		struct dentry			*control_root;
+	} dentry;
+	struct {
+		struct ltt_channel_struct	*facilities;
+		struct ltt_channel_struct	*interrupts;
+		struct ltt_channel_struct	*processes;
+		struct ltt_channel_struct	*modules;
+		struct ltt_channel_struct	*network;
+		struct ltt_channel_struct	*cpu;
+		struct ltt_channel_struct	*compact;
+	} channel;
+	struct rchan_callbacks callbacks;
+	struct kref kref; /* Each channel has a kref of the trace struct */
+} ____cacheline_aligned;
+
+enum ltt_channels { LTT_CHANNEL_FACILITIES, LTT_CHANNEL_INTERRUPTS,
+	LTT_CHANNEL_PROCESSES, LTT_CHANNEL_MODULES, LTT_CHANNEL_CPU,
+	LTT_CHANNEL_COMPACT, LTT_CHANNEL_NETWORK };
+
+/* Hardcoded event headers */
+
+/* event header for a trace with active heartbeat : 32 bits timestamps */
+
+/* headers are 8 bytes aligned : that means members are aligned on memory
+ * boundaries *if* structure starts on a 8 bytes boundary. In order to insure
+ * such alignment, a dynamic per trace alignment value must be set.
+ *
+ * Remeber that the C compiler does align each member on the boundary equivalent
+ * to their own size.
+ *
+ * As relay subbuffers are aligned on pages, we are sure that they are 8 bytes
+ * aligned, so the buffer header and trace header are aligned.
+ *
+ * Event headers are aligned depending on the trace alignment option. */
+
+struct ltt_event_header_hb {
+	uint32_t timestamp;
+	unsigned char facility_id;
+	unsigned char event_id;
+	uint16_t event_size;
+} __attribute((packed));
+
+struct ltt_event_header_nohb {
+	uint64_t timestamp;
+	unsigned char facility_id;
+	unsigned char event_id;
+	uint16_t event_size;
+} __attribute((packed));
+
+struct ltt_event_header_compact {
+	uint32_t bitfield; /* E bits for event ID, 32-E bits for timestamp */
+} __attribute((packed));
+
+struct ltt_trace_header {
+	uint32_t magic_number;
+	uint32_t arch_type;
+	uint32_t arch_variant;
+	uint32_t float_word_order;	 /* Only useful for user space traces */
+	uint8_t arch_size;
+	uint8_t major_version;
+	uint8_t minor_version;
+	uint8_t flight_recorder;
+	uint8_t has_heartbeat;
+	uint8_t has_alignment;		/* Event header alignment */
+	uint8_t tsc_lsb_truncate;	/* LSB truncate for compact channel */
+	uint8_t tscbits;		/* TSC bits kept for compact channel */
+	uint32_t freq_scale;
+	uint64_t start_freq;
+	uint64_t start_tsc;
+	uint64_t start_monotonic;
+	uint64_t start_time_sec;
+	uint64_t start_time_usec;
+} __attribute((packed));
+
+
+/* We use asm/timex.h : cpu_khz/HZ variable in here : we might have to deal
+ * specifically with CPU frequency scaling someday, so using an interpolation
+ * between the start and end of buffer values is not flexible enough. Using an
+ * immediate frequency value permits to calculate directly the times for parts
+ * of a buffer that would be before a frequency change. */
+struct ltt_block_start_header {
+	struct {
+		uint64_t cycle_count;
+		uint64_t freq; /* khz */
+	} begin;
+	struct {
+		uint64_t cycle_count;
+		uint64_t freq; /* khz */
+	} end;
+	uint32_t lost_size;	/* Size unused at the end of the buffer */
+	uint32_t buf_size;	/* The size of this sub-buffer */
+	struct ltt_trace_header	trace;
+} __attribute((packed));
+
+/*
+ * ltt_subbuf_header_len - called on buffer-switch to a new sub-buffer
+ *
+ * returns the client header size at the beginning of the buffer.
+ */
+static inline unsigned int ltt_subbuf_header_len(void)
+{
+	return sizeof(struct ltt_block_start_header);
+}
+
+/* Get the offset of the channel in the ltt_trace_struct */
+#define GET_CHANNEL_INDEX(chan)	\
+	(unsigned int)&((struct ltt_trace_struct*)NULL)->channel.chan
+
+static inline struct ltt_channel_struct *ltt_get_channel_from_index(
+		struct ltt_trace_struct *trace, unsigned int index)
+{
+	return *(struct ltt_channel_struct **)((void*)trace+index);
+}
+
+
+/*
+ * ltt_get_header_size
+ *
+ * Calculate alignment offset for arch size void*. This is the
+ * alignment offset of the event header.
+ *
+ * Important note :
+ * The event header must be a size multiple of the void* size. This is necessary
+ * to be able to calculate statically the alignment offset of the variable
+ * length data fields that follows. The total offset calculated here :
+ *
+ *	 Alignment of header struct on arch size
+ * + sizeof(header struct)
+ * + padding added to end of struct to align on arch size.
+ * */
+static inline unsigned char ltt_get_header_size(
+		struct ltt_channel_struct *channel,
+		void *address,
+		size_t data_size,
+		size_t *before_hdr_pad)
+{
+	unsigned int padding;
+	unsigned int header;
+	size_t after_hdr_pad;
+
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	if (unlikely(channel->compact))
+		header = sizeof(struct ltt_event_header_compact);
+	else
+		header = sizeof(struct ltt_event_header_hb);
+#else //CONFIG_LTT_HEARTBEAT_EVENT
+	header = sizeof(struct ltt_event_header_nohb);
+#endif // CONFIG_LTT_HEARTBEAT_EVENT
+
+	/* Padding before the header. Calculated dynamically */
+	*before_hdr_pad = ltt_align((unsigned long)address, header);
+	padding = *before_hdr_pad;
+
+	/* Padding after header, considering header aligned on ltt_align.
+	 * Calculated statically if header size is known. For compact
+	 * channels, do not align the data. */
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	if (unlikely(channel->compact))
+		after_hdr_pad = 0;
+	else
+		after_hdr_pad = ltt_align(header, sizeof(void*));
+#else
+	after_hdr_pad = ltt_align(header, sizeof(void*));
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+	padding += after_hdr_pad;
+
+	return header+padding;
+}
+
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+extern int ltt_tsc_lsb_truncate;
+extern int ltt_tscbits;
+extern int ltt_compact_data_shift;
+
+static inline char *ltt_write_compact_header(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void *ptr, uint8_t fID,
+		uint32_t eID, size_t event_size,
+		u64 tsc, u32 data)
+{
+	struct ltt_event_header_compact *compact_hdr;
+	int tscbits = ltt_tscbits;
+	int lsb_truncate = ltt_tsc_lsb_truncate;
+	u32 compact_tsc;
+
+	compact_hdr = (struct ltt_event_header_compact *)ptr;
+	compact_tsc = ((u32)tsc >> lsb_truncate) & ((1 << tscbits) - 1);
+	compact_hdr->bitfield = (data << ltt_compact_data_shift)
+				| (eID << tscbits) | compact_tsc;
+	return ptr + sizeof(*compact_hdr);
+}
+#else //CONFIG_LTT_HEARTBEAT_EVENT
+static inline char *ltt_write_compact_header(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void *ptr, uint8_t fID,
+		uint32_t eID, size_t event_size,
+		u64 tsc, u32 data)
+{
+	return ptr;
+}
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+
+/* ltt_write_event_header
+ *
+ * Writes the event header to the pointer.
+ *
+ * @channel : pointer to the channel structure
+ * @ptr : buffer pointer
+ * @fID : facility ID
+ * @eID : event ID
+ * @event_size : size of the event, excluding the event header.
+ * @tsc : time stamp counter.
+ *
+ * returns : pointer where the event data must be written.
+ */
+static inline char *ltt_write_event_header(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void *ptr, uint8_t fID,
+		uint32_t eID, size_t event_size,
+		u64 tsc)
+{
+	size_t after_hdr_pad;
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	struct ltt_event_header_hb *hb;
+
+	event_size = min(event_size, (size_t)0xFFFFU);
+	hb = (struct ltt_event_header_hb *)ptr;
+	hb->timestamp = (u32)tsc;
+	hb->facility_id = fID;
+	hb->event_id = eID;
+	hb->event_size = (uint16_t)event_size;
+	if (unlikely(channel->compact))
+		after_hdr_pad = 0;
+	else
+		after_hdr_pad = ltt_align(sizeof(*hb), sizeof(void*));
+	return ptr + sizeof(*hb) + after_hdr_pad;
+#else
+	struct ltt_event_header_nohb *nohb;
+
+	event_size = min(event_size, (size_t)0xFFFFU);
+	nohb = (struct ltt_event_header_nohb *)ptr;
+	nohb->timestamp = (u64)tsc;
+	nohb->facility_id = fID;
+	nohb->event_id = eID;
+	nohb->event_size = (uint16_t)event_size;
+	after_hdr_pad = ltt_align(sizeof(*nohb), sizeof(void*));
+	return ptr + sizeof(*nohb) + after_hdr_pad;
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+
+}
+
+/* for flight recording. must be called after relay_commit.
+ * This function does not protect from corruption resulting from writing non
+ * sequentially in the buffer (and trying to read this buffer after a crash
+ * which occured at the wrong moment).
+ * That's why sequential writes are good!
+ *
+ * This function does nothing if trace is in normal mode. */
+#if 0
+static inline void ltt_write_commit_counter(struct rchan_buf *buf,
+		void *reserved)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct*)buf->chan->client_data;
+	struct ltt_block_start_header *header =
+		(struct ltt_block_start_header*)buf->data;
+	unsigned offset, subbuf_idx;
+
+	offset = reserved - buf->start;
+	subbuf_idx = offset / buf->chan->subbuf_size;
+
+	if (channel->trace->mode == LTT_TRACE_FLIGHT)
+		header->lost_size = buf->chan->subbuf_size -
+			buf->commit[subbuf_idx];
+
+}
+#endif //0
+
+/* Lockless LTTng */
+
+/* Buffer offset macros */
+
+#define BUFFER_OFFSET(offset, chan) ((offset) & (chan->alloc_size-1))
+#define SUBBUF_OFFSET(offset, chan) ((offset) & (chan->subbuf_size-1))
+#define SUBBUF_ALIGN(offset, chan) \
+	(((offset) + chan->subbuf_size) & (~(chan->subbuf_size-1)))
+#define SUBBUF_TRUNC(offset, chan) \
+	((offset) & (~(chan->subbuf_size-1)))
+#define SUBBUF_INDEX(offset, chan) \
+	(BUFFER_OFFSET((offset),chan)/chan->subbuf_size)
+
+/* ltt_reserve_slot
+ *
+ * Atomic slot reservation in a LTTng buffer. It will take care of
+ * sub-buffer switching.
+ *
+ * Parameters:
+ *
+ * @trace : the trace structure to log to.
+ * @buf : the buffer to reserve space into.
+ * @data_size : size of the variable length data to log.
+ * @slot_size : pointer to total size of the slot (out)
+ * @tsc : pointer to the tsc at the slot reservation (out)
+ *
+ * Return : NULL if not enough space, else returns the pointer
+ * 					to the beginning of the reserved slot. */
+static inline void *ltt_reserve_slot(
+		struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *channel,
+		void **transport_data,
+		size_t data_size,
+		size_t *slot_size,
+		u64 *tsc)
+{
+	return trace->ops->reserve_slot(trace, channel, transport_data,
+			data_size, slot_size, tsc);
+}
+
+
+/* ltt_commit_slot
+ *
+ * Atomic unordered slot commit. Increments the commit count in the
+ * specified sub-buffer, and delivers it if necessary.
+ *
+ * Parameters:
+ *
+ * @buf : the buffer to commit to.
+ * @reserved : address of the beginnig of the reserved slot.
+ * @slot_size : size of the reserved slot.
+ *
+ */
+static inline void ltt_commit_slot(
+		struct ltt_channel_struct *channel,
+		void **transport_data,
+		void *reserved,
+		size_t slot_size)
+{
+	struct ltt_trace_struct *trace = channel->trace;
+
+	trace->ops->commit_slot(channel, transport_data, reserved, slot_size);
+}
+
+/* 4 control channels :
+ * ltt/control/facilities
+ * ltt/control/interrupts
+ * ltt/control/processes
+ * ltt/control/network
+ *
+ * 2 cpu channels :
+ * ltt/cpu
+ * ltt/compact
+ */
+#define LTT_RELAY_ROOT		"ltt"
+#define LTT_CONTROL_ROOT	"control"
+#define LTT_FACILITIES_CHANNEL	"facilities"
+#define LTT_INTERRUPTS_CHANNEL	"interrupts"
+#define LTT_PROCESSES_CHANNEL	"processes"
+#define LTT_MODULES_CHANNEL	"modules"
+#define LTT_NETWORK_CHANNEL	"network"
+#define LTT_CPU_CHANNEL		"cpu"
+#define LTT_COMPACT_CHANNEL	"compact"
+#define LTT_FLIGHT_PREFIX	"flight-"
+
+/* System types */
+#define LTT_SYS_TYPE_VANILLA_LINUX	1
+
+/* Architecture types */
+#define LTT_ARCH_TYPE_I386		1
+#define LTT_ARCH_TYPE_PPC		2
+#define LTT_ARCH_TYPE_SH		3
+#define LTT_ARCH_TYPE_S390		4
+#define LTT_ARCH_TYPE_MIPS		5
+#define LTT_ARCH_TYPE_ARM		6
+#define LTT_ARCH_TYPE_PPC64		7
+#define LTT_ARCH_TYPE_X86_64		8
+#define LTT_ARCH_TYPE_C2		9
+#define LTT_ARCH_TYPE_POWERPC		10
+
+/* Standard definitions for variants */
+#define LTT_ARCH_VARIANT_NONE		0
+
+/* Tracer properties */
+#define LTT_DEFAULT_SUBBUF_SIZE_LOW	65536
+#define LTT_DEFAULT_N_SUBBUFS_LOW	2
+#define LTT_DEFAULT_SUBBUF_SIZE_MED	262144
+#define LTT_DEFAULT_N_SUBBUFS_MED	2
+#define LTT_DEFAULT_SUBBUF_SIZE_HIGH	1048576
+#define LTT_DEFAULT_N_SUBBUFS_HIGH	2
+#define LTT_TRACER_MAGIC_NUMBER		0x00D6B7ED
+#define LTT_TRACER_VERSION_MAJOR	0
+#define LTT_TRACER_VERSION_MINOR	8
+
+/* Size reserved for high priority events (interrupts, NMI, BH) at the end of a
+ * nearly full buffer. User space won't use this last amount of space when in
+ * blocking mode. This space also includes the event header that would be
+ * written by this user space event. */
+#define LTT_RESERVE_CRITICAL		4096
+
+/* Register and unregister function pointers */
+
+enum ltt_module_function {
+	LTT_FUNCTION_RUN_FILTER,
+	LTT_FUNCTION_FILTER_CONTROL,
+	LTT_FUNCTION_STATEDUMP
+};
+
+
+typedef int (*ltt_run_filter_functor)(struct ltt_trace_struct *trace,
+	uint8_t fID, uint8_t eID);
+
+extern ltt_run_filter_functor ltt_run_filter;
+
+extern int ltt_module_register(enum ltt_module_function name, void *function,
+		struct module *owner);
+extern void ltt_module_unregister(enum ltt_module_function name);
+
+void ltt_transport_register(struct ltt_transport *transport);
+void ltt_transport_unregister(struct ltt_transport *transport);
+
+/* Exported control function */
+
+enum ltt_heartbeat_functor_msg { LTT_HEARTBEAT_START, LTT_HEARTBEAT_STOP };
+
+enum ltt_control_msg {
+	LTT_CONTROL_START,
+	LTT_CONTROL_STOP,
+	LTT_CONTROL_CREATE_TRACE,
+	LTT_CONTROL_DESTROY_TRACE
+};
+
+union ltt_control_args {
+	struct {
+		enum trace_mode mode;
+		unsigned subbuf_size_low;
+		unsigned n_subbufs_low;
+		unsigned subbuf_size_med;
+		unsigned n_subbufs_med;
+		unsigned subbuf_size_high;
+		unsigned n_subbufs_high;
+	} new_trace;
+};
+
+extern int ltt_control(enum ltt_control_msg msg, char *trace_name,
+		char *trace_type, union ltt_control_args args);
+
+enum ltt_filter_control_msg {
+	LTT_FILTER_DEFAULT_ACCEPT,
+	LTT_FILTER_DEFAULT_REJECT };
+
+extern int ltt_filter_control(enum ltt_filter_control_msg msg,
+		char *trace_name);
+
+void ltt_write_trace_header(struct ltt_trace_struct *trace,
+		struct ltt_trace_header *header);
+extern void ltt_buffer_destroy(struct ltt_channel_struct *ltt_chan);
+extern void ltt_wakeup_writers(struct work_struct *work);
+
+void ltt_core_register(int (*function)(u8, void*));
+
+void ltt_core_unregister(void);
+
+void ltt_release_trace(struct kref *kref);
+void ltt_release_transport(struct kref *kref);
+
+void ltt_write_full_tsc(void);
+
+#ifdef CONFIG_LTT_HEARTBEAT
+int ltt_heartbeat_trigger(enum ltt_heartbeat_functor_msg msg);
+#endif //CONFIG_LTT_HEARTBEAT
+
+/* Relay IOCTL */
+
+/* Get the next sub buffer that can be read. */
+#define RELAY_GET_SUBBUF		_IOR(0xF5, 0x00,__u32)
+/* Release the oldest reserved (by "get") sub buffer. */
+#define RELAY_PUT_SUBBUF		_IOW(0xF5, 0x01,__u32)
+/* returns the number of sub buffers in the per cpu channel. */
+#define RELAY_GET_N_SUBBUFS		_IOR(0xF5, 0x02,__u32)
+/* returns the size of the sub buffers. */
+#define RELAY_GET_SUBBUF_SIZE		_IOR(0xF5, 0x03,__u32)
+
+#endif /* CONFIG_LTT */
+
+#endif /* _LTT_TRACER_H */
Index: linux/include/linux/marker.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/include/linux/marker.h	2007-08-30 12:39:33.799641000 +0100
@@ -0,0 +1,121 @@
+#ifndef _LINUX_MARKER_H
+#define _LINUX_MARKER_H
+
+/*
+ * marker.h
+ *
+ * Code markup for dynamic and static tracing.
+ *
+ * See Documentation/marker.txt.
+ *
+ * (C) Copyright 2006 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ */
+
+#ifdef __KERNEL__
+
+struct __mark_marker_data;
+
+typedef void marker_probe_func(const struct __mark_marker_data *mdata,
+	const char *fmt, ...);
+
+struct __mark_marker_data {
+	const char *name;
+	const char *format;
+	int flags;
+	marker_probe_func *call;
+	void *pdata;
+} __attribute__((packed));
+
+struct __mark_marker {
+	struct __mark_marker_data *mdata;
+	void *enable;
+} __attribute__((packed));
+
+#ifdef CONFIG_MARKERS
+
+/* Marker flags : selects the mechanism used to connect the probes to the
+ * markers and what can be executed within the probes. This is primarily
+ * used at reentrancy-unfriendly sites. */
+#define MF_OPTIMIZED	(1 << 0)	/* Use optimized markers */
+#define MF_LOCKDEP	(1 << 1)	/* Can call lockdep */
+#define MF_PRINTK	(1 << 2)	/* vprintk can be called in the probe */
+#define _MF_NR		3		/* Number of marker flags */
+
+#define DECLARE_MARKER_DATA(flags, name, format) \
+
+/* Generic marker flavor always available */
+#define MARK_GENERIC(flags, name, format, args...) \
+	do { \
+		static const char __mstrtab_name_##name[] \
+		__attribute__((section("__markers_strings"))) \
+		= #name; \
+		static const char __mstrtab_format_##name[] \
+		__attribute__((section("__markers_strings"))) \
+		= format; \
+		static struct __mark_marker_data __mark_data_##name \
+		__attribute__((section("__markers_data"))) = \
+		{ __mstrtab_name_##name,  __mstrtab_format_##name, \
+		(flags) & ~MF_OPTIMIZED, __mark_empty_function, NULL }; \
+		static char __marker_enable_##name = 0; \
+		static const struct __mark_marker __mark_##name \
+			__attribute__((section("__markers"))) = \
+			{ &__mark_data_##name, &__marker_enable_##name } ; \
+		asm volatile ( "" : : "i" (&__mark_##name)); \
+		__mark_check_format(format, ## args); \
+		if (unlikely(__marker_enable_##name)) { \
+			preempt_disable(); \
+			(*__mark_data_##name.call)(&__mark_data_##name, \
+						format, ## args); \
+			preempt_enable(); \
+		} \
+	} while (0)
+
+#define MARK_GENERIC_ENABLE_IMMEDIATE_OFFSET 0
+#define MARK_GENERIC_ENABLE_TYPE char
+/* Dereference enable as lvalue from a pointer to its instruction */
+#define MARK_GENERIC_ENABLE(a) \
+	*(MARK_GENERIC_ENABLE_TYPE*) \
+		((char*)a+MARK_GENERIC_ENABLE_IMMEDIATE_OFFSET)
+
+static inline int marker_generic_set_enable(void *address, char enable)
+{
+	MARK_GENERIC_ENABLE(address) = enable;
+	return 0;
+}
+
+#else /* !CONFIG_MARKERS */
+#define MARK_GENERIC(flags, name, format, args...) \
+		__mark_check_format(format, ## args)
+#endif /* CONFIG_MARKERS */
+
+#ifdef CONFIG_MARKERS_ENABLE_OPTIMIZATION
+#include <asm/marker.h>			/* optimized marker flavor */
+#else
+#include <asm-generic/marker.h>		/* fallback on generic markers */
+#endif
+
+#define MARK_MAX_FORMAT_LEN	1024
+/* Pass this as a format string for a marker with no argument */
+#define MARK_NOARGS " "
+
+/* To be used for string format validity checking with sparse */
+static inline
+void __mark_check_format(const char *fmt, ...)
+{ }
+
+extern marker_probe_func __mark_empty_function;
+
+extern int _marker_set_probe(int flags, const char *name, const char *format,
+				marker_probe_func *probe, void *pdata);
+
+#define marker_set_probe(name, format, probe, pdata) \
+	_marker_set_probe(MF_DEFAULT, name, format, probe, pdata)
+
+extern int marker_remove_probe(const char *name);
+extern int marker_list_probe(marker_probe_func *probe);
+
+#endif /* __KERNEL__ */
+#endif
Index: linux/include/linux/module.h
===================================================================
--- linux.orig/include/linux/module.h	2007-08-30 11:04:38.560885000 +0100
+++ linux/include/linux/module.h	2007-08-30 12:39:33.804640000 +0100
@@ -339,6 +339,9 @@
 	/* The command line arguments (may be mangled).  People like
 	   keeping pointers to this stuff */
 	char *args;
+
+	const struct __mark_marker *markers;
+	unsigned int num_markers;
 };
 
 /* FIXME: It'd be nice to isolate modules during init, too, so they
@@ -459,6 +462,7 @@
 int unregister_module_notifier(struct notifier_block * nb);
 
 extern void print_modules(void);
+extern void list_modules(void);
 
 struct device_driver;
 void module_add_driver(struct module *, struct device_driver *);
Index: linux/include/linux/netlink.h
===================================================================
--- linux.orig/include/linux/netlink.h	2006-06-18 02:49:35.000000000 +0100
+++ linux/include/linux/netlink.h	2007-08-30 12:39:33.808640000 +0100
@@ -21,6 +21,7 @@
 #define NETLINK_DNRTMSG		14	/* DECnet routing messages */
 #define NETLINK_KOBJECT_UEVENT	15	/* Kernel messages to userspace */
 #define NETLINK_GENERIC		16
+#define NETLINK_LTT           31      /* Linux Trace Toolkit FIXME */
 
 #define MAX_LINKS 32		
 
Index: linux/include/linux/relay.h
===================================================================
--- linux.orig/include/linux/relay.h	2006-06-18 02:49:35.000000000 +0100
+++ linux/include/linux/relay.h	2007-08-30 12:39:33.812641000 +0100
@@ -10,7 +10,6 @@
 #ifndef _LINUX_RELAY_H
 #define _LINUX_RELAY_H
 
-#include <linux/config.h>
 #include <linux/types.h>
 #include <linux/sched.h>
 #include <linux/wait.h>
@@ -25,7 +24,7 @@
 /*
  * Tracks changes to rchan/rchan_buf structs
  */
-#define RELAYFS_CHANNEL_VERSION		6
+#define RELAYFS_CHANNEL_VERSION		7
 
 /*
  * Per-cpu relay channel buffer
@@ -39,7 +38,8 @@
 	size_t subbufs_consumed;	/* count of sub-buffers consumed */
 	struct rchan *chan;		/* associated channel */
 	wait_queue_head_t read_wait;	/* reader wait queue */
-	struct work_struct wake_readers; /* reader wake-up work struct */
+//	struct delayed_work wake_readers; /* reader wake-up work struct */
+	struct work_struct wake_readers;
 	struct dentry *dentry;		/* channel file dentry */
 	struct kref kref;		/* channel buffer refcount */
 	struct page **page_array;	/* array of current buffer pages */
@@ -65,6 +65,10 @@
 	void *private_data;		/* for user-defined data */
 	size_t last_toobig;		/* tried to log event > subbuf size */
 	struct rchan_buf *buf[NR_CPUS]; /* per-cpu channel buffers */
+	int is_global;			/* One global buffer ? */
+	struct list_head list;		/* for channel list */
+	struct dentry *parent;		/* parent dentry passed to open */
+	char base_filename[NAME_MAX];	/* saved base filename */
 };
 
 /*
@@ -163,7 +167,8 @@
 			 struct dentry *parent,
 			 size_t subbuf_size,
 			 size_t n_subbufs,
-			 struct rchan_callbacks *cb);
+			 struct rchan_callbacks *cb,
+			 void *private_data);
 extern void relay_close(struct rchan *chan);
 extern void relay_flush(struct rchan *chan);
 extern void relay_subbufs_consumed(struct rchan *chan,
@@ -275,7 +280,7 @@
 /*
  * exported relay file operations, kernel/relay.c
  */
-extern struct file_operations relay_file_operations;
+extern const struct file_operations relay_file_operations;
 
 #endif /* _LINUX_RELAY_H */
 
Index: linux/include/linux/sched.h
===================================================================
--- linux.orig/include/linux/sched.h	2006-06-18 02:49:35.000000000 +0100
+++ linux/include/linux/sched.h	2007-08-30 12:39:33.821641000 +0100
@@ -110,6 +110,7 @@
 #include <linux/hrtimer.h>
 
 #include <asm/processor.h>
+#include <linux/ltt-facilities.h>
 
 /*
  * Task state bitmask. NOTE! These bits are also
@@ -888,6 +889,10 @@
 	 * cache last used pipe for splice
 	 */
 	struct pipe_inode_info *splice_pipe;
+#ifdef CONFIG_LTT_USERSPACE_GENERIC
+        uint8_t ltt_facilities[LTT_FAC_PER_PROCESS];
+#endif //CONFIG_LTT_USERSPACE_GENERIC
+
 };
 
 static inline pid_t process_group(struct task_struct *tsk)
Index: linux/Documentation/marker.txt
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/Documentation/marker.txt	2007-08-30 12:39:33.825640000 +0100
@@ -0,0 +1,266 @@
+ 	             Using the Linux Kernel Markers
+
+			    Mathieu Desnoyers
+
+
+	This document introduces to markers and discusses its purpose. It
+shows some usage examples of the Linux Kernel Markers : how to insert markers
+within the kernel and how to connect probes to a marker. Finally, it has some
+probe module examples. This is what connects to a marker.
+
+
+* Purpose of markers
+
+A marker placed in your code provides a hook to a function (probe) that
+you can provide at runtime. A marker can be "on" (a probe is connected to it)
+or "off" (no probe is attached). An "off" marker has no effect. When turned on,
+the function you provide is called each time the marker is executed in the
+execution context of the caller. When the function provided ends its execution,
+it returns to the caller (marker site).
+
+You can put markers at important locations in the code. They act as
+lightweight hooks that can pass an arbitrary number of parameters,
+described in a printk-like format string, to a function whenever the marker
+code is reached.
+
+They can be used for tracing (LTTng, LKET over SystemTAP), overall performance
+accounting (SystemTAP). They could also be used to implement
+efficient hooks for SELinux or any other subsystem that would have this
+kind of need.
+
+Using the markers for system audit (SELinux) would require to pass a
+variable by address that would be later checked by the marked routine.
+
+
+* Usage
+
+In order to use the macro MARK, you should include linux/marker.h.
+
+#include <linux/marker.h>
+
+Add, in your code :
+
+MARK(subsystem_event, "%d %s", someint, somestring);
+Where :
+- subsystem_event is an identifier unique to your event
+    - subsystem is the name of your subsystem.
+    - event is the name of the event to mark.
+- "%d %s" is the formatted string for the serializer.
+- someint is an integer.
+- somestring is a char pointer.
+
+Connecting a function (probe) to a marker is done by providing a probe
+(function to call) for the specific marker through marker_set_probe(). It will
+automatically connect the function and enable the marker site. Removing a probe
+is done through marker_remove_probe(). Probe removal is preempt safe because
+preemption is disabled around the probe call. See the "Probe example" section
+below for a sample probe module.
+
+The marker mechanism supports multiple instances of the same marker.
+Markers can be put in inline functions, inlined static functions and
+unrolled loops.
+
+Note : It is safe to put markers within preempt-safe code : preempt_enable()
+will not call the scheduler due to the tests in preempt_schedule().
+
+
+* Optimization for a given architecture
+
+You will find, in asm-*/marker.h, optimisations for given architectures
+(currently i386 and powerpc). They use a load immediate instead of a data load,
+which saves a data cache hit, but also requires cross CPU code modification. In
+order to support embedded systems which use read-only memory for their code, the
+optimization can be disabled through menu options.
+
+The MF_* flags can be used to control the type of marker. See the
+include/marker.h header for the list of flags. They can be specified as the
+first parameter of the _MARK() macro, such as the following example which is
+safe with respect to lockdep.c (useful for marking lockdep.c and printk
+functions).
+
+_MARK(MF_DEFAULT | ~MF_LOCKDEP, subsystem_eventb, MARK_NOARGS);
+
+Another example is to specify that a specific marker must never call printk :
+_MARK(MF_DEFAULT | ~MF_PRINTK, subsystem_eventc,
+  "%d %s", someint, somestring,);
+
+Flag compatibility is checked before connecting the probe to the marker : the
+right flags must be given to _marker_set_enable().
+
+
+* Probe example
+
+You can build the kernel modules, probe-example.ko and marker-example.ko,
+using the following Makefile:
+------------------------------ CUT -------------------------------------
+obj-m := probe-example.o marker-example.o
+KDIR := /lib/modules/$(shell uname -r)/build
+PWD := $(shell pwd)
+default:
+	$(MAKE) -C $(KDIR) SUBDIRS=$(PWD) modules
+clean:
+	rm -f *.mod.c *.ko *.o
+------------------------------ CUT -------------------------------------
+/* probe-example.c
+ *
+ * Connects two functions to marker call sites.
+ *
+ * (C) Copyright 2007 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ */
+
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/marker.h>
+#include <asm/atomic.h>
+
+#define NUM_PROBES ARRAY_SIZE(probe_array)
+
+struct probe_data {
+	const char *name;
+	const char *format;
+	marker_probe_func *probe_func;
+};
+
+void probe_subsystem_event(const struct __mark_marker_c *mdata,
+		const char *format, ...)
+{
+	va_list ap;
+	/* Declare args */
+	unsigned int value;
+	const char *mystr;
+
+	/* Assign args */
+	va_start(ap, format);
+	value = va_arg(ap, typeof(value));
+	mystr = va_arg(ap, typeof(mystr));
+
+	/* Call printk */
+	printk("Value %u, string %s\n", value, mystr);
+
+	/* or count, check rights, serialize data in a buffer */
+
+	va_end(ap);
+}
+
+atomic_t eventb_count = ATOMIC_INIT(0);
+
+void probe_subsystem_eventb(const struct __mark_marker_c *mdata,
+	const char *format, ...)
+{
+	/* Increment counter */
+	atomic_inc(&eventb_count);
+}
+
+static struct probe_data probe_array[] =
+{
+	{	.name = "subsystem_event",
+		.format = "%d %s",
+		.probe_func = probe_subsystem_event },
+	{	.name = "subsystem_eventb",
+		.format = MARK_NOARGS,
+		.probe_func = probe_subsystem_eventb },
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				probe_array[eID].probe_func, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "Unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	printk("Number of event b : %u\n", atomic_read(&eventb_count));
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("SUBSYSTEM Probe");
+------------------------------ CUT -------------------------------------
+/* marker-example.c
+ *
+ * Executes a marker when /proc/marker-example is opened.
+ *
+ * (C) Copyright 2007 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ */
+
+#include <linux/module.h>
+#include <linux/marker.h>
+#include <linux/sched.h>
+#include <linux/proc_fs.h>
+
+struct proc_dir_entry *pentry_example = NULL;
+
+static int my_open(struct inode *inode, struct file *file)
+{
+	int i;
+
+	MARK(subsystem_event, "%d %s", 123, "example string");
+	for (i=0; i<10; i++) {
+		MARK(subsystem_eventb, MARK_NOARGS);
+	}
+	return -EPERM;
+}
+
+static struct file_operations mark_ops = {
+	.open = my_open,
+};
+
+static int example_init(void)
+{
+	printk(KERN_ALERT "example init\n");
+	pentry_example = create_proc_entry("marker-example", 0444, NULL);
+	if (pentry_example)
+		pentry_example->proc_fops = &mark_ops;
+	else
+		return -EPERM;
+	return 0;
+}
+
+static void example_exit(void)
+{
+	printk(KERN_ALERT "example exit\n");
+	remove_proc_entry("marker-example", NULL);
+}
+
+module_init(example_init)
+module_exit(example_exit)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit example");
+------------------------------ CUT -------------------------------------
+Sequence of operations : (as root)
+make
+insmod marker-example.ko
+insmod probe-example.ko
+  (it is important to load the probe after the marked code)
+cat /proc/marker-example (returns an expected error)
+rmmod marker-example probe-example
+dmesg
+------------------------------ CUT -------------------------------------
Index: linux/Makefile
===================================================================
--- linux.orig/Makefile	2007-08-30 11:04:23.688824000 +0100
+++ linux/Makefile	2007-08-30 12:39:33.833640000 +0100
@@ -518,7 +518,7 @@
 
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/ ltt/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
Index: linux/fs/buffer.c
===================================================================
--- linux.orig/fs/buffer.c	2007-08-30 11:04:26.025832000 +0100
+++ linux/fs/buffer.c	2007-08-30 12:39:33.847640000 +0100
@@ -90,7 +90,9 @@
  */
 void __wait_on_buffer(struct buffer_head * bh)
 {
+	MARK(fs_buffer_wait_start, "%p", bh);
 	wait_on_bit(&bh->b_state, BH_Lock, sync_buffer, TASK_UNINTERRUPTIBLE);
+	MARK(fs_buffer_wait_end, "%p", bh);
 }
 
 static void
Index: linux/fs/compat.c
===================================================================
--- linux.orig/fs/compat.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/fs/compat.c	2007-08-30 12:39:33.856640000 +0100
@@ -46,6 +46,8 @@
 #include <linux/rwsem.h>
 #include <linux/acct.h>
 #include <linux/mm.h>
+#include <linux/ltt-core.h>
+#include <linux/ltt-facilities.h>
 
 #include <net/sock.h>		/* siocdevprivate_ioctl */
 
@@ -1555,6 +1557,18 @@
 
 	retval = search_binary_handler(bprm, regs);
 	if (retval >= 0) {
+#ifdef CONFIG_LTT_USERSPACE_GENERIC
+		{
+			int i;
+			for (i = 0; i < LTT_FAC_PER_PROCESS; i++) {
+				if (current->ltt_facilities[i] == 0)
+					break;
+				WARN_ON(ltt_facility_unregister(
+						current->ltt_facilities[i]));
+			}
+		}
+#endif //CONFIG_LTT_USERSPACE_GENERIC
+		MARK(fs_exec, "%s", filename);
 		free_arg_pages(bprm);
 
 		/* execve success */
Index: linux/fs/exec.c
===================================================================
--- linux.orig/fs/exec.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/fs/exec.c	2007-08-30 12:39:33.863645000 +0100
@@ -49,6 +49,7 @@
 #include <linux/rmap.h>
 #include <linux/acct.h>
 #include <linux/cn_proc.h>
+#include <linux/ltt-facilities.h>
 
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
@@ -1205,6 +1206,18 @@
 
 	retval = search_binary_handler(bprm,regs);
 	if (retval >= 0) {
+#ifdef CONFIG_LTT_USERSPACE_GENERIC
+		{
+			int i;
+			for (i = 0; i < LTT_FAC_PER_PROCESS; i++) {
+				if (current->ltt_facilities[i] == 0)
+					break;
+				WARN_ON(ltt_facility_unregister(
+						current->ltt_facilities[i]));
+			}
+		}
+#endif //CONFIG_LTT_USERSPACE_GENERIC
+		MARK(fs_exec, "%s", filename);
 		free_arg_pages(bprm);
 
 		/* execve success */
Index: linux/fs/ioctl.c
===================================================================
--- linux.orig/fs/ioctl.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/fs/ioctl.c	2007-08-30 12:39:33.867644000 +0100
@@ -167,6 +167,8 @@
 	if (!filp)
 		goto out;
 
+	MARK(fs_ioctl, "%u %u %lu", fd, cmd, arg);
+
 	error = security_file_ioctl(filp, cmd, arg);
 	if (error)
 		goto out_fput;
Index: linux/fs/open.c
===================================================================
--- linux.orig/fs/open.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/fs/open.c	2007-08-30 12:39:33.873644000 +0100
@@ -1091,6 +1091,7 @@
 				fsnotify_open(f->f_dentry);
 				fd_install(fd, f);
 			}
+			MARK(fs_open, "%d %s", fd, tmp);
 		}
 		putname(tmp);
 	}
@@ -1180,6 +1181,7 @@
 	filp = fdt->fd[fd];
 	if (!filp)
 		goto out_unlock;
+	MARK(fs_close, "%u", fd);
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	FD_CLR(fd, fdt->close_on_exec);
 	__put_unused_fd(files, fd);
Index: linux/fs/read_write.c
===================================================================
--- linux.orig/fs/read_write.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/fs/read_write.c	2007-08-30 12:39:33.878644000 +0100
@@ -15,6 +15,7 @@
 #include <linux/module.h>
 #include <linux/syscalls.h>
 #include <linux/pagemap.h>
+#include <linux/ltt-tracer.h> /* for LTT_LOG_RW_SIZE */
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -143,6 +144,9 @@
 		if (res != (loff_t)retval)
 			retval = -EOVERFLOW;	/* LFS: should only happen on 32 bit platforms */
 	}
+
+	MARK(fs_lseek, "%u %8b %u", fd, offset, origin);
+
 	fput_light(file, fput_needed);
 bad:
 	return retval;
@@ -170,6 +174,8 @@
 	offset = vfs_llseek(file, ((loff_t) offset_high << 32) | offset_low,
 			origin);
 
+	MARK(fs_llseek, "%u %8b %u", fd, offset, origin);
+
 	retval = (int)offset;
 	if (offset >= 0) {
 		retval = -EFAULT;
@@ -348,7 +354,10 @@
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		MARK(fs_read, "%u %zu", fd, count);
 		ret = vfs_read(file, buf, count, &pos);
+		MARK(fs_read_data, "%u %zd %k",
+			fd, ret, min(LTT_LOG_RW_SIZE, max(0L, (long)ret)), buf);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
 	}
@@ -366,7 +375,10 @@
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		MARK(fs_write, "%u %zu", fd, count);
 		ret = vfs_write(file, buf, count, &pos);
+		MARK(fs_write_data, "%u %zd %k",
+			fd, ret, min(LTT_LOG_RW_SIZE, max(0L, (long)ret)), buf);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
 	}
@@ -387,8 +399,11 @@
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		ret = -ESPIPE;
-		if (file->f_mode & FMODE_PREAD)
+		if (file->f_mode & FMODE_PREAD) {
+			MARK(fs_pread64, "%u %zu %8b", fd, count, pos);
 			ret = vfs_read(file, buf, count, &pos);
+		}
+
 		fput_light(file, fput_needed);
 	}
 
@@ -408,8 +423,11 @@
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		ret = -ESPIPE;
-		if (file->f_mode & FMODE_PWRITE)  
+ 		if (file->f_mode & FMODE_PWRITE) {
+			MARK(fs_pwrite64, "%u %zu %8b",
+				fd, count, pos);
 			ret = vfs_write(file, buf, count, &pos);
+		}
 		fput_light(file, fput_needed);
 	}
 
@@ -604,6 +622,7 @@
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		MARK(fs_readv, "%lu %lu", fd, vlen);
 		ret = vfs_readv(file, vec, vlen, &pos);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
@@ -625,6 +644,7 @@
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		loff_t pos = file_pos_read(file);
+		MARK(fs_writev, "%lu %lu", fd, vlen);
 		ret = vfs_writev(file, vec, vlen, &pos);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
Index: linux/fs/select.c
===================================================================
--- linux.orig/fs/select.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/fs/select.c	2007-08-30 12:39:33.883645000 +0100
@@ -239,6 +239,8 @@
 				file = fget_light(i, &fput_needed);
 				if (file) {
 					f_op = file->f_op;
+					MARK(fs_select, "%d %8b",
+							i, *timeout);
 					mask = DEFAULT_POLLMASK;
 					if (f_op && f_op->poll)
 						mask = (*f_op->poll)(file, retval ? NULL : wait);
@@ -564,6 +566,7 @@
 			struct file * file = fget_light(fd, &fput_needed);
 			mask = POLLNVAL;
 			if (file != NULL) {
+				MARK(fs_pollfd, "%d", fd);
 				mask = DEFAULT_POLLMASK;
 				if (file->f_op && file->f_op->poll)
 					mask = file->f_op->poll(file, *pwait);
Index: linux/ipc/msg.c
===================================================================
--- linux.orig/ipc/msg.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/ipc/msg.c	2007-08-30 12:39:33.890644000 +0100
@@ -237,6 +237,9 @@
 		msg_unlock(msq);
 	}
 	mutex_unlock(&msg_ids.mutex);
+
+	MARK(ipc_msg_create, "%d %d", ret, msgflg);
+
 	return ret;
 }
 
Index: linux/ipc/sem.c
===================================================================
--- linux.orig/ipc/sem.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/ipc/sem.c	2007-08-30 12:39:33.898644000 +0100
@@ -247,6 +247,7 @@
 	}
 
 	mutex_unlock(&sem_ids.mutex);
+	MARK(ipc_sem_create, "%d %d", err, semflg);
 	return err;
 }
 
Index: linux/ipc/shm.c
===================================================================
--- linux.orig/ipc/shm.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/ipc/shm.c	2007-08-30 12:39:33.903644000 +0100
@@ -302,6 +302,7 @@
 	}
 	mutex_unlock(&shm_ids.mutex);
 
+	MARK(ipc_shm_create, "%d %d", err, shmflg);
 	return err;
 }
 
Index: linux/mm/filemap.c
===================================================================
--- linux.orig/mm/filemap.c	2007-08-30 11:04:26.864836000 +0100
+++ linux/mm/filemap.c	2007-08-30 12:39:33.936644000 +0100
@@ -482,9 +482,13 @@
 {
 	DEFINE_WAIT_BIT(wait, &page->flags, bit_nr);
 
+	MARK(mm_filemap_wait_start, "%p", page_address(page));
+
 	if (test_bit(bit_nr, &page->flags))
 		__wait_on_bit(page_waitqueue(page), &wait, sync_page,
 							TASK_UNINTERRUPTIBLE);
+
+	MARK(mm_filemap_wait_end, "%p", page_address(page));
 }
 EXPORT_SYMBOL(wait_on_page_bit);
 
Index: linux/mm/memory.c
===================================================================
--- linux.orig/mm/memory.c	2007-08-30 11:04:35.348872000 +0100
+++ linux/mm/memory.c	2007-08-30 12:39:33.948644000 +0100
@@ -1884,6 +1884,7 @@
 again:
 	page = lookup_swap_cache(entry);
 	if (!page) {
+		MARK(mm_swap_in, "%lu", address);
  		swapin_readahead(entry, address, vma);
  		page = read_swap_cache_async(entry, vma, address);
 		if (!page) {
@@ -2258,30 +2259,44 @@
 int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 		unsigned long address, int write_access)
 {
+	int res;
 	pgd_t *pgd;
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
 
+	MARK(mm_handle_fault_entry, "%lu %ld", address, KSTK_EIP(current));
+
 	__set_current_state(TASK_RUNNING);
 
 	inc_page_state(pgfault);
 
-	if (unlikely(is_vm_hugetlb_page(vma)))
-		return hugetlb_fault(mm, vma, address, write_access);
-
-	pgd = pgd_offset(mm, address);
-	pud = pud_alloc(mm, pgd, address);
-	if (!pud)
-		return VM_FAULT_OOM;
-	pmd = pmd_alloc(mm, pud, address);
-	if (!pmd)
-		return VM_FAULT_OOM;
-	pte = pte_alloc_map(mm, pmd, address);
-	if (!pte)
-		return VM_FAULT_OOM;
-
-	return handle_pte_fault(mm, vma, address, pte, pmd, write_access);
+        if (unlikely(is_vm_hugetlb_page(vma))) {
+                res = hugetlb_fault(mm, vma, address, write_access);
+                goto end;
+        }
+
+        pgd = pgd_offset(mm, address);
+        pud = pud_alloc(mm, pgd, address);
+        if (!pud) {
+                res = VM_FAULT_OOM;
+                goto end;
+        }
+        pmd = pmd_alloc(mm, pud, address);
+        if (!pmd) {
+                res = VM_FAULT_OOM;
+                goto end;
+        }
+        pte = pte_alloc_map(mm, pmd, address);
+        if (!pte) {
+                res = VM_FAULT_OOM;
+                goto end;
+        }
+
+        res = handle_pte_fault(mm, vma, address, pte, pmd, write_access);
+end:
+        MARK(mm_handle_fault_exit, MARK_NOARGS);
+        return res;
 }
 
 EXPORT_SYMBOL_GPL(__handle_mm_fault);
Index: linux/mm/page_alloc.c
===================================================================
--- linux.orig/mm/page_alloc.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/mm/page_alloc.c	2007-08-30 12:39:33.959644000 +0100
@@ -443,6 +443,8 @@
 		mutex_debug_check_no_locks_freed(page_address(page),
 						 PAGE_SIZE<<order);
 
+	MARK(mm_page_free, "%u %p", order, page_address(page));
+
 	for (i = 0 ; i < (1 << order) ; ++i)
 		reserved += free_pages_check(page + i);
 	if (reserved)
@@ -1087,6 +1089,7 @@
 	page = alloc_pages(gfp_mask, order);
 	if (!page)
 		return 0;
+	MARK(mm_page_alloc, "%u %p", order, page_address(page));
 	return (unsigned long) page_address(page);
 }
 
Index: linux/mm/page_io.c
===================================================================
--- linux.orig/mm/page_io.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/mm/page_io.c	2007-08-30 12:39:33.963645000 +0100
@@ -103,6 +103,7 @@
 		rw |= (1 << BIO_RW_SYNC);
 	inc_page_state(pswpout);
 	set_page_writeback(page);
+	MARK(mm_swap_out, "%p", page_address(page));
 	unlock_page(page);
 	submit_bio(rw, bio);
 out:
Index: linux/net/core/dev.c
===================================================================
--- linux.orig/net/core/dev.c	2007-08-30 11:04:26.990836000 +0100
+++ linux/net/core/dev.c	2007-08-30 12:39:33.977644000 +0100
@@ -1344,6 +1344,8 @@
 	      	if (skb_checksum_help(skb, 0))
 	      		goto out_kfree_skb;
 
+	MARK(net_dev_xmit, "%p %2b", skb, skb->protocol);
+
 	spin_lock_prefetch(&dev->queue_lock);
 
 	/* Disable soft irqs for various locks below. Also 
@@ -1693,6 +1695,8 @@
 
 	__get_cpu_var(netdev_rx_stat).total++;
 
+	MARK(net_dev_receive, "%p %2b", skb, skb->protocol);
+
 	skb->h.raw = skb->nh.raw = skb->data;
 	skb->mac_len = skb->nh.raw - skb->mac.raw;
 
Index: linux/net/ipv4/devinet.c
===================================================================
--- linux.orig/net/ipv4/devinet.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/net/ipv4/devinet.c	2007-08-30 12:39:33.986644000 +0100
@@ -251,6 +251,7 @@
 		struct in_ifaddr **ifap1 = &ifa1->ifa_next;
 
 		while ((ifa = *ifap1) != NULL) {
+			MARK(net_del_ifa, "%s", ifa->ifa_label);
 			if (!(ifa->ifa_flags & IFA_F_SECONDARY) && 
 			    ifa1->ifa_scope <= ifa->ifa_scope)
 				last_prim = ifa;
@@ -354,6 +355,8 @@
 			}
 			ifa->ifa_flags |= IFA_F_SECONDARY;
 		}
+		MARK(net_insert_ifa, "%s %4b", ifa->ifa_label,
+			ifa->ifa_address);
 	}
 
 	if (!(ifa->ifa_flags & IFA_F_SECONDARY)) {
Index: linux/net/socket.c
===================================================================
--- linux.orig/net/socket.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/net/socket.c	2007-08-30 12:39:33.994644000 +0100
@@ -602,6 +602,10 @@
 	struct sock_iocb siocb;
 	int ret;
 
+	MARK(net_socket_sendmsg, "%p %d %d %d %zu",
+		sock, sock->sk->sk_family, sock->sk->sk_type,
+		sock->sk->sk_protocol, size);
+
 	init_sync_kiocb(&iocb, NULL);
 	iocb.private = &siocb;
 	ret = __sock_sendmsg(&iocb, sock, msg, size);
@@ -654,6 +658,10 @@
 	struct sock_iocb siocb;
 	int ret;
 
+        MARK(net_socket_recvmsg, "%p %d %d %d %zu",
+                sock, sock->sk->sk_family, sock->sk->sk_type,
+                sock->sk->sk_protocol, size);
+
         init_sync_kiocb(&iocb, NULL);
 	iocb.private = &siocb;
 	ret = __sock_recvmsg(&iocb, sock, msg, size, flags);
@@ -805,7 +813,8 @@
 	struct sock_iocb siocb;
 	int ret;
 
-	init_sync_kiocb(&iocb, NULL);
+        init_sync_kiocb(&iocb, NULL);
+
 	iocb.private = &siocb;
 
 	ret = do_sock_write(&msg, &iocb, file, (struct iovec *)iov, nr_segs);
@@ -1248,6 +1257,10 @@
 	if (retval < 0)
 		goto out_release;
 
+	MARK(net_socket_create, "%p %d %d %d %d",
+		sock, sock->sk->sk_family, sock->sk->sk_type,
+		sock->sk->sk_protocol, retval);
+
 out:
 	/* It may be already another descriptor 8) Not kernel problem. */
 	return retval;
@@ -1986,6 +1999,8 @@
 
 	a0=a[0];
 	a1=a[1];
+
+	MARK(net_socket_call, "%d %lu", call, a0);
 	
 	switch(call) 
 	{
Index: linux/kernel/Kconfig.marker
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/kernel/Kconfig.marker	2007-08-30 12:39:34.002645000 +0100
@@ -0,0 +1,20 @@
+# Code markers configuration
+
+config MARKERS
+	bool "Activate markers"
+	depends on MODULES
+	help
+	  Place an empty function call at each marker site. Can be
+	  dynamically changed for a probe function.
+
+config MARKERS_DISABLE_OPTIMIZATION
+	bool "Disable marker optimization"
+	depends on MARKERS && EMBEDDED
+	default n
+	help
+	  Disable code replacement jump optimisations. Especially useful if your
+	  code is in a read-only rom/flash.
+
+config MARKERS_ENABLE_OPTIMIZATION
+	def_bool y
+	depends on MARKERS && !MARKERS_DISABLE_OPTIMIZATION
Index: linux/kernel/exit.c
===================================================================
--- linux.orig/kernel/exit.c	2007-08-30 11:04:26.725837000 +0100
+++ linux/kernel/exit.c	2007-08-30 12:39:34.009645000 +0100
@@ -36,6 +36,7 @@
 #include <linux/compat.h>
 #include <linux/pipe_fs_i.h>
 #include <linux/audit.h> /* for audit_free() */
+#include <linux/ltt-facilities.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -59,6 +60,17 @@
 
 		list_del_rcu(&p->tasks);
 		__get_cpu_var(process_counts)--;
+#ifdef CONFIG_LTT_USERSPACE_GENERIC
+		{
+			int i;
+			for (i = 0; i < LTT_FAC_PER_PROCESS; i++) {
+				if (p->ltt_facilities[i] == 0)
+					break;
+				WARN_ON(ltt_facility_unregister(
+							p->ltt_facilities[i]));
+			}
+		}
+#endif //CONFIG_LTT_USERSPACE_GENERIC
 	}
 	list_del_rcu(&p->thread_group);
 	remove_parent(p);
@@ -175,6 +187,7 @@
 	spin_unlock(&p->proc_lock);
 	proc_pid_flush(proc_dentry);
 	release_thread(p);
+	MARK(kernel_process_free, "%d", p->pid);
 	call_rcu(&p->rcu, delayed_put_task_struct);
 
 	p = leader;
@@ -907,6 +920,8 @@
 		audit_free(tsk);
 	exit_mm(tsk);
 
+	MARK(kernel_process_exit, "%d", tsk->pid);
+
 	exit_sem(tsk);
 	__exit_files(tsk);
 	__exit_fs(tsk);
@@ -1425,6 +1440,8 @@
 	struct task_struct *tsk;
 	int flag, retval;
 
+	MARK(kernel_process_wait, "%d", pid);
+
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
 	/*
Index: linux/kernel/extable.c
===================================================================
--- linux.orig/kernel/extable.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/extable.c	2007-08-30 12:39:34.013645000 +0100
@@ -51,6 +51,7 @@
 		return 1;
 	return 0;
 }
+EXPORT_SYMBOL_GPL(core_kernel_text);
 
 int __kernel_text_address(unsigned long addr)
 {
@@ -58,6 +59,7 @@
 		return 1;
 	return __module_text_address(addr) != NULL;
 }
+EXPORT_SYMBOL_GPL(__kernel_text_address);
 
 int kernel_text_address(unsigned long addr)
 {
@@ -65,3 +67,4 @@
 		return 1;
 	return module_text_address(addr) != NULL;
 }
+EXPORT_SYMBOL_GPL(kernel_text_address);
Index: linux/kernel/fork.c
===================================================================
--- linux.orig/kernel/fork.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/fork.c	2007-08-30 12:39:34.020644000 +0100
@@ -44,6 +44,7 @@
 #include <linux/rmap.h>
 #include <linux/acct.h>
 #include <linux/cn_proc.h>
+#include <linux/ltt-facilities.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1086,6 +1087,13 @@
 	 * of CLONE_PTRACE.
 	 */
 	clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);
+#ifdef CONFIG_MARKERS
+	/*
+	 * Syscall tracing must always be turned on when markers are enabled.
+	 * Use the syscall audit thread flag for now, as it is never cleared.
+	 */
+	set_tsk_thread_flag(p, TIF_SYSCALL_AUDIT);
+#endif
 #ifdef TIF_SYSCALL_EMU
 	clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);
 #endif
@@ -1095,6 +1103,20 @@
 	   
 	p->parent_exec_id = p->self_exec_id;
 
+#ifdef CONFIG_LTT_USERSPACE_GENERIC
+	if (clone_flags & CLONE_THREAD)
+		memset(p->ltt_facilities, 0, sizeof(p->ltt_facilities));
+	else {
+		int i;
+		for (i = 0; i < LTT_FAC_PER_PROCESS; i++) {
+			p->ltt_facilities[i] = current->ltt_facilities[i];
+			if (p->ltt_facilities[i] != 0)
+				ltt_facility_ref(p->ltt_facilities[i]);
+		}
+	}
+
+#endif //CONFIG_LTT_USERSPACE_GENERIC
+
 	/* ok, now we should be set up.. */
 	p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
 	p->pdeath_signal = 0;
@@ -1151,7 +1173,7 @@
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
-		goto bad_fork_cleanup_namespace;
+		goto bad_fork_cleanup_ltt_facilities;
 	}
 
 	if (clone_flags & CLONE_THREAD) {
@@ -1216,6 +1238,19 @@
 	proc_fork_connector(p);
 	return p;
 
+bad_fork_cleanup_ltt_facilities:
+#ifdef CONFIG_LTT_USERSPACE_GENERIC
+                {
+                        int i;
+                        for (i = 0; i < LTT_FAC_PER_PROCESS; i++) {
+                                if (p->ltt_facilities[i] == 0)
+                                        break;
+                                WARN_ON(ltt_facility_unregister(
+                                                        p->ltt_facilities[i]));
+                        }
+                }
+#endif //CONFIG_LTT_USERSPACE_GENERIC
+
 bad_fork_cleanup_namespace:
 	exit_namespace(p);
 bad_fork_cleanup_keys:
@@ -1328,6 +1363,9 @@
 	if (!IS_ERR(p)) {
 		struct completion vfork;
 
+		MARK(kernel_process_fork, "%d %d %d",
+			current->pid, p->pid, p->tgid);
+
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
 			init_completion(&vfork);
Index: linux/kernel/irq/handle.c
===================================================================
--- linux.orig/kernel/irq/handle.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/irq/handle.c	2007-08-30 12:39:34.025644000 +0100
@@ -65,6 +65,8 @@
 	.set_affinity = NULL
 };
 
+EXPORT_SYMBOL(irq_desc);
+
 /*
  * Special, empty irq handler:
  */
@@ -81,6 +83,12 @@
 {
 	int ret, retval = 0, status = 0;
 
+	MARK(kernel_irq_entry, "%u %u", irq, (regs)?(!user_mode(regs)):(1));
+        MARK(stack_arch_irq_dump_kernel_stack, MARK_NOARGS, regs);
+        MARK(stack_arch_irq_dump_process32_stack, MARK_NOARGS, regs);
+        MARK(stack_arch_irq_dump_process64_stack, MARK_NOARGS, regs);
+
+
 	if (!(action->flags & SA_INTERRUPT))
 		local_irq_enable();
 
@@ -96,6 +104,8 @@
 		add_interrupt_randomness(irq);
 	local_irq_disable();
 
+	MARK(kernel_irq_exit, MARK_NOARGS);
+
 	return retval;
 }
 
Index: linux/kernel/itimer.c
===================================================================
--- linux.orig/kernel/itimer.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/itimer.c	2007-08-30 12:39:34.029644000 +0100
@@ -133,6 +133,8 @@
 	struct signal_struct *sig =
 	    container_of(timer, struct signal_struct, real_timer);
 
+	MARK(kernel_timer_itimer_expired, "%d", sig->tsk->pid);
+
 	send_group_sig_info(SIGALRM, SEND_SIG_PRIV, sig->tsk);
 
 	if (sig->it_real_incr.tv64 != 0) {
@@ -216,6 +218,15 @@
 	 */
 	check_itimerval(value);
 
+	MARK(kernel_timer_itimer_set, "%d %*.*r %*.*r",
+			which,
+			sizeof(value->it_interval),
+			__alignof__(value->it_interval),
+			&value->it_interval,
+			sizeof(value->it_value),
+			__alignof__(value->it_value),
+			&value->it_value);
+
 	switch (which) {
 	case ITIMER_REAL:
 again:
Index: linux/kernel/kthread.c
===================================================================
--- linux.orig/kernel/kthread.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/kthread.c	2007-08-30 12:39:34.033644000 +0100
@@ -181,6 +181,8 @@
 	/* It could exit after stop_info.k set, but before wake_up_process. */
 	get_task_struct(k);
 
+	MARK(kernel_kthread_stop, "%d", k->pid);
+
 	/* Must init completion *before* thread sees kthread_stop_info.k */
 	init_completion(&kthread_stop_info.done);
 	smp_wmb();
@@ -199,6 +201,8 @@
 	ret = kthread_stop_info.err;
 	mutex_unlock(&kthread_stop_lock);
 
+	MARK(kernel_kthread_stop_ret, "%d", ret);
+
 	return ret;
 }
 EXPORT_SYMBOL(kthread_stop_sem);
Index: linux/kernel/module.c
===================================================================
--- linux.orig/kernel/module.c	2007-08-30 11:04:38.622884000 +0100
+++ linux/kernel/module.c	2007-08-30 12:39:34.044644000 +0100
@@ -32,6 +32,7 @@
 #include <linux/cpu.h>
 #include <linux/moduleparam.h>
 #include <linux/errno.h>
+#include <linux/marker.h>
 #include <linux/err.h>
 #include <linux/vermagic.h>
 #include <linux/notifier.h>
@@ -124,6 +125,8 @@
 extern const unsigned long __start___kcrctab[];
 extern const unsigned long __start___kcrctab_gpl[];
 extern const unsigned long __start___kcrctab_gpl_future[];
+extern const struct __mark_marker __start___markers[];
+extern const struct __mark_marker __stop___markers[];
 
 #ifndef CONFIG_MODVERSIONS
 #define symversion(base, idx) NULL
@@ -237,6 +240,225 @@
 	return NULL;
 }
 
+#ifdef CONFIG_MARKERS
+
+/* Empty callback provided as a probe to the markers. By providing this to a
+ * disabled marker, we makes sure the  execution flow is always valid even
+ * though the function pointer change and the marker enabling are two distinct
+ * operations that modifies the execution flow of preemptible code. */
+void __mark_empty_function(const struct __mark_marker_data *mdata,
+	const char *fmt, ...)
+{
+}
+EXPORT_SYMBOL_GPL(__mark_empty_function);
+
+/* Set the enable bit of the marker, choosing the generic or architecture
+ * specific functions depending on the marker's flags.
+ */
+static int marker_set_enable(void *address, char enable, int flags)
+{
+	if (flags & MF_OPTIMIZED)
+		return marker_optimized_set_enable(address, enable);
+	else
+		return marker_generic_set_enable(address, enable);
+}
+
+/* Sets the probe callback and enables the markers corresponding to a range of
+ * markers. The enable bit and function address are set out of order, and it's
+ * ok : the state is always coherent because of the empty callback we provide.
+ */
+static int _marker_set_probe_range(int flags, const char *name,
+	const char *format,
+	marker_probe_func *probe,
+	void *pdata,
+	const struct __mark_marker *begin,
+	const struct __mark_marker *end)
+{
+	const struct __mark_marker *iter;
+	int found = 0;
+
+	for (iter = begin; iter < end; iter++) {
+		if (strcmp(name, iter->mdata->name) == 0) {
+			if (format
+				&& strcmp(format, iter->mdata->format) != 0) {
+				printk(KERN_NOTICE
+					"Format mismatch for probe %s "
+					"(%s), marker (%s)\n",
+					name,
+					format,
+					iter->mdata->format);
+				continue;
+			}
+			if (flags & MF_LOCKDEP
+				&& !(iter->mdata->flags & MF_LOCKDEP)) {
+					printk(KERN_NOTICE
+					"Incompatible lockdep flags for "
+					"probe %s\n",
+					name);
+					continue;
+			}
+			if (flags & MF_PRINTK
+				&& !(iter->mdata->flags & MF_PRINTK)) {
+					printk(KERN_NOTICE
+					"Incompatible printk flags for "
+					"probe %s\n",
+					name);
+					continue;
+			}
+			if (probe == __mark_empty_function) {
+				if (iter->mdata->call
+					!= __mark_empty_function) {
+					iter->mdata->call =
+						__mark_empty_function;
+				}
+				marker_set_enable(iter->enable, 0,
+					iter->mdata->flags);
+			} else {
+				if (iter->mdata->call
+					!= __mark_empty_function) {
+					if (iter->mdata->call != probe) {
+						printk(KERN_NOTICE
+							"Marker %s busy, "
+							"probe %p already "
+							"installed\n",
+							name,
+							iter->mdata->call);
+						continue;
+					}
+				} else {
+					found++;
+					iter->mdata->call = probe;
+				}
+				iter->mdata->pdata = pdata;
+				smp_wmb();
+				marker_set_enable(iter->enable, 1,
+					iter->mdata->flags);
+			}
+			found++;
+		}
+	}
+	return found;
+}
+
+/* Sets a range of markers to a disabled state : unset the enable bit and
+ * provide the empty callback. */
+static int marker_remove_probe_range(const char *name,
+	const struct __mark_marker *begin,
+	const struct __mark_marker *end)
+{
+	const struct __mark_marker *iter;
+	int found = 0;
+
+	for (iter = begin; iter < end; iter++) {
+		if (strcmp(name, iter->mdata->name) == 0) {
+			marker_set_enable(iter->enable, 0,
+				iter->mdata->flags);
+			iter->mdata->call = __mark_empty_function;
+			found++;
+		}
+	}
+	return found;
+}
+
+/* Provides a listing of the markers present in the kernel with their type
+ * (optimized or generic), state (enabled or disabled), callback and format
+ * string. */
+static int marker_list_probe_range(marker_probe_func *probe,
+	const struct __mark_marker *begin,
+	const struct __mark_marker *end)
+{
+	const struct __mark_marker *iter;
+	int found = 0;
+
+	for (iter = begin; iter < end; iter++) {
+		if (probe)
+			if (probe != iter->mdata->call) continue;
+		printk("name %s \n", iter->mdata->name);
+		if (iter->mdata->flags & MF_OPTIMIZED)
+			printk("  enable %u optimized ",
+				MARK_OPTIMIZED_ENABLE(iter->enable));
+		else
+			printk("  enable %u generic ",
+				MARK_GENERIC_ENABLE(iter->enable));
+		printk("  func 0x%p format \"%s\"\n",
+			iter->mdata->call, iter->mdata->format);
+		found++;
+	}
+	return found;
+}
+
+/* Calls _marker_set_probe_range for the core markers and modules markers.
+ * Marker enabling/disabling use the modlist_lock to synchronise. */
+int _marker_set_probe(int flags, const char *name, const char *format,
+				marker_probe_func *probe,
+				void *pdata)
+{
+	struct module *mod;
+	int found = 0;
+
+	mutex_lock(&module_mutex);
+	/* Core kernel markers */
+	found += _marker_set_probe_range(flags, name, format, probe,
+			pdata,
+			__start___markers, __stop___markers);
+	/* Markers in modules. */
+	list_for_each_entry(mod, &modules, list) {
+		found += _marker_set_probe_range(flags, name, format,
+				probe, pdata,
+				mod->markers, mod->markers+mod->num_markers);
+	}
+	mutex_unlock(&module_mutex);
+	return found;
+}
+EXPORT_SYMBOL_GPL(_marker_set_probe);
+
+/* Calls _marker_remove_probe_range for the core markers and modules markers.
+ * Marker enabling/disabling use the modlist_lock to synchronise. */
+int marker_remove_probe(const char *name)
+{
+	struct module *mod;
+	int found = 0;
+
+	mutex_lock(&module_mutex);
+	/* Core kernel markers */
+	found += marker_remove_probe_range(name,
+			__start___markers, __stop___markers);
+	/* Markers in modules. */
+	list_for_each_entry(mod, &modules, list) {
+	found += marker_remove_probe_range(name,
+		mod->markers, mod->markers+mod->num_markers);
+	}
+	mutex_unlock(&module_mutex);
+	return found;
+}
+EXPORT_SYMBOL_GPL(marker_remove_probe);
+
+/* Calls _marker_list_probe_range for the core markers and modules markers.
+ * Marker listing uses the modlist_lock to synchronise.
+ * TODO : should output this listing to a procfs file. */
+int marker_list_probe(marker_probe_func *probe)
+{
+	struct module *mod;
+	int found = 0;
+
+	mutex_lock(&module_mutex);
+	/* Core kernel markers */
+	printk("Listing kernel markers\n");
+	found += marker_list_probe_range(probe,
+			__start___markers, __stop___markers);
+	/* Markers in modules. */
+	printk("Listing module markers\n");
+	list_for_each_entry(mod, &modules, list) {
+	printk("Listing markers for module %s\n", mod->name);
+	found += marker_list_probe_range(probe,
+		mod->markers, mod->markers+mod->num_markers);
+	}
+	mutex_unlock(&module_mutex);
+	return found;
+}
+EXPORT_SYMBOL_GPL(marker_list_probe);
+#endif
+
 #ifdef CONFIG_SMP
 /* Number of blocks used and allocated. */
 static unsigned int pcpu_num_used, pcpu_num_allocated;
@@ -1058,6 +1280,8 @@
 /* Free a module, remove from lists, etc (must hold module mutex). */
 static void free_module(struct module *mod)
 {
+	MARK(kernel_module_free, "%s", mod->name);
+
 	/* Delete from various lists */
 	stop_machine_run(__unlink_module, mod, NR_CPUS);
 	remove_sect_attrs(mod);
@@ -1460,6 +1684,9 @@
 	void *percpu = NULL, *ptr = NULL; /* Stops spurious gcc warning */
 	struct exception_table_entry *extable;
 	mm_segment_t old_fs;
+        unsigned int markersindex;
+        unsigned int markersdataindex;
+        unsigned int markersstringsindex;
 
 	DEBUGP("load_module: umod=%p, len=%lu, uargs=%p\n",
 	       umod, len, uargs);
@@ -1544,6 +1771,10 @@
 	versindex = find_sec(hdr, sechdrs, secstrings, "__versions");
 	infoindex = find_sec(hdr, sechdrs, secstrings, ".modinfo");
 	pcpuindex = find_pcpusec(hdr, sechdrs, secstrings);
+	markersindex = find_sec(hdr, sechdrs, secstrings, "__markers");
+	markersdataindex = find_sec(hdr, sechdrs, secstrings, "__markers_data");
+	markersstringsindex = find_sec(hdr, sechdrs, secstrings,
+				"__markers_strings");
 
 	/* Don't keep modinfo section */
 	sechdrs[infoindex].sh_flags &= ~(unsigned long)SHF_ALLOC;
@@ -1553,6 +1784,23 @@
 	sechdrs[strindex].sh_flags |= SHF_ALLOC;
 #endif
 
+#ifdef CONFIG_MARKERS
+        if (markersindex)
+                sechdrs[markersindex].sh_flags |= SHF_ALLOC;
+        if (markersdataindex)
+                sechdrs[markersdataindex].sh_flags |= SHF_ALLOC;
+        if (markersstringsindex)
+                sechdrs[markersstringsindex].sh_flags |= SHF_ALLOC;
+#else
+        if (markersindex)
+                sechdrs[markersindex].sh_flags &= ~(unsigned long)SHF_ALLOC;
+        if (markersdataindex)
+                sechdrs[markersdataindex].sh_flags &= ~(unsigned long)SHF_ALLOC;
+        if (markersstringsindex)
+                sechdrs[markersstringsindex].sh_flags
+                                        &= ~(unsigned long)SHF_ALLOC;
+#endif
+
 	/* Check module struct version now, before we try to use module. */
 	if (!check_modstruct_version(sechdrs, versindex, mod)) {
 		err = -ENOEXEC;
@@ -1684,6 +1932,11 @@
 	mod->gpl_future_syms = (void *)sechdrs[gplfutureindex].sh_addr;
 	if (gplfuturecrcindex)
 		mod->gpl_future_crcs = (void *)sechdrs[gplfuturecrcindex].sh_addr;
+	if (markersindex) {
+		mod->markers = (void *)sechdrs[markersindex].sh_addr;
+		mod->num_markers =
+			sechdrs[markersindex].sh_size / sizeof(*mod->markers);
+	}
 
 #ifdef CONFIG_MODVERSIONS
 	if ((mod->num_syms && !crcindex) || 
@@ -1789,6 +2042,8 @@
 	/* Get rid of temporary copy */
 	vfree(hdr);
 
+	MARK(kernel_module_load, "%s", mod->name);
+
 	/* Done! */
 	return mod;
 
@@ -2098,6 +2353,26 @@
 	.show	= m_show
 };
 
+void list_modules(void)
+{
+	/* Enumerate loaded modules */
+	struct list_head	*i;
+	struct module		*mod;
+	unsigned long refcount = 0;
+
+	mutex_lock(&module_mutex);
+	list_for_each(i, &modules) {
+		mod = list_entry(i, struct module, list);
+#ifdef CONFIG_MODULE_UNLOAD
+		refcount = local_read(&mod->ref[0].count);
+#endif //CONFIG_MODULE_UNLOAD
+		MARK(list_module, "%s %d %lu",
+				mod->name, mod->state, refcount);
+	}
+	mutex_unlock(&module_mutex);
+}
+EXPORT_SYMBOL_GPL(list_modules);
+
 /* Given an address, look for it in the module exception tables. */
 const struct exception_table_entry *search_module_extables(unsigned long addr)
 {
Index: linux/kernel/printk.c
===================================================================
--- linux.orig/kernel/printk.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/printk.c	2007-08-30 12:39:34.050644000 +0100
@@ -478,6 +478,7 @@
 	int r;
 
 	va_start(args, fmt);
+	MARK(kernel_printk, "%p", __builtin_return_address(0));
 	r = vprintk(fmt, args);
 	va_end(args);
 
@@ -508,6 +509,29 @@
 	/* Emit the output into the temporary buffer */
 	printed_len = vscnprintf(printk_buf, sizeof(printk_buf), fmt, args);
 
+	if (printed_len > 0) {
+		unsigned int loglevel;
+		int mark_len;
+		const char *mark_buf;
+
+		if (printk_buf[0] == '<' && printk_buf[1] >='0' &&
+		   printk_buf[1] <= '7' && printk_buf[2] == '>') {
+			loglevel = printk_buf[1] - '0';
+			mark_buf = &printk_buf[3];
+			mark_len = printed_len-3;
+		} else {
+			loglevel = default_message_loglevel;
+			mark_buf = printk_buf;
+			mark_len = printed_len;
+		}
+		if (mark_buf[mark_len-1] == '\n')
+			mark_len--;
+		_MARK(MF_DEFAULT & ~MF_LOCKDEP, kernel_vprintk,
+			"%c %*:*v %p",
+			loglevel, sizeof(char), mark_len, mark_buf,
+			__builtin_return_address(0));
+	}
+
 	/*
 	 * Copy the output into log_buf.  If the caller didn't provide
 	 * appropriate log level tags, we insert them here
Index: linux/kernel/relay.c
===================================================================
--- linux.orig/kernel/relay.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/relay.c	2007-08-30 12:39:34.057644000 +0100
@@ -7,6 +7,8 @@
  * Copyright (C) 1999-2005 - Karim Yaghmour (karim@opersys.com)
  *
  * Moved to kernel/relay.c by Paul Mundt, 2006.
+ * November 2006 - CPU hotplug support by Mathieu Desnoyers
+ * 	(mathieu.desnoyers@polymtl.ca)
  *
  * This file is released under the GPL.
  */
@@ -18,6 +20,11 @@
 #include <linux/relay.h>
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
+#include <linux/cpu.h>
+
+/* list of open channels, for cpu hotplug */
+static DEFINE_MUTEX(relay_channels_mutex);
+static LIST_HEAD(relay_channels);
 
 /*
  * close() vm_op implementation for relay file mapping.
@@ -95,7 +102,7 @@
  *	@buf: the buffer struct
  *	@size: total size of the buffer
  *
- *	Returns a pointer to the resulting buffer, NULL if unsuccessful. The
+ *	Returns a pointer to the resulting buffer, %NULL if unsuccessful. The
  *	passed in size will get page aligned, if it isn't already.
  */
 static void *relay_alloc_buf(struct rchan_buf *buf, size_t *size)
@@ -132,14 +139,13 @@
 
 /**
  *	relay_create_buf - allocate and initialize a channel buffer
- *	@alloc_size: size of the buffer to allocate
- *	@n_subbufs: number of sub-buffers in the channel
+ *	@chan: the relay channel
  *
- *	Returns channel buffer if successful, NULL otherwise
+ *	Returns channel buffer if successful, %NULL otherwise.
  */
 struct rchan_buf *relay_create_buf(struct rchan *chan)
 {
-	struct rchan_buf *buf = kcalloc(1, sizeof(struct rchan_buf), GFP_KERNEL);
+	struct rchan_buf *buf = kzalloc(sizeof(struct rchan_buf), GFP_KERNEL);
 	if (!buf)
 		return NULL;
 
@@ -163,6 +169,7 @@
 
 /**
  *	relay_destroy_channel - free the channel struct
+ *	@kref: target kernel reference that contains the relay channel
  *
  *	Should only be called from kref_put().
  */
@@ -187,6 +194,7 @@
 			__free_page(buf->page_array[i]);
 		kfree(buf->page_array);
 	}
+	chan->buf[buf->cpu] = NULL;
 	kfree(buf->padding);
 	kfree(buf);
 	kref_put(&chan->kref, relay_destroy_channel);
@@ -194,6 +202,7 @@
 
 /**
  *	relay_remove_buf - remove a channel buffer
+ *	@kref: target kernel reference that contains the relay buffer
  *
  *	Removes the file from the fileystem, which also frees the
  *	rchan_buf_struct and the channel buffer.  Should only be called from
@@ -301,32 +310,41 @@
 
 /**
  *	wakeup_readers - wake up readers waiting on a channel
- *	@private: the channel buffer
+ *	@work: work struct that contains the the channel buffer
  *
  *	This is the work function used to defer reader waking.  The
  *	reason waking is deferred is that calling directly from write
  *	causes problems if you're writing from say the scheduler.
  */
-static void wakeup_readers(void *private)
+/*static void wakeup_readers(struct work_struct *work)
 {
-	struct rchan_buf *buf = private;
+	struct rchan_buf *buf =
+		container_of(work, struct rchan_buf, wake_readers.work);
 	wake_up_interruptible(&buf->read_wait);
+}*/
+
+static void wakeup_readers(void *private)
+{
+        struct rchan_buf *buf = private;
+        wake_up_interruptible(&buf->read_wait);
 }
 
+
 /**
  *	__relay_reset - reset a channel buffer
  *	@buf: the channel buffer
  *	@init: 1 if this is a first-time initialization
  *
- *	See relay_reset for description of effect.
+ *	See relay_reset() for description of effect.
  */
-static inline void __relay_reset(struct rchan_buf *buf, unsigned int init)
+static void __relay_reset(struct rchan_buf *buf, unsigned int init)
 {
 	size_t i;
 
 	if (init) {
 		init_waitqueue_head(&buf->read_wait);
 		kref_init(&buf->kref);
+		/*INIT_DELAYED_WORK(&buf->wake_readers, NULL);*/
 		INIT_WORK(&buf->wake_readers, NULL, NULL);
 	} else {
 		cancel_delayed_work(&buf->wake_readers);
@@ -354,57 +372,75 @@
  *	and restarting the channel in its initial state.  The buffers
  *	are not freed, so any mappings are still in effect.
  *
- *	NOTE: Care should be taken that the channel isn't actually
+ *	NOTE. Care should be taken that the channel isn't actually
  *	being used by anything when this call is made.
  */
 void relay_reset(struct rchan *chan)
 {
 	unsigned int i;
-	struct rchan_buf *prev = NULL;
 
 	if (!chan)
 		return;
 
-	for (i = 0; i < NR_CPUS; i++) {
-		if (!chan->buf[i] || chan->buf[i] == prev)
-			break;
-		__relay_reset(chan->buf[i], 0);
-		prev = chan->buf[i];
+ 	if (chan->is_global && chan->buf[0]) {
+		__relay_reset(chan->buf[0], 0);
+		return;
 	}
+
+	mutex_lock(&relay_channels_mutex);
+	for_each_online_cpu(i)
+		if (chan->buf[i])
+			__relay_reset(chan->buf[i], 0);
+	mutex_unlock(&relay_channels_mutex);
 }
 EXPORT_SYMBOL_GPL(relay_reset);
 
-/**
+/*
  *	relay_open_buf - create a new relay channel buffer
  *
- *	Internal - used by relay_open().
+ *	used by relay_open() and CPU hotplug.
  */
-static struct rchan_buf *relay_open_buf(struct rchan *chan,
-					const char *filename,
-					struct dentry *parent,
-					int *is_global)
+static struct rchan_buf *relay_open_buf(struct rchan *chan, unsigned int cpu)
 {
-	struct rchan_buf *buf;
+ 	struct rchan_buf *buf = NULL;
 	struct dentry *dentry;
+ 	char *tmpname;
 
-	if (*is_global)
+ 	if (chan->is_global)
 		return chan->buf[0];
 
+	tmpname = kzalloc(NAME_MAX + 1, GFP_KERNEL);
+ 	if (!tmpname)
+ 		goto end;
+ 	snprintf(tmpname, NAME_MAX, "%s%d", chan->base_filename, cpu);
+
 	buf = relay_create_buf(chan);
 	if (!buf)
-		return NULL;
+ 		goto free_name;
+
+ 	buf->cpu = cpu;
+ 	__relay_reset(buf, 1);
 
 	/* Create file in fs */
-	dentry = chan->cb->create_buf_file(filename, parent, S_IRUSR,
-					   buf, is_global);
-	if (!dentry) {
-		relay_destroy_buf(buf);
-		return NULL;
-	}
+ 	dentry = chan->cb->create_buf_file(tmpname, chan->parent, S_IRUSR,
+ 					   buf, &chan->is_global);
+ 	if (!dentry)
+ 		goto free_buf;
 
 	buf->dentry = dentry;
-	__relay_reset(buf, 1);
 
+ 	if(chan->is_global) {
+ 		chan->buf[0] = buf;
+ 		buf->cpu = 0;
+  	}
+
+ 	goto free_name;
+
+free_buf:
+ 	relay_destroy_buf(buf);
+free_name:
+ 	kfree(tmpname);
+end:
 	return buf;
 }
 
@@ -416,7 +452,7 @@
  *	The channel buffer and channel buffer data structure are then freed
  *	automatically when the last reference is given up.
  */
-static inline void relay_close_buf(struct rchan_buf *buf)
+static void relay_close_buf(struct rchan_buf *buf)
 {
 	buf->finalized = 1;
 	cancel_delayed_work(&buf->wake_readers);
@@ -424,7 +460,7 @@
 	kref_put(&buf->kref, relay_remove_buf);
 }
 
-static inline void setup_callbacks(struct rchan *chan,
+static void setup_callbacks(struct rchan *chan,
 				   struct rchan_callbacks *cb)
 {
 	if (!cb) {
@@ -446,38 +482,77 @@
 }
 
 /**
+ * 	relay_hotcpu_callback - CPU hotplug callback
+ * 	@nb: notifier block
+ * 	@action: hotplug action to take
+ * 	@hcpu: CPU number
+ *
+ * 	Returns the success/failure of the operation. (%NOTIFY_OK, %NOTIFY_BAD)
+ */
+static int __cpuinit relay_hotcpu_callback(struct notifier_block *nb,
+				unsigned long action,
+				void *hcpu)
+{
+	unsigned int hotcpu = (unsigned long)hcpu;
+	struct rchan *chan;
+
+	switch(action) {
+	case CPU_UP_PREPARE:
+		mutex_lock(&relay_channels_mutex);
+		list_for_each_entry(chan, &relay_channels, list) {
+			if (chan->buf[hotcpu])
+				continue;
+			chan->buf[hotcpu] = relay_open_buf(chan, hotcpu);
+			if(!chan->buf[hotcpu]) {
+				printk(KERN_ERR
+					"relay_hotcpu_callback: cpu %d buffer "
+					"creation failed\n", hotcpu);
+				mutex_unlock(&relay_channels_mutex);
+				return NOTIFY_BAD;
+			}
+		}
+		mutex_unlock(&relay_channels_mutex);
+		break;
+	case CPU_DEAD:
+		/* No need to flush the cpu : will be flushed upon
+		 * final relay_flush() call. */
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+/**
  *	relay_open - create a new relay channel
  *	@base_filename: base name of files to create
- *	@parent: dentry of parent directory, NULL for root directory
+ *	@parent: dentry of parent directory, %NULL for root directory
  *	@subbuf_size: size of sub-buffers
  *	@n_subbufs: number of sub-buffers
  *	@cb: client callback functions
+ *	@private_data: user-defined data
  *
- *	Returns channel pointer if successful, NULL otherwise.
+ *	Returns channel pointer if successful, %NULL otherwise.
  *
  *	Creates a channel buffer for each cpu using the sizes and
  *	attributes specified.  The created channel buffer files
  *	will be named base_filename0...base_filenameN-1.  File
- *	permissions will be S_IRUSR.
+ *	permissions will be %S_IRUSR.
  */
 struct rchan *relay_open(const char *base_filename,
 			 struct dentry *parent,
 			 size_t subbuf_size,
 			 size_t n_subbufs,
-			 struct rchan_callbacks *cb)
+			 struct rchan_callbacks *cb,
+			 void *private_data)
 {
 	unsigned int i;
 	struct rchan *chan;
-	char *tmpname;
-	int is_global = 0;
-
 	if (!base_filename)
 		return NULL;
 
 	if (!(subbuf_size && n_subbufs))
 		return NULL;
 
-	chan = kcalloc(1, sizeof(struct rchan), GFP_KERNEL);
+	chan = kzalloc(sizeof(struct rchan), GFP_KERNEL);
 	if (!chan)
 		return NULL;
 
@@ -485,38 +560,32 @@
 	chan->n_subbufs = n_subbufs;
 	chan->subbuf_size = subbuf_size;
 	chan->alloc_size = FIX_SIZE(subbuf_size * n_subbufs);
+	chan->parent = parent;
+	chan->private_data = private_data;
+	strlcpy(chan->base_filename, base_filename, NAME_MAX);
 	setup_callbacks(chan, cb);
 	kref_init(&chan->kref);
 
-	tmpname = kmalloc(NAME_MAX + 1, GFP_KERNEL);
-	if (!tmpname)
-		goto free_chan;
-
+	mutex_lock(&relay_channels_mutex);
 	for_each_online_cpu(i) {
-		sprintf(tmpname, "%s%d", base_filename, i);
-		chan->buf[i] = relay_open_buf(chan, tmpname, parent,
-					      &is_global);
+		chan->buf[i] = relay_open_buf(chan, i);
 		if (!chan->buf[i])
 			goto free_bufs;
-
-		chan->buf[i]->cpu = i;
 	}
+	list_add(&chan->list, &relay_channels);
+	mutex_unlock(&relay_channels_mutex);
 
-	kfree(tmpname);
 	return chan;
 
 free_bufs:
-	for (i = 0; i < NR_CPUS; i++) {
+	for_each_online_cpu(i) {
 		if (!chan->buf[i])
 			break;
 		relay_close_buf(chan->buf[i]);
-		if (is_global)
-			break;
 	}
-	kfree(tmpname);
 
-free_chan:
 	kref_put(&chan->kref, relay_destroy_channel);
+	mutex_unlock(&relay_channels_mutex);
 	return NULL;
 }
 EXPORT_SYMBOL_GPL(relay_open);
@@ -548,7 +617,10 @@
 			buf->padding[old_subbuf];
 		smp_mb();
 		if (waitqueue_active(&buf->read_wait)) {
-			PREPARE_WORK(&buf->wake_readers, wakeup_readers, buf);
+/*			PREPARE_DELAYED_WORK(&buf->wake_readers,
+					     wakeup_readers);
+*/			PREPARE_WORK(&buf->wake_readers, wakeup_readers, buf);
+
 			schedule_delayed_work(&buf->wake_readers, 1);
 		}
 	}
@@ -585,7 +657,7 @@
  *	subbufs_consumed should be the number of sub-buffers newly consumed,
  *	not the total consumed.
  *
- *	NOTE: kernel clients don't need to call this function if the channel
+ *	NOTE. Kernel clients don't need to call this function if the channel
  *	mode is 'overwrite'.
  */
 void relay_subbufs_consumed(struct rchan *chan,
@@ -616,24 +688,26 @@
 void relay_close(struct rchan *chan)
 {
 	unsigned int i;
-	struct rchan_buf *prev = NULL;
 
 	if (!chan)
 		return;
 
-	for (i = 0; i < NR_CPUS; i++) {
-		if (!chan->buf[i] || chan->buf[i] == prev)
-			break;
-		relay_close_buf(chan->buf[i]);
-		prev = chan->buf[i];
-	}
+	mutex_lock(&relay_channels_mutex);
+	if (chan->is_global && chan->buf[0])
+		relay_close_buf(chan->buf[0]);
+	else
+		for_each_possible_cpu(i)
+			if (chan->buf[i])
+				relay_close_buf(chan->buf[i]);
 
 	if (chan->last_toobig)
 		printk(KERN_WARNING "relay: one or more items not logged "
 		       "[item size (%Zd) > sub-buffer size (%Zd)]\n",
 		       chan->last_toobig, chan->subbuf_size);
 
+	list_del(&chan->list);
 	kref_put(&chan->kref, relay_destroy_channel);
+	mutex_unlock(&relay_channels_mutex);
 }
 EXPORT_SYMBOL_GPL(relay_close);
 
@@ -641,22 +715,25 @@
  *	relay_flush - close the channel
  *	@chan: the channel
  *
- *	Flushes all channel buffers i.e. forces buffer switch.
+ *	Flushes all channel buffers, i.e. forces buffer switch.
  */
 void relay_flush(struct rchan *chan)
 {
 	unsigned int i;
-	struct rchan_buf *prev = NULL;
 
 	if (!chan)
 		return;
 
-	for (i = 0; i < NR_CPUS; i++) {
-		if (!chan->buf[i] || chan->buf[i] == prev)
-			break;
-		relay_switch_subbuf(chan->buf[i], 0);
-		prev = chan->buf[i];
+	if (chan->is_global && chan->buf[0]) {
+		relay_switch_subbuf(chan->buf[0], 0);
+		return;
 	}
+
+	mutex_lock(&relay_channels_mutex);
+	for_each_possible_cpu(i)
+		if (chan->buf[i])
+			relay_switch_subbuf(chan->buf[i], 0);
+	mutex_unlock(&relay_channels_mutex);
 }
 EXPORT_SYMBOL_GPL(relay_flush);
 
@@ -681,7 +758,7 @@
  *	@filp: the file
  *	@vma: the vma describing what to map
  *
- *	Calls upon relay_mmap_buf to map the file into user space.
+ *	Calls upon relay_mmap_buf() to map the file into user space.
  */
 static int relay_file_mmap(struct file *filp, struct vm_area_struct *vma)
 {
@@ -729,7 +806,7 @@
 	return 0;
 }
 
-/**
+/*
  *	relay_file_read_consume - update the consumed count for the buffer
  */
 static void relay_file_read_consume(struct rchan_buf *buf,
@@ -756,7 +833,7 @@
 	}
 }
 
-/**
+/*
  *	relay_file_read_avail - boolean, are there unconsumed bytes available?
  */
 static int relay_file_read_avail(struct rchan_buf *buf, size_t read_pos)
@@ -793,6 +870,8 @@
 
 /**
  *	relay_file_read_subbuf_avail - return bytes available in sub-buffer
+ *	@read_pos: file read position
+ *	@buf: relay channel buffer
  */
 static size_t relay_file_read_subbuf_avail(size_t read_pos,
 					   struct rchan_buf *buf)
@@ -818,8 +897,10 @@
 
 /**
  *	relay_file_read_start_pos - find the first available byte to read
+ *	@read_pos: file read position
+ *	@buf: relay channel buffer
  *
- *	If the read_pos is in the middle of padding, return the
+ *	If the @read_pos is in the middle of padding, return the
  *	position of the first actually available byte, otherwise
  *	return the original value.
  */
@@ -844,6 +925,9 @@
 
 /**
  *	relay_file_read_end_pos - return the new read position
+ *	@read_pos: file read position
+ *	@buf: relay channel buffer
+ *	@count: number of bytes to be read
  */
 static size_t relay_file_read_end_pos(struct rchan_buf *buf,
 				      size_t read_pos,
@@ -865,7 +949,7 @@
 	return end_pos;
 }
 
-/**
+/*
  *	subbuf_read_actor - read up to one subbuf's worth of data
  */
 static int subbuf_read_actor(size_t read_start,
@@ -879,7 +963,7 @@
 
 	from = buf->start + read_start;
 	ret = avail;
-	if (copy_to_user(desc->arg.data, from, avail)) {
+	if (copy_to_user(desc->arg.buf, from, avail)) {
 		desc->error = -EFAULT;
 		ret = 0;
 	}
@@ -890,7 +974,7 @@
 	return ret;
 }
 
-/**
+/*
  *	subbuf_send_actor - send up to one subbuf's worth of data
  */
 static int subbuf_send_actor(size_t read_start,
@@ -933,29 +1017,21 @@
 			       read_descriptor_t *desc,
 			       read_actor_t actor);
 
-/**
+/*
  *	relay_file_read_subbufs - read count bytes, bridging subbuf boundaries
  */
-static inline ssize_t relay_file_read_subbufs(struct file *filp,
-					      loff_t *ppos,
-					      size_t count,
-					      subbuf_actor_t subbuf_actor,
-					      read_actor_t actor,
-					      void *target)
+static ssize_t relay_file_read_subbufs(struct file *filp, loff_t *ppos,
+					subbuf_actor_t subbuf_actor,
+					read_actor_t actor,
+					read_descriptor_t *desc)
 {
 	struct rchan_buf *buf = filp->private_data;
 	size_t read_start, avail;
-	read_descriptor_t desc;
 	int ret;
 
-	if (!count)
+	if (!desc->count)
 		return 0;
 
-	desc.written = 0;
-	desc.count = count;
-	desc.arg.data = target;
-	desc.error = 0;
-
 	mutex_lock(&filp->f_dentry->d_inode->i_mutex);
 	do {
 		if (!relay_file_read_avail(buf, *ppos))
@@ -966,19 +1042,19 @@
 		if (!avail)
 			break;
 
-		avail = min(desc.count, avail);
-		ret = subbuf_actor(read_start, buf, avail, &desc, actor);
-		if (desc.error < 0)
+		avail = min(desc->count, avail);
+		ret = subbuf_actor(read_start, buf, avail, desc, actor);
+		if (desc->error < 0)
 			break;
 
 		if (ret) {
 			relay_file_read_consume(buf, read_start, ret);
 			*ppos = relay_file_read_end_pos(buf, read_start, ret);
 		}
-	} while (desc.count && ret);
+	} while (desc->count && ret);
 	mutex_unlock(&filp->f_dentry->d_inode->i_mutex);
 
-	return desc.written;
+	return desc->written;
 }
 
 static ssize_t relay_file_read(struct file *filp,
@@ -986,8 +1062,13 @@
 			       size_t count,
 			       loff_t *ppos)
 {
-	return relay_file_read_subbufs(filp, ppos, count, subbuf_read_actor,
-				       NULL, buffer);
+	read_descriptor_t desc;
+	desc.written = 0;
+	desc.count = count;
+	desc.arg.buf = buffer;
+	desc.error = 0;
+	return relay_file_read_subbufs(filp, ppos, subbuf_read_actor,
+				       NULL, &desc);
 }
 
 static ssize_t relay_file_sendfile(struct file *filp,
@@ -996,11 +1077,16 @@
 				   read_actor_t actor,
 				   void *target)
 {
-	return relay_file_read_subbufs(filp, ppos, count, subbuf_send_actor,
-				       actor, target);
+	read_descriptor_t desc;
+	desc.written = 0;
+	desc.count = count;
+	desc.arg.data = target;
+	desc.error = 0;
+	return relay_file_read_subbufs(filp, ppos, subbuf_send_actor,
+				       actor, &desc);
 }
 
-struct file_operations relay_file_operations = {
+const struct file_operations relay_file_operations = {
 	.open		= relay_file_open,
 	.poll		= relay_file_poll,
 	.mmap		= relay_file_mmap,
@@ -1010,3 +1096,12 @@
 	.sendfile       = relay_file_sendfile,
 };
 EXPORT_SYMBOL_GPL(relay_file_operations);
+
+static __init int relay_init(void)
+{
+
+	hotcpu_notifier(relay_hotcpu_callback, 0);
+	return 0;
+}
+
+module_init(relay_init);
Index: linux/kernel/sched.c
===================================================================
--- linux.orig/kernel/sched.c	2007-08-30 11:04:38.646885000 +0100
+++ linux/kernel/sched.c	2007-08-30 12:39:34.077644000 +0100
@@ -907,6 +907,7 @@
 
 repeat:
 	rq = task_rq_lock(p, &flags);
+	MARK(kernel_sched_wait_task, "%d %ld", p->pid, p->state);
 	/* Must be off runqueue entirely, not preempted. */
 	if (unlikely(p->array || task_running(rq, p))) {
 		/* If it's preempted, we yield.  It could be a while. */
@@ -1173,6 +1174,7 @@
 #endif
 
 	rq = task_rq_lock(p, &flags);
+	MARK(kernel_sched_try_wakeup, "%d %ld", p->pid, p->state);
 	old_state = p->state;
 	if (!(old_state & state))
 		goto out;
@@ -1402,6 +1404,7 @@
 	runqueue_t *rq, *this_rq;
 
 	rq = task_rq_lock(p, &flags);
+	MARK(kernel_sched_wakeup_new_task, "%d %ld", p->pid, p->state);
 	BUG_ON(p->state != TASK_RUNNING);
 	this_cpu = smp_processor_id();
 	cpu = task_cpu(p);
@@ -1763,6 +1766,8 @@
 	    || unlikely(cpu_is_offline(dest_cpu)))
 		goto out;
 
+	MARK(kernel_sched_migrate_task, "%d %ld %d",
+		p->pid, p->state, dest_cpu);
 	/* force the process onto the specified CPU */
 	if (migrate_task(p, dest_cpu, &req)) {
 		/* Need to wait for migration thread (might exit: take ref). */
@@ -3054,6 +3059,8 @@
 		++*switch_count;
 
 		prepare_task_switch(rq, next);
+		MARK(kernel_sched_schedule, "%d %d %ld",
+			prev->pid, next->pid, prev->state);
 		prev = context_switch(rq, prev, next);
 		barrier();
 		/*
Index: linux/kernel/signal.c
===================================================================
--- linux.orig/kernel/signal.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/signal.c	2007-08-30 12:39:34.088644000 +0100
@@ -776,6 +776,8 @@
 	if (sig_ignored(t, sig))
 		goto out;
 
+	MARK(kernel_process_signal, "%d %d", t->pid, sig);
+
 	/* Support queueing exactly one non-rt signal, so that we
 	   can get more detailed information about the cause of
 	   the signal. */
Index: linux/kernel/softirq.c
===================================================================
--- linux.orig/kernel/softirq.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/softirq.c	2007-08-30 12:39:34.092644000 +0100
@@ -93,7 +93,15 @@
 
 	do {
 		if (pending & 1) {
+			MARK(kernel_softirq_entry, "%lu",
+				((unsigned long)h
+					- (unsigned long)softirq_vec)
+					/ sizeof(*h));
 			h->action(h);
+			MARK(kernel_softirq_exit, "%lu",
+				((unsigned long)h
+					- (unsigned long)softirq_vec)
+					/ sizeof(*h));
 			rcu_bh_qsctr_inc(cpu);
 		}
 		h++;
@@ -265,7 +273,11 @@
 			if (!atomic_read(&t->count)) {
 				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
 					BUG();
+				MARK(kernel_tasklet_low_entry, "%p %lu",
+						t->func, t->data);
 				t->func(t->data);
+				MARK(kernel_tasklet_low_exit, "%p %lu",
+						t->func, t->data);
 				tasklet_unlock(t);
 				continue;
 			}
@@ -298,7 +310,11 @@
 			if (!atomic_read(&t->count)) {
 				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
 					BUG();
+				MARK(kernel_tasklet_high_entry, "%p %lu",
+						t->func, t->data);
 				t->func(t->data);
+				MARK(kernel_tasklet_high_exit, "%p %lu",
+						t->func, t->data);
 				tasklet_unlock(t);
 				continue;
 			}
Index: linux/kernel/sys_ni.c
===================================================================
--- linux.orig/kernel/sys_ni.c	2006-06-18 02:49:35.000000000 +0100
+++ linux/kernel/sys_ni.c	2007-08-30 12:39:34.096644000 +0100
@@ -110,6 +110,8 @@
 cond_syscall(sys_vm86);
 cond_syscall(compat_sys_ipc);
 cond_syscall(compat_sys_sysctl);
+cond_syscall(sys_ltt_trace_generic);
+cond_syscall(sys_ltt_register_generic);
 
 /* arch-specific weak syscall entries */
 cond_syscall(sys_pciconfig_read);
Index: linux/kernel/timer.c
===================================================================
--- linux.orig/kernel/timer.c	2007-08-30 11:04:38.662884000 +0100
+++ linux/kernel/timer.c	2007-08-30 12:39:34.103644000 +0100
@@ -41,6 +41,7 @@
 #include <asm/div64.h>
 #include <asm/timex.h>
 #include <asm/io.h>
+/*#include <asm/irq_regs.h>*/
 
 #ifdef CONFIG_TIME_INTERPOLATION
 static void time_interpolator_update(long delta_nsec);
@@ -131,6 +132,8 @@
 		i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
 		vec = base->tv5.vec + i;
 	}
+	MARK(kernel_timer_set, "%lu %p %lu",
+		expires, timer->function, timer->data);
 	/*
 	 * Timers are FIFO:
 	 */
@@ -943,6 +946,14 @@
 	/* prevent loading jiffies before storing new jiffies_64 value. */
 	barrier();
 	update_times();
+
+        MARK(kernel_timer_update_time,
+        "%8b %*.*r %*.*r",
+                jiffies_64,
+                sizeof(xtime), __alignof__(xtime), &xtime,
+                sizeof(wall_to_monotonic), __alignof__(wall_to_monotonic),
+                &wall_to_monotonic);
+
 }
 
 #ifdef __ARCH_WANT_SYS_ALARM
@@ -1024,6 +1035,10 @@
 
 static void process_timeout(unsigned long __data)
 {
+
+        struct task_struct *task = (struct task_struct *)__data;
+        MARK(kernel_timer_timeout, "%d", task->pid);
+
 	wake_up_process((task_t *)__data);
 }
 
Index: linux/ltt/Kconfig
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/Kconfig	2007-08-30 12:39:34.108644000 +0100
@@ -0,0 +1,310 @@
+config LTT
+	bool "Linux Trace Toolkit Instrumentation Support"
+	depends on EXPERIMENTAL
+	depends on MARKERS
+	select LTT_HEARTBEAT if MIPS || SUPERH
+	select LTT_SYNTHETIC_TSC if MIPS || SUPERH
+	default n
+	help
+	  It is possible for the kernel to log important events to a trace
+	  facility. Doing so, enables the use of the generated traces in order
+	  to reconstruct the dynamic behavior of the kernel, and hence the
+	  whole system.
+
+	  The tracing process contains 4 parts :
+	      1) The logging of events by key parts of the kernel.
+	      2) The tracer that keeps the events in a data buffer (uses
+	         relayfs).
+	      3) A trace daemon that interacts with the tracer and is
+	         notified every time there is a certain quantity of data to
+	         read from the tracer.
+	      4) A trace event data decoder that reads the accumulated data
+	         and formats it in a human-readable format.
+
+	  If you say Y, the first component will be built into the kernel.
+
+	  For more information on kernel tracing, the trace daemon or the event
+	  decoder, please check the following address :
+	       http://www.opersys.com/ltt
+	  See also the experimental page of the project :
+	       http://ltt.polymtl.ca
+
+config LTT_TRACER
+	tristate "Linux Trace Toolkit Tracer"
+	depends on LTT
+	default y
+	help
+	  If you enable this option, the Linux Trace Toolkit Tracer will be
+	  either built in the kernel or as module.
+
+	  Critical parts of the kernel will call upon the kernel tracing
+	  function. The data is then recorded by the tracer if a trace daemon
+	  is running in user-space and has issued a "start" command.
+
+	  For more information on kernel tracing, the trace daemon or the event
+	  decoder, please check the following address :
+	       http://www.opersys.com/ltt
+	  See also the experimental page of the project :
+	       http://ltt.polymtl.ca
+
+config LTT_RELAY
+	tristate "Linux Trace Toolkit Relay+DebugFS Support"
+	select RELAY
+	select DEBUG_FS
+	depends on LTT_TRACER
+	default y
+	help
+	  Support using relay and debugfs to log the data obtained through LTT.
+
+	  If you don't have special hardware, you almost certainly want
+	  to say Y here.
+
+config LTT_SERIALIZE
+	tristate "Linux Trace Toolkit Serializer"
+	depends on LTT_TRACER
+	default y
+	help
+	  Library for serializing information from format string and argument
+	  list to the trace buffers.
+
+config LTT_ALIGNMENT
+	bool "Align Linux Trace Toolkit Traces"
+	depends on LTT
+	default y
+	help
+	  This option enables dynamic alignment of data in buffers. The
+	  alignment is made on the smallest size between architecture size
+	  and the size of the value to be written.
+
+	  Dynamically calculating the offset of the data has a performance cost,
+	  but it is more efficient on some architectures (especially 64 bits) to
+	  align data than to write it unaligned.
+
+config LTT_HEARTBEAT
+	bool "Activate Linux Trace Toolkit Heartbeat Timer"
+	depends on LTT
+	default n
+	help
+	  The Linux Trace Toolkit Heartbeat Timer fires at an interval small
+	  enough to guarantee that the 32 bits truncated TSC won't overflow
+	  between two timer execution.
+
+config LTT_HEARTBEAT_EVENT
+	bool "Write heartbeat event to shrink traces (EXPERIMENTAL)"
+	depends on LTT_HEARTBEAT
+	depends on EXPERIMENTAL
+	default n
+	help
+	  This option makes the heartbeat timer write an event in each tracefile
+	  at an interval that is one tenth of the time it takes to overflow 32
+	  bits at your CPU frequency.
+
+	  If this option is not enabled, 64 bits fields must be used in each
+	  event header to save the full TSC : it can make traces about 1/10
+	  bigger. It is suggested that you enable this option to make more
+	  compact traces.
+
+	  Note : it is currently broken with CPU hotplug and does not support
+	  trace stop / restart correctly.
+
+config LTT_SYNTHETIC_TSC
+	bool "Keep a synthetic cpu timestamp counter"
+	depends on LTT_HEARTBEAT
+	default n
+	help
+	  This option is only useful on archtecture lacking a 64 bits timestamp
+	  counter : it generates a "synthetic" 64 bits timestamp by updating
+	  the 32 MSB at each heartbeat atomically. See kernel/ltt-heartbeat.c
+	  for details.
+
+config LTT_TEST_TSC
+	bool "Test TSC synchronicity"
+	depends on LTT
+	default y
+	depends on X86_32 || X86_64
+	help
+	  On i386 and x86_64, test TSC synchronicity. Fall back on a TSC|logical
+	  clock scheme where CPU with TSCs lagging behind use the logical
+	  relative to the highest TSC count read.
+
+config LTT_USERSPACE_GENERIC
+	bool "Allow tracing from userspace"
+	depends on LTT
+	default y
+	help
+	  This options allows processes to trace through the ltt_trace_generic
+	  system call after they have registered their facilities with
+	  ltt_register_generic.
+
+config LTT_NETLINK_CONTROL
+	tristate "Linux Trace Toolkit Netlink Controller"
+	depends on LTT_TRACER
+	select NET
+	default m
+	help
+	  If you enable this option, the Linux Trace Toolkit Netlink Controller
+	  will be either built in the kernel of as module.
+
+config LTT_STATEDUMP
+	tristate "Linux Trace Toolkit State Dump"
+	depends on LTT_TRACER
+	default m
+	help
+	  If you enable this option, the Linux Trace Toolkit State Dump will
+	  be either built in the kernel of as module.
+
+	  This module saves the state of the running kernel at trace start
+	  into the trace buffers along with the ongoing tracing information.
+
+menu "Probes"
+	depends on LTT && MARKERS
+
+config LTT_PROBE_CORE
+	tristate "Linux Trace Toolkit Core Probe"
+	depends on LTT_SERIALIZE
+	depends on LTT_TRACER
+	depends on CRC32
+	default m
+	help
+	  Probes the core LTT events, required to have the event descriptions
+	  and heartbeat events.
+
+config LTT_PROBE_FS
+	tristate "Linux Trace Toolkit File System Probe"
+	depends on LTT_SERIALIZE
+	depends on CRC32
+	default m
+	help
+	  Activate per facilities LTT probes. This is the dynamic mechanism
+	  where the data gathering is. Probes connect to markers when their
+	  module is loaded.
+
+	  Probe file system events. It instruments the file operations and
+	  dumps the beginning of data sent through read and write system calls.
+
+config LTT_PROBE_KERNEL
+	tristate "Linux Trace Toolkit Kernel Probe"
+	depends on LTT_SERIALIZE
+	depends on CRC32
+	default m
+	help
+	  LTT Kernel probe. Contains event definition for tasklet, irq,
+	  soft irq, timer and scheduling events.
+
+config LTT_PROBE_ARCH
+	tristate "Linux Trace Toolkit Architecture Specific Probe"
+	depends on LTT_SERIALIZE
+	depends on X86_32 || ARM || MIPS || PPC || X86_64 || SUPERH
+	depends on CRC32
+	default m
+	help
+	  LTT Arch Probe. Contains event definition specific to each
+	  architecture. (system calls, traps, ...)
+
+config LTT_PROBE_MM
+	tristate "Linux Trace Toolkit Memory Probe"
+	depends on LTT_SERIALIZE
+	depends on CRC32
+	default m
+	help
+	  Activate per facilities LTT probes. This is the dynamic mechanism
+	  where the data gathering is. Probes connect to markers when their
+	  module is loaded.
+
+	  LTT Memory probe.
+
+config LTT_PROBE_NET
+	tristate "Linux Trace Toolkit Network Probe"
+	depends on LTT_SERIALIZE
+	depends on CRC32
+	default m
+	help
+	  Activate per facilities LTT probes. This is the dynamic mechanism
+	  where the data gathering is. Probes connect to markers when their
+	  module is loaded.
+
+	  LTT Network probe.
+
+config LTT_PROBE_LIST
+	tristate "Linux Trace Toolkit system listing probe"
+	depends on LTT_SERIALIZE
+	depends on LTT_STATEDUMP
+	depends on CRC32
+	default m
+	help
+	  System listing probe. Contains events for dumping kernel
+	  state at the beginning of a trace.
+
+config LTT_PROBE_LOCKING
+	tristate "Linux Trace Toolkit locking"
+	depends on LTT_SERIALIZE
+	depends on X86_32 || MIPS || S390 || SUPERH || SPARC64 || UML || X86_64
+	depends on CRC32
+	select DEBUG_KERNEL
+	default m
+	help
+	  LTT locking probe. Traces currently only interrupt save/restore and
+	  disable/enable. Currently only for i386, MIPS, S390, SH, Sparc64, UML
+	  and x86_64. Will trace events from lockdep when connected with a
+	  probe.
+
+config LTT_PROBE_STACK
+	tristate "Sample process or kernel stacks (EXPERIMENTAL)"
+	depends on LTT_SERIALIZE
+	depends on X86 || X86_64
+	depends on CRC32
+	default n
+	help
+	  Get complete process and/or kernel stack (architecture specific)
+
+config LTT_PROCESS_STACK
+	bool "Get complete process stack"
+	depends on LTT_PROBE_STACK
+	depends on X86
+	depends on CRC32
+	select UNWIND_INFO
+	default n
+	help
+	  Get complete process stack.
+
+	  It has three limitations : it only considers functions in the loaded
+	  executable (not libraries) to be actual function pointers.
+	  Furthermore, it will dump the stack of many threads at once for
+	  multithreaded processes (it is protected from races between threads
+	  though). It can also believe that arbritrary data on the stack
+	  "looks like" a function pointer.
+
+config LTT_PROCESS_MAX_FUNCTION_STACK
+	int "Maximum number of longs on the stack between functions"
+	depends on LTT_PROCESS_STACK
+	default 100
+	help
+	  Maximum threshold over which, if we do not find a function pointer on
+	  the process stack, we stop dumping the pointers on the stack.
+
+config LTT_PROCESS_MAX_STACK_LEN
+	int "Maximum number of longs on the stack to read"
+	depends on LTT_PROCESS_STACK
+	default 250
+	help
+	  Maximum threshold of stack size over which we stop looking for
+	  function pointers.
+
+config LTT_KERNEL_STACK
+	bool "Get complete kernel stack"
+	depends on LTT_PROBE_STACK
+	default n
+	help
+	  Get complete kernel stack
+
+config LTT_PROBE_COMPACT
+	tristate "Linux Trace Toolkit Compact Probe"
+	depends on LTT_SERIALIZE
+	depends on LTT_HEARTBEAT_EVENT
+	depends on CRC32
+	default n
+	help
+	  Compact facility : 32 bits compact TSC and event ID. One single
+	  "compact" or "flight-compact" channel for all those events.
+
+endmenu
Index: linux/ltt/Makefile
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/Makefile	2007-08-30 12:39:34.114646000 +0100
@@ -0,0 +1,13 @@
+#
+# Makefile for the LTT objects.
+#
+obj-$(CONFIG_LTT)			+= ltt-core.o ltt-facilities.o \
+			 			probes/
+obj-$(CONFIG_LTT_USERSPACE_GENERIC)	+= ltt-syscall.o
+obj-$(CONFIG_LTT_HEARTBEAT)		+= ltt-heartbeat.o
+obj-$(CONFIG_LTT_TEST_TSC)		+= ltt-test-tsc.o
+obj-$(CONFIG_LTT_TRACER)		+= ltt-tracer.o
+obj-$(CONFIG_LTT_RELAY)			+= ltt-relay.o
+obj-$(CONFIG_LTT_NETLINK_CONTROL)	+= ltt-control.o
+obj-$(CONFIG_LTT_SERIALIZE)		+= ltt-serialize.o
+obj-$(CONFIG_LTT_STATEDUMP)		+= ltt-statedump.o
Index: linux/ltt/ltt-control.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-control.c	2007-08-30 12:39:34.120644000 +0100
@@ -0,0 +1,121 @@
+/* ltt-control.c
+ *
+ * LTT control module over a netlink socket.
+ *
+ * Inspired from Relay Apps, by Tom Zanussi and iptables
+ *
+ * Copyright 2005 -
+ * 	Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+#include <linux/netlink.h>
+#include <linux/inet.h>
+#include <linux/ip.h>
+#include <linux/security.h>
+#include <linux/skbuff.h>
+#include <linux/types.h>
+#include <net/sock.h>
+#include "ltt-control.h"
+
+#define LTTCTLM_BASE	0x10
+#define LTTCTLM_CONTROL	(LTTCTLM_BASE + 1)	/* LTT control message */
+
+static struct sock *socket;
+
+void ltt_control_input(struct sock *sk, int len)
+{
+	struct sk_buff *skb;
+	struct nlmsghdr *nlh = NULL;
+	u8 *payload = NULL;
+	lttctl_peer_msg_t *msg;
+	int err;
+
+	printk(KERN_DEBUG "ltt-control ltt_control_input\n");
+
+	while ((skb = skb_dequeue(&sk->sk_receive_queue)) != NULL) {
+
+		nlh = (struct nlmsghdr *)skb->data;
+		if (security_netlink_recv(skb)) {
+			netlink_ack(skb, nlh, EPERM);
+			kfree_skb(skb);
+			continue;
+		}
+		/* process netlink message pointed by skb->data */
+		err = EINVAL;
+		payload = NLMSG_DATA(nlh);
+		/* process netlink message with header pointed by
+		 * nlh and payload pointed by payload
+		 */
+		if (nlh->nlmsg_len !=
+				sizeof(lttctl_peer_msg_t) +
+				sizeof(struct nlmsghdr)) {
+			printk(KERN_ALERT "ltt-control bad message length %d vs. %zu\n",
+				nlh->nlmsg_len, sizeof(lttctl_peer_msg_t) +
+				sizeof(struct nlmsghdr));
+			netlink_ack(skb, nlh, EINVAL);
+			kfree_skb(skb);
+			continue;
+		}
+		msg = (lttctl_peer_msg_t*)payload;
+
+		switch (msg->op) {
+			case OP_CREATE:
+				err = ltt_control(LTT_CONTROL_CREATE_TRACE,
+						msg->trace_name,
+						msg->trace_type, msg->args);
+				break;
+			case OP_DESTROY:
+				err = ltt_control(LTT_CONTROL_DESTROY_TRACE,
+						msg->trace_name,
+						msg->trace_type, msg->args);
+				break;
+			case OP_START:
+				err = ltt_control(LTT_CONTROL_START,
+						msg->trace_name,
+						msg->trace_type, msg->args);
+				break;
+			case OP_STOP:
+				err = ltt_control(LTT_CONTROL_STOP,
+						msg->trace_name,
+						msg->trace_type, msg->args);
+				break;
+			default:
+				err = EBADRQC;
+				printk(KERN_INFO
+					"ltt-control invalid operation\n");
+		}
+		netlink_ack(skb, nlh, err);
+		kfree_skb(skb);
+	}
+}
+
+
+static int ltt_control_init(void)
+{
+	printk(KERN_INFO "ltt-control init\n");
+
+	socket = netlink_kernel_create(NETLINK_LTT, 1,
+			ltt_control_input, THIS_MODULE);
+	if (socket == NULL)
+		return -EPERM;
+	else
+		return 0;
+}
+
+static void ltt_control_exit(void)
+{
+	printk(KERN_INFO "ltt-control exit\n");
+	sock_release(socket->sk_socket);
+}
+
+
+module_init(ltt_control_init)
+module_exit(ltt_control_exit)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Controller");
+
Index: linux/ltt/ltt-control.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-control.h	2007-08-30 12:39:34.124645000 +0100
@@ -0,0 +1,31 @@
+/* ltt-control.h
+ *
+ * LTT control module over a netlink socket.
+ *
+ * Inspired from Relay Apps, by Tom Zanussi and iptables
+ *
+ * Copyright 2005 -
+ * 	Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#ifndef _LTT_CONTROL_H
+#define _LTT_CONTROL_H
+
+enum trace_op {
+	OP_CREATE,
+	OP_DESTROY,
+	OP_START,
+	OP_STOP,
+	OP_ALIGN,
+	OP_NONE
+};
+
+typedef struct lttctl_peer_msg {
+	char trace_name[NAME_MAX];
+	char trace_type[NAME_MAX];
+	enum trace_op op;
+	union ltt_control_args args;
+} lttctl_peer_msg_t;
+
+#endif //_LTT_CONTROL_H
+
Index: linux/ltt/ltt-core.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-core.c	2007-08-30 12:39:34.130644000 +0100
@@ -0,0 +1,26 @@
+
+/*
+ * LTT core in-kernel infrastructure.
+ *
+ * Copyright 2006 - Mathieu Desnoyers mathieu.desnoyers@polymtl.ca
+ *
+ * Distributed under the GPL license
+ */
+
+#include <linux/ltt-core.h>
+#include <linux/module.h>
+
+/* Traces structures */
+struct ltt_traces ltt_traces = {
+	.head = LIST_HEAD_INIT(ltt_traces.head),
+	.num_active_traces = 0
+};
+
+EXPORT_SYMBOL(ltt_traces);
+
+volatile unsigned int ltt_nesting[NR_CPUS] = { [ 0 ... NR_CPUS-1 ] = 0 } ;
+
+EXPORT_SYMBOL(ltt_nesting);
+
+atomic_t lttng_logical_clock = ATOMIC_INIT(0);
+EXPORT_SYMBOL(lttng_logical_clock);
Index: linux/ltt/ltt-facilities.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-facilities.c	2007-08-30 12:39:34.135644000 +0100
@@ -0,0 +1,344 @@
+/*
+ * ltt-facilities.c
+ *
+ * (C) Copyright	2005 -
+ *		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Contains the kernel core code for Linux Trace Toolkit facilities.
+ *
+ * Facilities are a group of events that can be recorded by instrumentation
+ * points to a trace. We keep track of the active facilities to know which type
+ * of information can be present in a trace.
+ *
+ * We never reuse a facility id.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/ltt-facilities.h>
+#include <linux/spinlock.h>
+#include <linux/string.h>
+#include <linux/bitops.h>
+#include <linux/marker.h>
+#include <linux/ltt-tracer.h>
+
+static struct ltt_facility_info facilities[LTT_MAX_NUM_FACILITIES];
+static spinlock_t facilities_lock;
+
+int ltt_compact_facility_num_events = 0;
+EXPORT_SYMBOL_GPL(ltt_compact_facility_num_events);
+
+/* Facility registration :
+ * hash based on the checksum, except for the core facility, which is 0.
+ * This function assumes that the facility has never been registered before.
+ * User ltt_facility_verify for this. */
+int ltt_facility_register(enum ltt_facility_type type,
+		const char *name,
+		const unsigned int num_events,
+		const u32 checksum,
+		size_t int_size,
+		size_t long_size,
+		size_t pointer_size,
+		size_t size_t_size,
+		size_t alignment)
+{
+	int fac_id;
+	int chk_fac_id;
+	int found=0;
+
+	spin_lock(&facilities_lock);
+	if (type == LTT_FACILITY_TYPE_KERNEL &&
+			strncmp(name, "core", sizeof("core")) == 0) {
+		fac_id = 0;
+	} else {
+		/* fac_id based on checksum%LTT_MAX_NUM_FACILITIES
+		 * find an empty slot */
+		chk_fac_id = fac_id = checksum % LTT_MAX_NUM_FACILITIES;
+		do {
+			if (atomic_read(&facilities[fac_id].ref) == 0) {
+				found = 1;
+				break;
+			}
+			fac_id = (fac_id+1) % LTT_MAX_NUM_FACILITIES;
+		} while (fac_id != chk_fac_id);
+
+		if (!found) {
+			fac_id = -EPERM;
+			goto unlock;
+		}
+	}
+	switch (type) {
+		case LTT_FACILITY_TYPE_USER:
+			if (strncmp(name, "user_", sizeof("user_")-1) != 0) {
+				fac_id = -EPERM;
+				goto unlock;
+			}
+			break;
+		case LTT_FACILITY_TYPE_KERNEL:
+			break;
+	}
+	strncpy(facilities[fac_id].name, name, FACNAME_LEN-1);
+	facilities[fac_id].name[FACNAME_LEN-1] = '\0';
+	facilities[fac_id].type = type;
+	facilities[fac_id].num_events = num_events;
+	facilities[fac_id].alignment = alignment;
+	facilities[fac_id].checksum = checksum;
+	facilities[fac_id].int_size = int_size;
+	facilities[fac_id].long_size = long_size;
+	facilities[fac_id].pointer_size = pointer_size;
+	facilities[fac_id].size_t_size = size_t_size;
+#ifdef CONFIG_LTT_HEARTBEAT
+	if (strcmp(name, "compact") == 0) {
+		ltt_compact_facility_num_events = facilities[fac_id].num_events;
+		ltt_init_compact_facility();
+	}
+#endif
+	if (atomic_read(&facilities[fac_id].ref) == 0) {
+		atomic_add(2, &facilities[fac_id].ref);
+		MARK(core_facility_load, "%s %4b %4b %4b %4b %4b %4b %4b",
+				facilities[fac_id].name,
+				checksum,
+				fac_id,
+				int_size,
+				long_size,
+				pointer_size,
+				size_t_size,
+				alignment);
+	} else {
+		atomic_inc(&facilities[fac_id].ref);
+	}
+unlock:
+	spin_unlock(&facilities_lock);
+	return fac_id;
+}
+
+/* Verifies if a facility is already registered. If it is,
+ * it returns the facility id. If it is not registered, it returns 0. (0 is the
+ * core facility which must never use ltt_facility_verify).
+ * If the facility is already registered, increment its refcount. */
+int ltt_facility_verify(enum ltt_facility_type type,
+		const char *name,
+		const unsigned int num_events,
+		const u32 checksum,
+		size_t int_size,
+		size_t long_size,
+		size_t pointer_size,
+		size_t size_t_size,
+		size_t alignment)
+{
+	int fac_id;
+	int chk_fac_id;
+	int found=0;
+
+	spin_lock(&facilities_lock);
+	if (type == LTT_FACILITY_TYPE_KERNEL &&
+			strncmp(name, "core", sizeof("core")) == 0) {
+		fac_id = 0; /* Core facility */
+		goto unlock;
+	} else {
+		switch (type) {
+			case LTT_FACILITY_TYPE_USER:
+				if (strncmp(name,
+					"user_", sizeof("user_")-1) != 0) {
+					fac_id = 0;
+					goto unlock;
+				}
+				break;
+			case LTT_FACILITY_TYPE_KERNEL:
+				break;
+		}
+		/* fac_id based on checksum%LTT_MAX_NUM_FACILITIES */
+		chk_fac_id = fac_id = checksum % LTT_MAX_NUM_FACILITIES;
+		do {
+			if (facilities[fac_id].checksum == checksum) {
+				/* Possibly found : check carefully */
+				if ((atomic_read(&facilities[fac_id].ref) > 0)
+				&& strncmp(facilities[fac_id].name,
+						name, FACNAME_LEN-1) == 0 &&
+				facilities[fac_id].type == type &&
+				facilities[fac_id].num_events == num_events &&
+				facilities[fac_id].alignment == alignment &&
+				facilities[fac_id].checksum == checksum &&
+				facilities[fac_id].int_size == int_size &&
+				facilities[fac_id].long_size == long_size &&
+				facilities[fac_id].pointer_size
+					== pointer_size &&
+				facilities[fac_id].size_t_size == size_t_size) {
+					found = 1;
+					break;
+				}
+			}
+			fac_id = (fac_id+1) % LTT_MAX_NUM_FACILITIES;
+		} while (fac_id != chk_fac_id);
+
+		if (!found) {
+			fac_id = 0;
+			goto unlock;
+		}
+		atomic_inc(&facilities[fac_id].ref);
+	}
+unlock:
+	spin_unlock(&facilities_lock);
+	return fac_id;
+}
+
+
+unsigned int ltt_facility_kernel_register(struct ltt_facility *facility)
+{
+	size_t alignment;
+#ifdef CONFIG_LTT_ALIGNMENT
+	if (facility->alignment)
+		alignment = sizeof(void*);
+	else
+		alignment = 0;
+#else
+	alignment = 0;
+#endif
+	return ltt_facility_register(LTT_FACILITY_TYPE_KERNEL,
+			facility->name, facility->num_events,
+			facility->checksum,
+			sizeof(int), sizeof(long), sizeof(void*),
+			sizeof(size_t), alignment);
+}
+
+void ltt_facility_ref(uint8_t facility_id)
+{
+	atomic_inc(&facilities[facility_id].ref);
+}
+
+int ltt_facility_unregister(uint8_t facility_id)
+{
+	int ret;
+	int freed = 0;
+
+	spin_lock(&facilities_lock);
+	if (atomic_read(&facilities[facility_id].ref) == 0) {
+		ret = -EPERM;
+		goto unlock;
+	}
+	printk(KERN_DEBUG "LTT unregister facility %hu\n", facility_id);
+	atomic_dec(&facilities[facility_id].ref);
+
+	/* Disable preemption because we read the traces list, and want it to be
+	 * RCU coherent. */
+	preempt_disable();
+	ltt_nesting[smp_processor_id()]++;
+	barrier();
+
+	/* If no more trace in the list, we can free the unused facility,
+	 * otherwise it will be freed later when the last trace is destroyed (by
+	 * ltt_facility_free_unused()). */
+	if (list_empty(&ltt_traces.head))
+		if (atomic_read(&facilities[facility_id].ref) == 1) {
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+			if (strcmp(facilities[facility_id].name,
+				"compact") == 0)
+				ltt_compact_data_shift = 0;
+#endif
+			atomic_dec(&facilities[facility_id].ref);
+			freed = 1;
+		}
+
+	barrier();
+	ltt_nesting[smp_processor_id()]--;
+	preempt_enable();
+
+	/* Ok, if we think about it, it's never going to be traced as there are
+	 * no traces in the list. In fact, we never really want to free a
+	 * facility id when there is tracing active.
+	 * FIXME : this unload could go away. */
+	if (freed)
+		MARK(core_facility_unload, "%4b", facility_id);
+	ret = 0;
+unlock:
+	spin_unlock(&facilities_lock);
+	return ret;
+}
+
+int ltt_facility_user_access_ok(uint8_t fac_id)
+{
+	if (atomic_read(&facilities[fac_id].ref) == 0)
+		return 0;
+	if (facilities[fac_id].type == LTT_FACILITY_TYPE_KERNEL)
+		return 0;
+
+	return 1;
+}
+
+/* Cleanup all the unregistered facilities. This must be done when all the
+ * traces are destroyed.
+ */
+void ltt_facility_free_unused(void)
+{
+	int fac_id;
+
+	spin_lock(&facilities_lock);
+	for (fac_id = 0; fac_id < LTT_MAX_NUM_FACILITIES; fac_id++)
+		if (atomic_read(&facilities[fac_id].ref) == 1) {
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+			if (strcmp(facilities[fac_id].name,
+				"compact") == 0)
+				ltt_compact_data_shift = 0;
+#endif
+			atomic_dec(&facilities[fac_id].ref);
+		}
+	spin_unlock(&facilities_lock);
+}
+
+void ltt_facility_state_dump(struct ltt_trace_struct *trace)
+{
+	int fac_id;
+	struct ltt_facility_info *facility;
+	u32 int_size, long_size, pointer_size, size_t_size;
+
+	int_size = sizeof(int);
+	long_size = sizeof(long);
+	pointer_size = sizeof(void*);
+	size_t_size = sizeof(size_t);
+
+	spin_lock(&facilities_lock);
+	for (fac_id = 0; fac_id < LTT_MAX_NUM_FACILITIES; fac_id++) {
+		if (atomic_read(&facilities[fac_id].ref) > 1) {
+			facility = &facilities[fac_id];
+			printk(KERN_DEBUG "Dumping facility %s\n",
+					facility->name);
+			_MARK(MF_DEFAULT | LF_TRACE,
+				core_state_dump_facility_load,
+				"%s %4b %4b %4b %4b %4b %4b %4b",
+				trace,
+				facility->name,
+				facility->checksum,
+				fac_id,
+				facility->int_size,
+				facility->long_size,
+				facility->pointer_size,
+				facility->size_t_size,
+				facility->alignment);
+		}
+	}
+	spin_unlock(&facilities_lock);
+}
+
+EXPORT_SYMBOL(ltt_facility_kernel_register);
+EXPORT_SYMBOL(ltt_facility_ref);
+EXPORT_SYMBOL(ltt_facility_unregister);
+EXPORT_SYMBOL(ltt_facility_free_unused);
+EXPORT_SYMBOL(ltt_facility_state_dump);
+
+static int __init ltt_facilities_init(void)
+{
+	int i;
+	printk(KERN_INFO "LTT : ltt-facilities init\n");
+
+	spin_lock_init(&facilities_lock);
+	for (i = 0; i < LTT_MAX_NUM_FACILITIES; i++) {
+		atomic_set(&facilities[i].ref, 0);
+	}
+
+	return 0;
+}
+__initcall(ltt_facilities_init);
Index: linux/ltt/ltt-heartbeat.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-heartbeat.c	2007-08-30 12:39:34.142646000 +0100
@@ -0,0 +1,421 @@
+/*
+ * ltt-heartbeat.c
+ *
+ * (C) Copyright	2006 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * notes : heartbeat timer cannot be used for early tracing in the boot process,
+ * as it depends on timer interrupts.
+ *
+ * The timer needs to be only on one CPU to support hotplug.
+ * We have the choice between schedule_delayed_work_on and an IPI to get each
+ * CPU to write the heartbeat. IPI have been chosen because it is considered
+ * faster than passing through the timer to get the work scheduled on all the
+ * CPUs.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/ltt-core.h>
+#include <linux/workqueue.h>
+#include <linux/cpu.h>
+#include <linux/timex.h>
+#include <linux/bitops.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+#define BITS_OF_COMPACT_DATA		11
+
+/* Expected minimum probe duration, in cycles */
+#define MIN_PROBE_DURATION		400
+/* Expected maximum interrupt latency in ms : 15ms, *2 for security */
+#define EXPECTED_INTERRUPT_LATENCY	30
+
+static struct timer_list heartbeat_timer;
+static unsigned int precalc_heartbeat_expire = 0;
+
+int ltt_compact_data_shift = 0;
+EXPORT_SYMBOL_GPL(ltt_compact_data_shift);
+
+int ltt_tsc_lsb_truncate = 0;	/* Number of LSB removed from compact tsc */
+EXPORT_SYMBOL_GPL(ltt_tsc_lsb_truncate);
+int ltt_tscbits = 0;		/* 32 - tsc_lsb_truncate - tsc_msb_cutoff */
+EXPORT_SYMBOL_GPL(ltt_tscbits);
+
+#ifdef CONFIG_LTT_SYNTHETIC_TSC
+/* For architectures with 32 bits TSC */
+static struct synthetic_tsc_struct {
+	u32 tsc[2][2];	/* a pair of 2 32 bits. [0] is the MSB, [1] is LSB */
+	unsigned int index;	/* Index of the current synth. tsc. */
+} ____cacheline_aligned synthetic_tsc[NR_CPUS];
+
+static void ltt_heartbeat_init_synthetic_cpu_tsc(int cpu)
+{
+	synthetic_tsc[cpu].tsc[0][0] = 0;
+	synthetic_tsc[cpu].tsc[0][1] = 0;
+	synthetic_tsc[cpu].tsc[1][0] = 0;
+	synthetic_tsc[cpu].tsc[1][1] = 0;
+	synthetic_tsc[cpu].index = 0;
+}
+
+/* Called from one CPU, before any tracing starts, to init each structure */
+static void ltt_heartbeat_init_synthetic_tsc(void)
+{
+	int cpu;
+	for_each_possible_cpu(cpu)
+		ltt_heartbeat_init_synthetic_cpu_tsc(cpu);
+	smp_wmb();
+}
+
+/* Called from heartbeat IPI : either in interrupt or process context */
+static void ltt_heartbeat_update_synthetic_tsc(void)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	u32 tsc;
+
+	preempt_disable();
+	cpu_synth = &synthetic_tsc[smp_processor_id()];
+	tsc = (u32)get_cycles();	/* We deal with a 32 LSB TSC */
+
+	if (tsc < cpu_synth->tsc[cpu_synth->index][1]) {
+		unsigned int new_index = cpu_synth->index ? 0 : 1; /* 0 <-> 1 */
+		/* Overflow */
+		/* Non atomic update of the non current synthetic TSC, followed
+		 * by an atomic index change. There is no write concurrency,
+		 * so the index read/write does not need to be atomic. */
+		cpu_synth->tsc[new_index][1] = tsc; /* LSB update */
+		cpu_synth->tsc[new_index][0] =
+			cpu_synth->tsc[cpu_synth->index][0]+1; /* MSB update */
+		cpu_synth->index = new_index;	/* atomic change of index */
+	} else {
+		/* No overflow : we can simply update the 32 LSB of the current
+		 * synthetic TSC as it's an atomic write. */
+		cpu_synth->tsc[cpu_synth->index][1] = tsc;
+	}
+	preempt_enable();
+}
+
+/* Called from buffer switch : in _any_ context (even NMI) */
+u64 ltt_heartbeat_read_synthetic_tsc(void)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	u64 ret;
+	unsigned int index;
+	u32 tsc;
+
+	preempt_disable();
+	cpu_synth = &synthetic_tsc[smp_processor_id()];
+	index = cpu_synth->index; /* atomic read */
+	tsc = (u32)get_cycles();	/* We deal with a 32 LSB TSC */
+
+	if (tsc < cpu_synth->tsc[index][1]) {
+		/* Overflow */
+		ret = ((u64)(cpu_synth->tsc[index][0]+1) << 32) | ((u64)tsc);
+	} else {
+		/* no overflow */
+		ret = ((u64)cpu_synth->tsc[index][0] << 32) | ((u64)tsc);
+	}
+	preempt_enable();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_heartbeat_read_synthetic_tsc);
+#endif //CONFIG_LTT_SYNTHETIC_TSC
+
+
+static void heartbeat_ipi(void *info)
+{
+#ifdef CONFIG_LTT_SYNTHETIC_TSC
+	ltt_heartbeat_update_synthetic_tsc();
+#endif //CONFIG_LTT_SYNTHETIC_TSC
+
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	/* Log a heartbeat event for each trace, each tracefile */
+	_MARK(MF_DEFAULT | LF_CHANNEL,
+		core_time_heartbeat,
+		MARK_NOARGS,
+		GET_CHANNEL_INDEX(facilities));
+	_MARK(MF_DEFAULT | LF_CHANNEL,
+		core_time_heartbeat,
+		MARK_NOARGS,
+		GET_CHANNEL_INDEX(interrupts));
+	_MARK(MF_DEFAULT | LF_CHANNEL,
+		core_time_heartbeat,
+		MARK_NOARGS,
+		GET_CHANNEL_INDEX(processes));
+	_MARK(MF_DEFAULT | LF_CHANNEL,
+		core_time_heartbeat,
+		MARK_NOARGS,
+		GET_CHANNEL_INDEX(modules));
+	_MARK(MF_DEFAULT | LF_CHANNEL,
+		core_time_heartbeat,
+		MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu));
+	_MARK(MF_DEFAULT | LF_CHANNEL,
+		core_time_heartbeat,
+		MARK_NOARGS,
+		GET_CHANNEL_INDEX(network));
+	_MARK(MF_DEFAULT | LF_COMPACT,
+		compact_time_heartbeat,
+		MARK_NOARGS);
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+}
+
+static void heartbeat_ipi_full(void *info)
+{
+#ifdef CONFIG_LTT_SYNTHETIC_TSC
+	ltt_heartbeat_update_synthetic_tsc();
+#endif //CONFIG_LTT_SYNTHETIC_TSC
+
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	/* Log a heartbeat event for each trace, each tracefile */
+	_MARK(MF_DEFAULT | LF_CHANNEL | LF_FORCE,
+		core_time_heartbeat_full,
+		"%8b",
+		GET_CHANNEL_INDEX(facilities),
+		ltt_get_timestamp64());
+	_MARK(MF_DEFAULT | LF_CHANNEL | LF_FORCE,
+		core_time_heartbeat_full,
+		"%8b",
+		GET_CHANNEL_INDEX(interrupts),
+		ltt_get_timestamp64());
+	_MARK(MF_DEFAULT | LF_CHANNEL | LF_FORCE,
+		core_time_heartbeat_full,
+		"%8b",
+		GET_CHANNEL_INDEX(processes),
+		ltt_get_timestamp64());
+	_MARK(MF_DEFAULT | LF_CHANNEL | LF_FORCE,
+		core_time_heartbeat_full,
+		"%8b",
+		GET_CHANNEL_INDEX(modules),
+		ltt_get_timestamp64());
+	_MARK(MF_DEFAULT | LF_CHANNEL | LF_FORCE,
+		core_time_heartbeat_full,
+		"%8b",
+		GET_CHANNEL_INDEX(cpu),
+		ltt_get_timestamp64());
+	_MARK(MF_DEFAULT | LF_CHANNEL | LF_FORCE,
+		core_time_heartbeat_full,
+		"%8b",
+		GET_CHANNEL_INDEX(network),
+		ltt_get_timestamp64());
+	_MARK(MF_DEFAULT | LF_COMPACT | LF_FORCE,
+		compact_time_heartbeat_full,
+		"%8b",
+		ltt_get_timestamp64());
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+}
+
+/* Write the full TSC in the traces. To be called when we cannot assume that the heartbeat events
+ * will keep a trace buffer synchronized. (missing timer events, tracing starts, tracing restarts)
+ */
+void ltt_write_full_tsc(void)
+{
+	on_each_cpu(heartbeat_ipi_full, NULL, 1, 0);
+}
+EXPORT_SYMBOL_GPL(ltt_write_full_tsc);
+
+/* We need to be in process context to do an IPI */
+static void heartbeat_work(struct work_struct *work)
+{
+	on_each_cpu(heartbeat_ipi, NULL, 1, 0);
+}
+static DECLARE_WORK(hb_work, heartbeat_work, NULL);
+
+/**
+ * heartbeat_timer : - Timer function generating hearbeat.
+ * @data: unused
+ *
+ * Guarantees at least 1 execution of heartbeat before low word of TSC wraps.
+ */
+static void heartbeat_timer_fct(unsigned long data)
+{
+	PREPARE_WORK(&hb_work, heartbeat_work, NULL);
+	schedule_work(&hb_work);
+
+	mod_timer(&heartbeat_timer, jiffies + precalc_heartbeat_expire);
+}
+
+
+/**
+ * init_heartbeat_timer: - Start timer generating hearbeat events.
+ */
+static void init_heartbeat_timer(void)
+{
+	int tsc_msb_cutoff;
+	int data_bits = BITS_OF_COMPACT_DATA;
+	int max_tsc_msb_cutoff;
+	unsigned long mask;
+
+	if (loops_per_jiffy > 0) {
+		printk(KERN_DEBUG "LTT : ltt-heartbeat init\n");
+		printk("Requested number of bits %d\n", data_bits);
+
+		ltt_tsc_lsb_truncate = max(0,
+			(int)get_count_order(MIN_PROBE_DURATION)-1);
+		max_tsc_msb_cutoff =
+			32 - 1 - get_count_order(((1UL << 1)
+			+ (EXPECTED_INTERRUPT_LATENCY*HZ/1000)
+			+ LTT_PERCPU_TIMER_INTERVAL + 1)
+			* (loops_per_jiffy << 1));
+		printk("Available number of bits %d\n",
+			ltt_tsc_lsb_truncate + max_tsc_msb_cutoff);
+		if (ltt_tsc_lsb_truncate + max_tsc_msb_cutoff <
+			data_bits) {
+			printk("Number of bits truncated to %d\n",
+				ltt_tsc_lsb_truncate + max_tsc_msb_cutoff);
+			data_bits = ltt_tsc_lsb_truncate + max_tsc_msb_cutoff;
+		}
+
+		tsc_msb_cutoff = data_bits - ltt_tsc_lsb_truncate;
+
+		if (tsc_msb_cutoff > 0)
+			mask = (1UL<<(32-tsc_msb_cutoff))-1;
+		else
+			mask = 0xFFFFFFFFUL;
+		precalc_heartbeat_expire =
+			(mask/(loops_per_jiffy << 1)
+				- 1 - LTT_PERCPU_TIMER_INTERVAL
+				- (EXPECTED_INTERRUPT_LATENCY*HZ/1000)) >> 1;
+		WARN_ON(precalc_heartbeat_expire == 0);
+		printk("Heartbeat timer will fire each %u jiffies.\n",
+			precalc_heartbeat_expire);
+
+		//precalc_heartbeat_expire = ( 0xFFFFFFFFUL/(loops_per_jiffy << 1)
+		//	- 1 - LTT_PERCPU_TIMER_INTERVAL) >> 1;
+
+		//tsc_msb_cutoff = 32 - 1 -
+		//	get_count_order(( (EXPECTED_INTERRUPT_LATENCY
+		//		+ (precalc_heartbeat_expire * 1000 / HZ))
+		//		* cpu_khz ));
+		ltt_tscbits = 32 - ltt_tsc_lsb_truncate - tsc_msb_cutoff;
+		printk("Compact TSC init : truncate %d lsb, cutoff %d msb.\n",
+			ltt_tsc_lsb_truncate, tsc_msb_cutoff);
+	} else
+		printk(KERN_WARNING
+			"LTT: no tsc for heartbeat timer "
+			"- continuing without one \n");
+}
+
+/* ltt_init_compact_facility reserves the number of bits to identify the event
+ * numbers in the compact headers. It must be called every time the compact
+ * facility is changed. */
+void ltt_init_compact_facility(void)
+{
+	if (loops_per_jiffy > 0) {
+		ltt_compact_data_shift =
+			get_count_order(ltt_compact_facility_num_events)
+						+ ltt_tscbits;
+		printk("Data shifted from %d bits\n", ltt_compact_data_shift);
+
+		printk("%d bits used for event IDs, %d available for data.\n",
+			get_count_order(ltt_compact_facility_num_events),
+			32 - ltt_compact_data_shift);
+	} else
+		printk(KERN_WARNING
+			"LTT: no tsc for heartbeat timer "
+			"- continuing without one \n");
+}
+EXPORT_SYMBOL_GPL(ltt_init_compact_facility);
+
+
+static void start_heartbeat_timer(void)
+{
+	if (precalc_heartbeat_expire > 0) {
+		printk(KERN_DEBUG "LTT : ltt-heartbeat start\n");
+
+		init_timer(&heartbeat_timer);
+		heartbeat_timer.function = heartbeat_timer_fct;
+		heartbeat_timer.expires = jiffies + precalc_heartbeat_expire;
+		add_timer(&heartbeat_timer);
+	} else
+		printk(KERN_WARNING
+			"LTT: no tsc for heartbeat timer "
+			"- continuing without one \n");
+}
+
+/**
+ * stop_heartbeat_timer: - Stop timer generating hearbeat events.
+ */
+static void stop_heartbeat_timer(void)
+{
+ 	if (loops_per_jiffy > 0) {
+		printk(KERN_DEBUG "LTT : ltt-heartbeat stop\n");
+		del_timer(&heartbeat_timer);
+	}
+}
+
+#ifdef CONFIG_LTT_SYNTHETIC_TSC
+/**
+ * 	heartbeat_hotcpu_callback - CPU hotplug callback
+ * 	@nb: notifier block
+ * 	@action: hotplug action to take
+ * 	@hcpu: CPU number
+ *
+ *	Sets the new CPU's current synthetic TSC to the same value as the
+ *	currently running CPU.
+ *
+ * 	Returns the success/failure of the operation. (NOTIFY_OK, NOTIFY_BAD)
+ */
+static int __cpuinit heartbeat_hotcpu_callback(struct notifier_block *nb,
+				unsigned long action,
+				void *hcpu)
+{
+	unsigned int hotcpu = (unsigned long)hcpu;
+	struct synthetic_tsc_struct *cpu_synth;
+	u64 local_count;
+
+	switch(action) {
+	case CPU_UP_PREPARE:
+		ltt_heartbeat_init_synthetic_cpu_tsc(hotcpu);
+		cpu_synth = &synthetic_tsc[hotcpu];
+		local_count = ltt_heartbeat_read_synthetic_tsc();
+		cpu_synth->tsc[0][1] = (u32)local_count; /* LSB */
+		cpu_synth->tsc[0][0] = (u32)(local_count >> 32); /* MSB */
+		smp_wmb();
+		break;
+	case CPU_ONLINE:
+		/* FIXME : heartbeat events are currently broken with CPU
+		 * hotplug : events can be recorded before heartbeat, heartbeat
+		 * too far from trace start and are broken with trace
+		 * stop/start as well.
+		 */
+		/* As we are preemptible, make sure it runs on the right cpu */
+		smp_call_function_single(hotcpu, heartbeat_ipi, NULL, 1, 0);
+		break;
+	}
+	return NOTIFY_OK;
+}
+#endif //CONFIG_LTT_SYNTHETIC_TSC
+
+int ltt_heartbeat_trigger(enum ltt_heartbeat_functor_msg msg)
+{
+	printk(KERN_DEBUG "LTT : ltt-heartbeat trigger\n");
+	switch (msg) {
+		case LTT_HEARTBEAT_START:
+			start_heartbeat_timer();
+			break;
+		case LTT_HEARTBEAT_STOP:
+			stop_heartbeat_timer();
+			break;
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL_GPL(ltt_heartbeat_trigger);
+
+static int __init ltt_heartbeat_init(void)
+{
+	printk(KERN_INFO "LTT : ltt-heartbeat init\n");
+#ifdef CONFIG_LTT_SYNTHETIC_TSC
+	/* higher priority than relay */
+	hotcpu_notifier(heartbeat_hotcpu_callback, 1);
+	ltt_heartbeat_init_synthetic_tsc();
+#endif //CONFIG_LTT_SYNTHETIC_TSC
+	init_heartbeat_timer();
+	return 0;
+}
+
+__initcall(ltt_heartbeat_init);
Index: linux/ltt/ltt-relay.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-relay.c	2007-08-30 12:39:34.153645000 +0100
@@ -0,0 +1,1285 @@
+/*
+ * ltt-relay.c
+ *
+ * (C) Copyright 2005-2006 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Contains the kernel code for the Linux Trace Toolkit.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *	Karim Yaghmour (karim@opersys.com)
+ *	Tom Zanussi (zanussi@us.ibm.com)
+ *	Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  19/10/05, Complete lockless mechanism. (Mathieu Desnoyers)
+ *	27/05/05, Modular redesign and rewrite. (Mathieu Desnoyers)
+
+ * Comments :
+ * num_active_traces protects the functors. Changing the pointer is an atomic
+ * operation, but the functions can only be called when in tracing. It is then
+ * safe to unload a module in which sits a functor when no tracing is active.
+ *
+ * filter_control functor is protected by incrementing its module refcount.
+ *
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/ltt-facilities.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+
+static struct dentry *ltt_root_dentry;
+static struct file_operations ltt_file_operations;
+
+/* How a force_switch must be done ?
+ *
+ * Is it done during tracing or as a final flush after tracing
+ * (so it won't write in the new sub-buffer).
+ */
+enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+	struct ltt_channel_struct *ltt_chan,
+	struct rchan_buf *buf,
+	unsigned int cpu,
+	unsigned n_subbufs);
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu);
+
+/* Trace callbacks */
+
+static void ltt_buffer_begin_callback(struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct*)buf->chan->private_data;
+	struct ltt_block_start_header *header =
+		(struct ltt_block_start_header*)
+			(buf->start + (subbuf_idx*buf->chan->subbuf_size));
+
+	header->begin.cycle_count = tsc;
+	header->begin.freq = ltt_frequency();
+	header->lost_size = 0xFFFFFFFF; // for debugging...
+	header->buf_size = buf->chan->subbuf_size;
+	ltt_write_trace_header(channel->trace, &header->trace);
+}
+
+static void ltt_buffer_end_callback(struct rchan_buf *buf,
+			u64 tsc, unsigned int offset, unsigned int subbuf_idx)
+{
+	struct ltt_block_start_header *header =
+		(struct ltt_block_start_header*)
+			(buf->start + (subbuf_idx*buf->chan->subbuf_size));
+
+	/* offset is assumed to never be 0 here : never deliver a completely
+	 * empty subbuffer.
+	 * The lost size is between 0 and subbuf_size-1 */
+	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
+				buf->chan);
+	header->end.cycle_count = tsc;
+	header->end.freq = ltt_frequency();
+}
+
+static int ltt_subbuf_start_callback(struct rchan_buf *buf,
+				void *subbuf,
+				void *prev_subbuf,
+				size_t prev_padding)
+{
+	return 0;
+}
+
+
+
+static void ltt_deliver(struct rchan_buf *buf,
+		unsigned subbuf_idx,
+		void *subbuf)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct*)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &channel->buf[buf->cpu];
+
+	atomic_set(&ltt_buf->wakeup_readers, 1);
+}
+
+static void ltt_buf_mapped_callback(struct rchan_buf *buf,
+		struct file *filp)
+{
+}
+
+static void ltt_buf_unmapped_callback(struct rchan_buf *buf,
+		struct file *filp)
+{
+}
+
+static struct dentry *ltt_create_buf_file_callback(const char *filename,
+				  struct dentry *parent,
+				  int mode,
+				  struct rchan_buf *buf,
+				  int *is_global)
+{
+	struct ltt_channel_struct *ltt_chan;
+	int err;
+	struct dentry *dentry;
+
+	ltt_chan = buf->chan->private_data;
+	err = ltt_relay_create_buffer(ltt_chan->trace, ltt_chan,
+					buf, buf->cpu,
+					buf->chan->n_subbufs);
+	if(err)
+		return ERR_PTR(err);
+
+	dentry = debugfs_create_file(filename, mode, parent, buf,
+			&ltt_file_operations);
+	if(!dentry)
+		goto error;
+	return dentry;
+error:
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+	return NULL;
+}
+
+static int ltt_remove_buf_file_callback(struct dentry *dentry)
+{
+        struct rchan_buf *buf = dentry->d_inode->u.generic_ip;
+	struct ltt_channel_struct *ltt_chan = buf->chan->private_data;
+
+	debugfs_remove(dentry);
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+
+	return 0;
+}
+
+/* This function should not be called from NMI interrupt context */
+static void ltt_buf_unfull(struct rchan_buf *buf,
+		unsigned subbuf_idx,
+		void *subbuf)
+{
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct*)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	if (waitqueue_active(&ltt_buf->write_wait))
+		schedule_work(&ltt_buf->wake_writers);
+}
+
+
+/**
+ *	ltt_poll - poll file op for ltt files
+ *	@filp: the file
+ *	@wait: poll table
+ *
+ *	Poll implemention.
+ */
+static unsigned int ltt_poll(struct file *filp, poll_table *wait)
+{
+	unsigned int mask = 0;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct rchan_buf *buf = inode->u.generic_ip;
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct*)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+
+	//printk(KERN_DEBUG "DEBUG : in LTT poll %p\n", filp);
+	if (filp->f_mode & FMODE_READ) {
+		poll_wait(filp, &buf->read_wait, wait);
+
+		if (atomic_long_read(&ltt_buf->active_readers) != 0) {
+			return 0;
+		} else {
+			if (SUBBUF_TRUNC(
+				local_read(&ltt_buf->offset), buf->chan)
+			- SUBBUF_TRUNC(
+				atomic_long_read(&ltt_buf->consumed), buf->chan)
+			== 0) {
+				if (buf->finalized) return POLLHUP;
+				else return 0;
+			} else {
+				struct rchan *rchan =
+					ltt_channel->trans_channel_data;
+				if (SUBBUF_TRUNC(local_read(&ltt_buf->offset),
+						buf->chan)
+			- SUBBUF_TRUNC(atomic_long_read(&ltt_buf->consumed),
+							buf->chan)
+				>= rchan->alloc_size)
+					return POLLPRI | POLLRDBAND;
+				else
+					return POLLIN | POLLRDNORM;
+			}
+		}
+	}
+	return mask;
+}
+
+
+/**
+ *	ltt_ioctl - ioctl control on the debugfs file
+ *
+ *	@inode: the inode
+ *	@filp: the file
+ *	@cmd: the command
+ *	@arg: command arg
+ *
+ *	This ioctl implements three commands necessary for a minimal
+ *	producer/consumer implementation :
+ *	RELAY_GET_SUBBUF
+ *		Get the next sub buffer that can be read. It never blocks.
+ *	RELAY_PUT_SUBBUF
+ *		Release the currently read sub-buffer. Parameter is the last
+ *		put subbuffer (returned by GET_SUBBUF).
+ *	RELAY_GET_N_BUBBUFS
+ *		returns the number of sub buffers in the per cpu channel.
+ *	RELAY_GET_SUBBUF_SIZE
+ *		returns the size of the sub buffers.
+ *
+ */
+static int ltt_ioctl(struct inode *inode, struct file *filp,
+		unsigned int cmd, unsigned long arg)
+{
+	struct rchan_buf *buf = inode->u.generic_ip;
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct*)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	u32 __user *argp = (u32 __user *)arg;
+
+	switch (cmd) {
+		case RELAY_GET_SUBBUF:
+		{
+			long consumed_old, consumed_idx;
+			atomic_long_inc(&ltt_buf->active_readers);
+			consumed_old = atomic_long_read(&ltt_buf->consumed);
+			consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+			if (SUBBUF_OFFSET(
+				local_read(
+					&ltt_buf->commit_count[consumed_idx]),
+				buf->chan) != 0) {
+				atomic_long_dec(&ltt_buf->active_readers);
+				return -EAGAIN;
+			}
+			if ((SUBBUF_TRUNC(
+				local_read(&ltt_buf->offset), buf->chan)
+			- SUBBUF_TRUNC(consumed_old, buf->chan))
+			== 0) {
+				atomic_long_dec(&ltt_buf->active_readers);
+				return -EAGAIN;
+			}
+			smp_rmb();
+			//printk(KERN_DEBUG "LTT ioctl get subbuf %d\n",
+			//		consumed_old);
+			return put_user((u32)consumed_old, argp);
+			break;
+		}
+		case RELAY_PUT_SUBBUF:
+		{
+			u32 uconsumed_old;
+			int ret;
+			long consumed_new, consumed_old;
+
+			ret = get_user(uconsumed_old, argp);
+			if (ret)
+				return ret; /* will return -EFAULT */
+
+			//printk(KERN_DEBUG "LTT ioctl put subbuf %d\n",
+			//		consumed_old);
+			consumed_old = atomic_long_read(&ltt_buf->consumed);
+			consumed_old = consumed_old & (~0xFFFFFFFFL);
+			consumed_old = consumed_old | uconsumed_old;
+			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+
+			spin_lock(&ltt_buf->full_lock);
+			if (atomic_long_cmpxchg(
+				&ltt_buf->consumed, consumed_old, consumed_new)
+					!= consumed_old) {
+				/* We have been pushed by the writer : the last
+				 * buffer read _is_ corrupted! It can also
+				 * happen if this is a buffer we never got. */
+				atomic_long_dec(&ltt_buf->active_readers);
+				spin_unlock(&ltt_buf->full_lock);
+				return -EIO;
+			} else {
+				/* tell the client that buffer is now unfull */
+				int index;
+				void *data;
+				index = SUBBUF_INDEX(consumed_old, buf->chan);
+				data = buf->start +
+					BUFFER_OFFSET(consumed_old, buf->chan);
+				ltt_buf_unfull(buf, index, data);
+				atomic_long_dec(&ltt_buf->active_readers);
+				spin_unlock(&ltt_buf->full_lock);
+			}
+			break;
+		}
+		case RELAY_GET_N_SUBBUFS:
+			//printk(KERN_DEBUG "LTT ioctl get n subbufs\n");
+			return put_user((u32)buf->chan->n_subbufs, argp);
+			break;
+		case RELAY_GET_SUBBUF_SIZE:
+			//printk(KERN_DEBUG "LTT ioctl get subbuf size\n");
+			return put_user((u32)buf->chan->subbuf_size, argp);
+			break;
+		default:
+			return -ENOIOCTLCMD;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+
+static long ltt_compat_ioctl(struct file *file, unsigned cmd, unsigned long arg)
+{
+	long ret = -ENOIOCTLCMD;
+
+	lock_kernel();
+	ret = ltt_ioctl(file->f_dentry->d_inode, file, cmd, arg);
+	unlock_kernel();
+
+	return ret;
+}
+
+#endif //CONFIG_COMPAT
+
+static void ltt_relay_print_subbuffer_errors(
+	struct ltt_channel_struct *ltt_chan,
+	long cons_off, unsigned int i)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	long cons_idx;
+
+	printk(KERN_WARNING
+		"LTT : unread channel %s offset is %ld "
+		"and cons_off : %ld (cpu %u)\n",
+		ltt_chan->channel_name,
+		local_read(&ltt_chan->buf[i].offset), cons_off, i);
+	/* Check each sub-buffer for non zero commit count */
+	cons_idx = SUBBUF_INDEX(cons_off, rchan);
+	if (SUBBUF_OFFSET(local_read(&ltt_chan->buf[i].commit_count[cons_idx]),
+				rchan))
+		printk(KERN_ALERT
+			"LTT : %s : subbuffer %lu has non zero "
+			"commit count.\n",
+			ltt_chan->channel_name, cons_idx);
+	printk(KERN_ALERT "LTT : %s : commit count : %lu, subbuf size %zd\n",
+			ltt_chan->channel_name,
+			local_read(&ltt_chan->buf[i].commit_count[cons_idx]),
+			rchan->subbuf_size);
+}
+
+static void ltt_relay_print_errors(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan, int cpu)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	long cons_off;
+
+	for (cons_off = atomic_long_read(&ltt_chan->buf[cpu].consumed);
+		(SUBBUF_TRUNC(local_read(&ltt_chan->buf[cpu].offset),
+				rchan)
+			- cons_off) > 0;
+		cons_off = SUBBUF_ALIGN(cons_off, rchan)) {
+		ltt_relay_print_subbuffer_errors(ltt_chan, cons_off, cpu);
+	}
+}
+
+static void ltt_relay_print_buffer_errors(struct ltt_channel_struct *ltt_chan,
+			unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+
+	if (local_read(&ltt_chan->buf[cpu].events_lost))
+		printk(KERN_ALERT
+			"LTT : %s : %ld events lost "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			local_read(&ltt_chan->buf[cpu].events_lost),
+			ltt_chan->channel_name, cpu);
+	if (local_read(&ltt_chan->buf[cpu].corrupted_subbuffers))
+		printk(KERN_ALERT
+			"LTT : %s : %ld corrupted subbuffers "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			local_read(
+				&ltt_chan->buf[cpu].corrupted_subbuffers),
+			ltt_chan->channel_name, cpu);
+
+	ltt_relay_print_errors(trace, ltt_chan, cpu);
+}
+
+static void ltt_relay_remove_dirs(struct ltt_trace_struct *trace)
+{
+	debugfs_remove(trace->dentry.control_root);
+	debugfs_remove(trace->dentry.trace_root);
+}
+
+static void ltt_relay_release_channel(struct kref *kref)
+{
+	struct ltt_channel_struct *ltt_chan = container_of(kref,
+			struct ltt_channel_struct, kref);
+	kfree(ltt_chan);
+}
+
+/* Create ltt buffer.
+ */
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+	struct ltt_channel_struct *ltt_chan,
+	struct rchan_buf *buf,
+	unsigned int cpu,
+	unsigned n_subbufs)
+{
+	unsigned int j;
+
+	ltt_chan->buf[cpu].commit_count =
+		kmalloc(sizeof(local_t) * n_subbufs, GFP_KERNEL);
+	if (!ltt_chan->buf[cpu].commit_count)
+		return -ENOMEM;
+	kref_get(&trace->kref);
+	kref_get(&trace->ltt_transport_kref);
+	kref_get(&ltt_chan->kref);
+	local_set(&ltt_chan->buf[cpu].offset,
+		ltt_subbuf_header_len());
+	atomic_long_set(&ltt_chan->buf[cpu].consumed, 0);
+	atomic_long_set(&ltt_chan->buf[cpu].active_readers, 0);
+	for (j = 0; j < n_subbufs; j++)
+		local_set(&ltt_chan->buf[cpu].commit_count[j], 0);
+	init_waitqueue_head(&ltt_chan->buf[cpu].write_wait);
+	atomic_set(&ltt_chan->buf[cpu].wakeup_readers, 0);
+
+	INIT_WORK(&ltt_chan->buf[cpu].wake_writers, ltt_wakeup_writers,
+		  &ltt_chan->buf[cpu]);
+
+	spin_lock_init(&ltt_chan->buf[cpu].full_lock);
+
+	ltt_buffer_begin_callback(buf, trace->start_tsc, 0);
+	/* atomic_add made on local variable on data that belongs to
+	 * various CPUs : ok because tracing not started (for this cpu). */
+	local_add(ltt_subbuf_header_len(),
+		&ltt_chan->buf[cpu].commit_count[0]);
+
+	local_set(&ltt_chan->buf[cpu].events_lost, 0);
+	local_set(&ltt_chan->buf[cpu].corrupted_subbuffers, 0);
+
+	return 0;
+}
+
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+					unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+
+	kref_put(&ltt_chan->trace->ltt_transport_kref,
+		ltt_release_transport);
+	ltt_relay_print_buffer_errors(ltt_chan, cpu);
+	kfree(ltt_chan->buf[cpu].commit_count);
+	ltt_chan->buf[cpu].commit_count = NULL;
+	kref_put(&ltt_chan->kref, ltt_relay_release_channel);
+	kref_put(&trace->kref, ltt_release_trace);
+}
+
+/* Create channel.
+ */
+static int ltt_relay_create_channel(char *trace_name,
+		struct ltt_trace_struct *trace,
+		struct dentry *dir,
+		char *channel_name,
+		struct ltt_channel_struct **ltt_chan,
+		unsigned int subbuf_size, unsigned int n_subbufs,
+		int overwrite)
+{
+	char *tmpname;
+	unsigned int tmpname_len;
+	int err = 0;
+
+	tmpname = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (!tmpname)
+		return EPERM;
+	if (overwrite) {
+		strncpy(tmpname, LTT_FLIGHT_PREFIX, PATH_MAX-1);
+		strncat(tmpname, channel_name,
+			PATH_MAX-1-sizeof(LTT_FLIGHT_PREFIX));
+	} else {
+		strncpy(tmpname, channel_name, PATH_MAX-1);
+	}
+	strncat(tmpname, "_", PATH_MAX-1-strlen(tmpname));
+
+	*ltt_chan = kzalloc(sizeof(struct ltt_channel_struct), GFP_KERNEL);
+	if (!(*ltt_chan))
+		goto ltt_chan_alloc_error;
+	kref_init(&(*ltt_chan)->kref);
+
+	(*ltt_chan)->trace = trace;
+	(*ltt_chan)->buffer_begin = ltt_buffer_begin_callback;
+	(*ltt_chan)->buffer_end = ltt_buffer_end_callback;
+	(*ltt_chan)->overwrite = overwrite;
+	if (strcmp(channel_name, LTT_COMPACT_CHANNEL) == 0)
+		(*ltt_chan)->compact = 1;
+	else
+		(*ltt_chan)->compact = 0;
+	(*ltt_chan)->trans_channel_data = relay_open(tmpname,
+			dir,
+			subbuf_size,
+			n_subbufs,
+			&trace->callbacks, *ltt_chan);
+	tmpname_len = strlen(tmpname);
+	if (tmpname_len > 0) {
+		/* Remove final _ for pretty printing */
+		tmpname[tmpname_len-1] = '\0';
+	}
+	if ((*ltt_chan)->trans_channel_data == NULL) {
+		printk(KERN_ERR "LTT : Can't open %s channel for trace %s\n",
+				tmpname, trace_name);
+		goto relay_open_error;
+	}
+
+	strncpy((*ltt_chan)->channel_name, tmpname, PATH_MAX-1);
+
+	err = 0;
+	goto end;
+
+relay_open_error:
+	kfree(*ltt_chan);
+	*ltt_chan = NULL;
+ltt_chan_alloc_error:
+	err = EPERM;
+end:
+	kfree(tmpname);
+	return err;
+}
+
+static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
+{
+	new_trace->dentry.trace_root = debugfs_create_dir(new_trace->trace_name,
+			ltt_root_dentry);
+	if (new_trace->dentry.trace_root == NULL) {
+		printk(KERN_ERR "LTT : Trace directory name %s already taken\n",
+				new_trace->trace_name);
+		return EEXIST;
+	}
+
+	new_trace->dentry.control_root = debugfs_create_dir(LTT_CONTROL_ROOT,
+			new_trace->dentry.trace_root);
+	if (new_trace->dentry.control_root == NULL) {
+		printk(KERN_ERR "LTT : Trace control subdirectory name "\
+				"%s/%s already taken\n",
+				new_trace->trace_name, LTT_CONTROL_ROOT);
+		debugfs_remove(new_trace->dentry.trace_root);
+		return EEXIST;
+	}
+
+	new_trace->callbacks.subbuf_start = ltt_subbuf_start_callback;
+	new_trace->callbacks.buf_mapped = ltt_buf_mapped_callback;
+	new_trace->callbacks.buf_unmapped = ltt_buf_unmapped_callback;
+	new_trace->callbacks.create_buf_file = ltt_create_buf_file_callback;
+	new_trace->callbacks.remove_buf_file = ltt_remove_buf_file_callback;
+
+	return 0;
+}
+
+/* Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ *
+ * Note, however, that as an local_cmpxchg is used for some atomic
+ * operations, this function must be called from the CPU which owns the buffer
+ * for a ACTIVE flush.
+ */
+static void ltt_force_switch(struct rchan_buf *buf, enum force_switch_mode mode)
+{
+	struct ltt_channel_struct *ltt_channel =
+			(struct ltt_channel_struct*)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+
+	u64 tsc;
+	long offset_begin, offset_end, offset_old;
+	long reserve_commit_diff;
+	long consumed_old, consumed_new;
+	long commit_count;
+	long end_switch_old;
+
+	do {
+		offset_old = local_read(&ltt_buf->offset);
+		offset_begin = offset_old;
+		end_switch_old = 0;
+
+		if (SUBBUF_OFFSET(offset_begin, buf->chan) != 0) {
+			offset_begin = SUBBUF_ALIGN(offset_begin, buf->chan);
+			end_switch_old = 1;
+		} else {
+			/* we do not have to switch : buffer is empty */
+			return;
+		}
+		if (mode == FORCE_ACTIVE)
+			offset_begin += ltt_subbuf_header_len();
+		/* Always begin_switch in FORCE_ACTIVE mode */
+		/* Test new buffer integrity */
+		reserve_commit_diff = SUBBUF_OFFSET(
+			buf->chan->subbuf_size
+			- local_read(
+			&ltt_buf->commit_count[SUBBUF_INDEX(offset_begin,
+						buf->chan)]), buf->chan);
+		if (reserve_commit_diff == 0) {
+			/* Next buffer not corrupted. */
+			if (mode == FORCE_ACTIVE && !ltt_channel->overwrite &&
+			(offset_begin - atomic_long_read(&ltt_buf->consumed))
+				>= rchan->alloc_size) {
+	  			/* We do not overwrite non consumed buffers
+				 * and we are full :
+				 * ignore switch while tracing is active. */
+				return;
+			}
+		} else {
+			/* Next subbuffer corrupted. Force pushing reader even
+			 * in normal mode */
+		}
+		offset_end = offset_begin;
+
+		tsc = ltt_get_timestamp64();
+		if (tsc == 0) {
+			/* Error in getting the timestamp : should not happen :
+			 * it would mean we are called from an NMI during a
+			 * write seqlock on xtime. */
+			return;
+		}
+	} while (local_cmpxchg(&ltt_buf->offset, offset_old, offset_end)
+							!= offset_old);
+
+	if (mode == FORCE_ACTIVE) {
+		/* Push the reader if necessary */
+		do {
+			consumed_old = atomic_long_read(&ltt_buf->consumed);
+			/* If buffer is in overwrite mode, push the reader
+			 * consumed count if the write position has reached it
+			 * and we are not at the first iteration (don't push
+			 * the reader farther than the writer). This operation
+			 * can be done concurrently by many writers in the same
+			 * buffer, the writer being at the fartest write
+			 * position sub-buffer index in the buffer being the
+			 * one which will win this loop.
+			 * If the buffer is not in overwrite mode, pushing the
+			 * reader only happen if a sub-buffer is corrupted */
+			if ((SUBBUF_TRUNC(offset_end-1, buf->chan)
+					- SUBBUF_TRUNC(consumed_old,
+						buf->chan))
+					>= rchan->alloc_size)
+				consumed_new =
+					SUBBUF_ALIGN(consumed_old, buf->chan);
+			else {
+				consumed_new = consumed_old;
+				break;
+			}
+		} while (atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+					consumed_new) != consumed_old);
+
+		if (consumed_old != consumed_new) {
+			/* Reader pushed : we are the winner of the push, we
+			 * can therefore reequilibrate reserve and commit.
+			 * Atomic increment of the commit count permits other
+			 * writers to play around with this variable before us.
+			 * We keep track of corrupted_subbuffers even in
+			 * overwrite mode :
+			 * we never want to write over a non completely
+			 * committed sub-buffer : possible causes : the buffer
+			 * size is too low compared to the unordered data input,
+			 * or there is a writer who died between the reserve
+			 * and the commit. */
+			if (reserve_commit_diff) {
+				/* We have to alter the sub-buffer commit
+				 * count : a sub-buffer is corrupted */
+				local_add(reserve_commit_diff,
+					&ltt_buf->commit_count[SUBBUF_INDEX(
+						offset_begin, buf->chan)]);
+				local_inc(&ltt_buf->corrupted_subbuffers);
+			}
+		}
+	}
+
+	/* Always switch */
+	if (end_switch_old) {
+		/* old subbuffer */
+		/* Concurrency safe because we are the last and only thread to
+		 * alter this sub-buffer. As long as it is not delivered and
+		 * read, no other thread can alter the offset, alter the
+		 * reserve_count or call the client_buffer_end_callback on this
+		 * sub-buffer. The only remaining threads could be the ones
+		 * with pending commits. They will have to do the deliver
+		 * themself.
+		 * Not concurrency safe in overwrite mode.
+		 * We detect corrupted subbuffers with commit and reserve
+		 * counts. We keep a corrupted sub-buffers count and push the
+		 * readers across these sub-buffers. Not concurrency safe if a
+		 * writer is stalled in a subbuffer and another writer switches
+		 * in, finding out it's corrupted. The result will be than the
+		 * old (uncommited) subbuffer will be declared corrupted, and
+		 * that the new subbuffer will be declared corrupted too because
+		 * of the commit count adjustment.
+		 * Offset old should never be 0. */
+		ltt_channel->buffer_end(buf, tsc, offset_old,
+				SUBBUF_INDEX((offset_old-1), buf->chan));
+		/* Must write buffer end before incrementing commit count */
+		smp_wmb();
+		commit_count =
+			local_add_return(buf->chan->subbuf_size
+				- (SUBBUF_OFFSET(offset_old-1, buf->chan) + 1),
+				&ltt_buf->commit_count[SUBBUF_INDEX(
+						offset_old-1, buf->chan)]);
+		if (SUBBUF_OFFSET(commit_count, buf->chan) == 0) {
+			ltt_deliver(buf,
+				SUBBUF_INDEX((offset_old-1), buf->chan), NULL);
+		}
+	}
+
+	if (mode == FORCE_ACTIVE) {
+		/* New sub-buffer */
+		/* This code can be executed unordered : writers may already
+		 * have written to the sub-buffer before this code gets
+		 * executed, caution. */
+		/* The commit makes sure that this code is executed before the
+		 * deliver of this sub-buffer */
+		ltt_channel->buffer_begin(buf, tsc,
+				SUBBUF_INDEX(offset_begin, buf->chan));
+		/* Must write buffer begin before incrementing commit count */
+		smp_wmb();
+		commit_count =
+			local_add_return(ltt_subbuf_header_len(),
+			 &ltt_buf->commit_count[SUBBUF_INDEX(offset_begin,
+				 buf->chan)]);
+		/* Check if the written buffer has to be delivered */
+		if (SUBBUF_OFFSET(commit_count, buf->chan) == 0) {
+			ltt_deliver(buf,
+				SUBBUF_INDEX(offset_begin, buf->chan), NULL);
+		}
+	}
+}
+
+/* LTTng channel flush function.
+ *
+ * Must be called when no tracing is active in the channel, because of
+ * accesses across CPUs. */
+static void ltt_relay_buffer_flush(struct rchan_buf *buf)
+{
+	buf->finalized = 1;
+	ltt_force_switch(buf, FORCE_FLUSH);
+}
+
+static void ltt_relay_async_wakeup_chan(struct ltt_channel_struct *ltt_channel)
+{
+	unsigned int i;
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+
+	for_each_possible_cpu(i) {
+		if (atomic_read(&ltt_channel->buf[i].wakeup_readers) == 1) {
+			atomic_set(&ltt_channel->buf[i].wakeup_readers, 0);
+			wake_up_interruptible(&rchan->buf[i]->read_wait);
+		}
+	}
+}
+
+/* Wake writers :
+ *
+ * This must be done after the trace is removed from the RCU list so that there
+ * are no stalled writers. */
+static void ltt_relay_wake_writers(struct ltt_channel_buf_struct *ltt_buf)
+{
+
+	if (waitqueue_active(&ltt_buf->write_wait))
+		schedule_work(&ltt_buf->wake_writers);
+}
+
+static void ltt_relay_finish_buffer(struct ltt_channel_struct *ltt_channel,
+	unsigned int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf;
+
+	if (rchan->buf[cpu]) {
+		ltt_buf = &ltt_channel->buf[cpu];;
+		ltt_relay_buffer_flush(rchan->buf[cpu]);
+		ltt_relay_wake_writers(ltt_buf);
+	}
+}
+
+
+static void ltt_relay_finish_channel(struct ltt_channel_struct *ltt_channel)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i) {
+		ltt_relay_finish_buffer(ltt_channel, i);
+	}
+}
+
+static void ltt_relay_remove_channel(struct ltt_channel_struct *channel)
+{
+	struct rchan *rchan = channel->trans_channel_data;
+
+	relay_close(rchan);
+	kref_put(&channel->kref, ltt_relay_release_channel);
+}
+
+/* ltt_relay_reserve_slot
+ *
+ * Atomic slot reservation in a LTTng buffer. It will take care of
+ * sub-buffer switching.
+ *
+ * Parameters:
+ *
+ * @trace : the trace structure to log to.
+ * @ltt_channel : channel structure
+ * @transport_data : data structure specific to ltt relay
+ * @data_size : size of the variable length data to log.
+ * @slot_size : pointer to total size of the slot (out)
+ * @tsc : pointer to the tsc at the slot reservation (out)
+ *
+ * Return : NULL if not enough space, else returns the pointer
+ * 		to the beginning of the reserved slot, aligned for the
+ * 		event header. */
+static void *ltt_relay_reserve_slot(
+		struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel,
+		void **transport_data,
+		size_t data_size,
+		size_t *slot_size,
+		u64 *tsc)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data = rchan->buf[smp_processor_id()];
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	long offset_begin, offset_end, offset_old;
+	long begin_switch, end_switch_current, end_switch_old;
+	long reserve_commit_diff = 0;
+	size_t size = 0;
+	size_t before_hdr_pad;
+	long consumed_old, consumed_new;
+	long commit_count;
+
+	if (ltt_nesting[smp_processor_id()] > 4) {
+		local_inc(&ltt_buf->events_lost);
+		return NULL;
+	}
+
+	do {
+		offset_old = local_read(&ltt_buf->offset);
+		offset_begin = offset_old;
+		begin_switch = 0;
+		end_switch_current = 0;
+		end_switch_old = 0;
+
+		if (SUBBUF_OFFSET(offset_begin, buf->chan) == 0) {
+			begin_switch = 1; /* For offset_begin */
+		} else {
+			size = ltt_get_header_size(ltt_channel,
+					buf->start + offset_begin,
+					data_size, &before_hdr_pad) + data_size;
+			if ((SUBBUF_OFFSET(offset_begin, buf->chan)+size)
+					> buf->chan->subbuf_size) {
+				end_switch_old = 1;	/* For offset_old */
+				begin_switch = 1;	/* For offset_begin */
+			}
+		}
+		if (begin_switch) {
+			if (end_switch_old) {
+				offset_begin =
+					SUBBUF_ALIGN(offset_begin, buf->chan);
+			}
+			offset_begin = offset_begin +
+				ltt_subbuf_header_len();
+			/* Test new buffer integrity */
+			reserve_commit_diff = SUBBUF_OFFSET(
+				buf->chan->subbuf_size - local_read(
+				&ltt_buf->commit_count[
+						SUBBUF_INDEX(offset_begin,
+						buf->chan)]), buf->chan);
+			if (reserve_commit_diff == 0) {
+				/* Next buffer not corrupted. */
+				if (!ltt_channel->overwrite &&
+					(SUBBUF_TRUNC(offset_begin, buf->chan)
+					- SUBBUF_TRUNC(
+					atomic_long_read(&ltt_buf->consumed),
+					buf->chan))
+					>= rchan->alloc_size) {
+					/* We do not overwrite non consumed
+					 * buffers and we are full : event
+					 * is lost. */
+					local_inc(&ltt_buf->events_lost);
+					return NULL;
+				} else {
+					/* next buffer not corrupted, we are
+					 * either in overwrite mode or the
+					 * buffer is not full. It's safe to
+					 * write in this new subbuffer.*/
+				}
+			} else {
+				/* Next subbuffer corrupted. Force pushing
+				 * reader even in normal mode. It's safe to
+				 * write in this new subbuffer. */
+			}
+			size = ltt_get_header_size(ltt_channel,
+					buf->start + offset_begin,
+					data_size, &before_hdr_pad) + data_size;
+			if ((SUBBUF_OFFSET(offset_begin,buf->chan) + size)
+					> buf->chan->subbuf_size) {
+				/* Event too big for subbuffers, report error,
+				 * don't complete the sub-buffer switch. */
+				local_inc(&ltt_buf->events_lost);
+				return NULL;
+			} else {
+				/* We just made a successful buffer switch and
+				 * the event fits in the new subbuffer. Let's
+				 * write. */
+			}
+		} else {
+			/* Event fits in the current buffer and we are not on a
+			 * switch boundary. It's safe to write */
+		}
+		offset_end = offset_begin + size;
+
+		if ((SUBBUF_OFFSET(offset_end, buf->chan)) == 0) {
+			/* The offset_end will fall at the very beginning of
+			 * the next subbuffer. */
+			end_switch_current = 1;	/* For offset_begin */
+		}
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+		if (begin_switch || end_switch_old || end_switch_current)
+			*tsc = ltt_get_timestamp64();
+		else
+			*tsc = ltt_get_timestamp32();
+#else
+		*tsc = ltt_get_timestamp64();
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+		if (*tsc == 0) {
+			/* Error in getting the timestamp, event lost */
+			local_inc(&ltt_buf->events_lost);
+			return NULL;
+		}
+
+	} while (local_cmpxchg(&ltt_buf->offset, offset_old, offset_end)
+							!= offset_old);
+
+
+	/* Push the reader if necessary */
+	do {
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		/* If buffer is in overwrite mode, push the reader consumed
+		 * count if the write position has reached it and we are not
+		 * at the first iteration (don't push the reader farther than
+		 * the writer). This operation can be done concurrently by many
+		 * writers in the same buffer, the writer being at the fartest
+		 * write position sub-buffer index in the buffer being the one
+		 * which will win this loop. */
+		/* If the buffer is not in overwrite mode, pushing the reader
+		 * only happen if a sub-buffer is corrupted */
+		if ((SUBBUF_TRUNC(offset_end-1, buf->chan)
+					- SUBBUF_TRUNC(consumed_old, buf->chan))
+					>= rchan->alloc_size)
+			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		else {
+			consumed_new = consumed_old;
+			break;
+		}
+	} while (atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+			consumed_new) != consumed_old);
+
+	if (consumed_old != consumed_new) {
+		/* Reader pushed : we are the winner of the push, we can
+		 * therefore reequilibrate reserve and commit. Atomic increment
+		 * of the commit count permits other writers to play around
+		 * with this variable before us. We keep track of
+		 * corrupted_subbuffers even in overwrite mode :
+		 * we never want to write over a non completely committed
+		 * sub-buffer : possible causes : the buffer size is too low
+		 * compared to the unordered data input, or there is a writer
+		 * who died between the reserve and the commit. */
+		if (reserve_commit_diff) {
+			/* We have to alter the sub-buffer commit count : a
+			 * sub-buffer is corrupted. We do not deliver it. */
+			local_add(
+				reserve_commit_diff,
+				&ltt_buf->commit_count[
+					SUBBUF_INDEX(offset_begin, buf->chan)]);
+			local_inc(&ltt_buf->corrupted_subbuffers);
+		}
+	}
+
+	if (end_switch_old) {
+		/* old subbuffer */
+		/* Concurrency safe because we are the last and only thread to
+		 * alter this sub-buffer. As long as it is not delivered and
+		 * read, no other thread can alter the offset, alter the
+		 * reserve_count or call the client_buffer_end_callback on
+		 * this sub-buffer.
+		 * The only remaining threads could be the ones with pending
+		 * commits. They will have to do the deliver themself.
+		 * Not concurrency safe in overwrite mode. We detect corrupted
+		 * subbuffers with commit and reserve counts. We keep a
+		 * corrupted sub-buffers count and push the readers across
+		 * these sub-buffers.
+		 * Not concurrency safe if a writer is stalled in a subbuffer
+		 * and another writer switches in, finding out it's corrupted.
+		 * The result will be than the old (uncommited) subbuffer will
+		 * be declared corrupted, and that the new subbuffer will be
+		 * declared corrupted too because of the commit count
+		 * adjustment.
+		 * Note : offset_old should never be 0 here.*/
+		ltt_channel->buffer_end(buf, *tsc, offset_old,
+			SUBBUF_INDEX((offset_old-1), buf->chan));
+		/* Must write buffer end before incrementing commit count */
+		smp_wmb();
+		commit_count =
+			local_add_return(buf->chan->subbuf_size
+				- (SUBBUF_OFFSET(offset_old-1, buf->chan)+1),
+				&ltt_buf->commit_count[SUBBUF_INDEX(
+						offset_old-1, buf->chan)]);
+		if (SUBBUF_OFFSET(commit_count, buf->chan) == 0) {
+			ltt_deliver(buf, SUBBUF_INDEX((offset_old-1),
+						buf->chan), NULL);
+		}
+	}
+
+	if (begin_switch) {
+		/* New sub-buffer */
+		/* This code can be executed unordered : writers may already
+		 * have written to the sub-buffer before this code gets
+		 * executed, caution. */
+		/* The commit makes sure that this code is executed before the
+		 * deliver of this sub-buffer */
+		ltt_channel->buffer_begin(buf, *tsc, SUBBUF_INDEX(offset_begin,
+					buf->chan));
+		commit_count = local_add_return(
+				ltt_subbuf_header_len(),
+				&ltt_buf->commit_count[
+					SUBBUF_INDEX(offset_begin, buf->chan)]);
+		/* Check if the written buffer has to be delivered */
+		if (SUBBUF_OFFSET(commit_count, buf->chan) == 0) {
+			ltt_deliver(buf,
+				SUBBUF_INDEX(offset_begin, buf->chan), NULL);
+		}
+	}
+
+	if (end_switch_current) {
+		/* current subbuffer */
+		/* Concurrency safe because we are the last and only thread to
+		 * alter this sub-buffer. As long as it is not delivered and
+		 * read, no other thread can alter the offset, alter the
+		 * reserve_count or call the client_buffer_end_callback on this
+		 * sub-buffer.
+		 * The only remaining threads could be the ones with pending
+		 * commits. They will have to do the deliver themself.
+		 * Not concurrency safe in overwrite mode. We detect corrupted
+		 * subbuffers with commit and reserve counts. We keep a
+		 * corrupted sub-buffers count and push the readers across
+		 * these sub-buffers.
+		 * Not concurrency safe if a writer is stalled in a subbuffer
+		 * and another writer switches in, finding out it's corrupted.
+		 * The result will be than the old (uncommited) subbuffer will
+		 * be declared corrupted, and that the new subbuffer will be
+		 * declared corrupted too because of the commit count
+		 * adjustment. */
+		ltt_channel->buffer_end(buf, *tsc, offset_end,
+			SUBBUF_INDEX((offset_end-1), buf->chan));
+		/* Must write buffer begin before incrementing commit count */
+		smp_wmb();
+		commit_count =
+			local_add_return(buf->chan->subbuf_size
+				- (SUBBUF_OFFSET(offset_end-1, buf->chan)+1),
+				&ltt_buf->commit_count[SUBBUF_INDEX(
+						offset_end-1, buf->chan)]);
+		if (SUBBUF_OFFSET(commit_count, buf->chan) == 0) {
+			ltt_deliver(buf,
+				SUBBUF_INDEX((offset_end-1), buf->chan), NULL);
+		}
+	}
+
+	*slot_size = size;
+
+	//BUG_ON(*slot_size != (data_size + *before_hdr_pad + *after_hdr_pad + *header_size));
+	//BUG_ON(*slot_size != (offset_end - offset_begin));
+
+	return buf->start + BUFFER_OFFSET(offset_begin, buf->chan)
+		+ before_hdr_pad;
+}
+
+
+/* ltt_relay_commit_slot
+ *
+ * Atomic unordered slot commit. Increments the commit count in the
+ * specified sub-buffer, and delivers it if necessary.
+ *
+ * Parameters:
+ *
+ * @buf : the buffer to commit to.
+ * @reserved : address following the event header.
+ * @slot_size : size of the reserved slot.
+ *
+ */
+static void ltt_relay_commit_slot(
+		struct ltt_channel_struct *ltt_channel,
+		void **transport_data,
+		void *reserved,
+		size_t slot_size)
+{
+	struct rchan_buf *buf = *transport_data;
+	struct ltt_channel_buf_struct *ltt_buf = &ltt_channel->buf[buf->cpu];
+	unsigned int offset_end = reserved - buf->start;
+	long commit_count;
+
+	/* Must write slot data before incrementing commit count */
+	smp_wmb();
+	commit_count = local_add_return(slot_size,
+		&ltt_buf->commit_count[SUBBUF_INDEX(offset_end-1, buf->chan)]);
+	/* Check if all commits have been done */
+	if (SUBBUF_OFFSET(commit_count, buf->chan) == 0)
+		ltt_deliver(buf, SUBBUF_INDEX(offset_end-1, buf->chan), NULL);
+}
+
+/* This is called with preemption disabled when user space has requested
+ * blocking mode.  If one of the active traces has free space below a
+ * specific threshold value, we reenable preemption and block.
+ */
+static int ltt_relay_user_blocking(struct ltt_trace_struct *trace,
+		unsigned int index, size_t data_size, struct user_dbg_data *dbg)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+	DECLARE_WAITQUEUE(wait, current);
+
+	channel = ltt_get_channel_from_index(trace, index);
+	rchan = channel->trans_channel_data;
+	relay_buf = rchan->buf[smp_processor_id()];
+	ltt_buf = &channel->buf[smp_processor_id()];
+	/* Check if data is too big for the channel : do not
+	 * block for it */
+	if (LTT_RESERVE_CRITICAL + data_size > relay_buf->chan->subbuf_size)
+		return 0;
+
+	/* If free space too low, we block. We restart from the
+	 * beginning after we resume (cpu id may have changed
+	 * while preemption is active).
+	 */
+	spin_lock(&ltt_buf->full_lock);
+	if (!channel->overwrite &&
+		(dbg->avail_size = (dbg->write = local_read(
+			&channel->buf[relay_buf->cpu].offset))
+		+ LTT_RESERVE_CRITICAL + data_size
+		 - SUBBUF_TRUNC((dbg->read = atomic_long_read(
+		&channel->buf[relay_buf->cpu].consumed)),
+			 		relay_buf->chan))
+			>= rchan->alloc_size) {
+		__set_current_state(TASK_INTERRUPTIBLE);
+		add_wait_queue(&ltt_buf->write_wait, &wait);
+		spin_unlock(&ltt_buf->full_lock);
+		preempt_enable();
+		schedule();
+		__set_current_state(TASK_RUNNING);
+		remove_wait_queue(&ltt_buf->write_wait, &wait);
+		if (signal_pending(current))
+			return -ERESTARTSYS;
+		preempt_disable();
+		return 1;
+	}
+	spin_unlock(&ltt_buf->full_lock);
+	return 0;
+}
+
+static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
+		unsigned int index, size_t data_size, struct user_dbg_data *dbg)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+
+	channel = ltt_get_channel_from_index(trace, index);
+	rchan = channel->trans_channel_data;
+	relay_buf = rchan->buf[smp_processor_id()];
+	ltt_buf = &channel->buf[smp_processor_id()];
+	printk(KERN_ERR "Error in LTT usertrace : "
+	"buffer full : event lost in blocking "
+	"mode. Increase LTT_RESERVE_CRITICAL.\n");
+	printk(KERN_ERR "LTT nesting level is %u.\n",
+		ltt_nesting[smp_processor_id()]);
+	printk(KERN_ERR "LTT avail size %lu.\n",
+		dbg->avail_size);
+	printk(KERN_ERR "avai write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+	printk(KERN_ERR "LTT cur size %lu.\n",
+		(dbg->write = local_read(
+		&channel->buf[relay_buf->cpu].offset))
+	+ LTT_RESERVE_CRITICAL + data_size
+	 - SUBBUF_TRUNC((dbg->read = atomic_long_read(
+	&channel->buf[relay_buf->cpu].consumed)),
+				relay_buf->chan));
+	printk(KERN_ERR "cur write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+}
+
+static struct ltt_transport ltt_relay_transport = {
+	.name = "relay",
+	.owner = THIS_MODULE,
+	.ops = {
+		.create_dirs = ltt_relay_create_dirs,
+		.remove_dirs = ltt_relay_remove_dirs,
+		.create_channel = ltt_relay_create_channel,
+		.finish_channel = ltt_relay_finish_channel,
+		.remove_channel = ltt_relay_remove_channel,
+		.wakeup_channel = ltt_relay_async_wakeup_chan,
+		.commit_slot = ltt_relay_commit_slot,
+		.reserve_slot = ltt_relay_reserve_slot,
+		.user_blocking = ltt_relay_user_blocking,
+		.user_errors = ltt_relay_print_user_errors,
+	},
+};
+
+static int __init ltt_relay_init(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay init\n");
+	ltt_root_dentry = debugfs_create_dir(LTT_RELAY_ROOT, NULL);
+	if (ltt_root_dentry == NULL)
+		return -EEXIST;
+
+	ltt_file_operations = relay_file_operations;
+	ltt_file_operations.owner = THIS_MODULE;
+	ltt_file_operations.poll = ltt_poll;
+	ltt_file_operations.ioctl = ltt_ioctl;
+#ifdef CONFIG_COMPAT
+	ltt_file_operations.compat_ioctl = ltt_compat_ioctl;
+#endif //CONFIG_COMPAT
+
+	ltt_transport_register(&ltt_relay_transport);
+
+	return 0;
+}
+
+static void __exit ltt_relay_exit(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay exit\n");
+
+	ltt_transport_unregister(&ltt_relay_transport);
+
+	debugfs_remove(ltt_root_dentry);
+}
+
+module_init(ltt_relay_init);
+module_exit(ltt_relay_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Tracer");
Index: linux/ltt/ltt-serialize.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-serialize.c	2007-08-30 12:39:34.159644000 +0100
@@ -0,0 +1,483 @@
+/*
+ * ltt-serialize.c
+ *
+ * LTTng serializing code.
+ *
+ * Copyright Mathieu Desnoyers, March 2007.
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <stdarg.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+
+static int skip_atoi(const char **s)
+{
+	int i=0;
+
+	while (isdigit(**s))
+		i = i*10 + *((*s)++) - '0';
+	return i;
+}
+
+/* Inspired from vsnprintf */
+/* The serialization format string supports the basic printf format strings.
+ * In addition, it also defines new formats that can be used to serialize
+ * more complex/non portable data structures.
+ *
+ * Serialization specific formats :
+ *
+ * Fixed length struct, union or array.
+ * %*r     expects sizeof(*ptr), ptr
+ * %*.*r   expects sizeof(*ptr), __alignof__(*ptr), ptr
+ *
+ * Variable length sequence
+ * %*.*:*v expects sizeof(*ptr), __alignof__(*ptr), elem_num, ptr
+ *         where elem_num is the number of elements in the sequence
+ *
+ * Callback
+ * %k      callback (taken from the probe data)
+ *
+ * Fixed size integers
+ * %1b     expects uint8_t
+ * %2b     expects uint16_t
+ * %4b     expects uint32_t
+ * %8b     expects uint64_t
+ * %*b     expects sizeof(data), data
+ *         where sizeof(data) is 1, 2, 4 or 8
+ */
+__attribute__((no_instrument_function))
+char *ltt_serialize_data(char *buffer, char *str,
+			struct ltt_serialize_closure *closure,
+			int align,
+			const char *fmt, va_list *args)
+{
+	const char *s;
+	int elem_size;		/* Size of the integer for 'b' */
+				/* Size of the data contained by 'r' */
+	int elem_alignment;	/* Element alignment for 'r' */
+	int elem_num;		/* Number of elements in 'v' */
+	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
+				/* 'z' support added 23/7/1999 S.H.    */
+				/* 'z' changed to 'Z' --davidm 1/25/99 */
+				/* 't' added for ptrdiff_t */
+
+	for (; *fmt ; ++fmt) {
+		if (*fmt != '%') {
+			/* Skip text */
+			continue;
+		}
+
+		/* process flags : ignore standard print formats for now. */
+		repeat:
+			++fmt;		/* this also skips first '%' */
+			switch (*fmt) {
+				case '-':
+				case '+':
+				case ' ':
+				case '#':
+				case '0': goto repeat;
+			}
+
+		/* get element size */
+		elem_size = -1;
+		if (isdigit(*fmt))
+			elem_size = skip_atoi(&fmt);
+		else if (*fmt == '*') {
+			++fmt;
+			/* it's the next argument */
+			elem_size = va_arg(*args, int);
+		}
+
+		/* get the alignment */
+		elem_alignment = -1;
+		if (*fmt == '.') {
+			++fmt;
+			if (isdigit(*fmt))
+				elem_alignment = skip_atoi(&fmt);
+			else if (*fmt == '*') {
+				++fmt;
+				/* it's the next argument */
+				elem_alignment = va_arg(*args, int);
+			}
+		}
+
+		/* get the number of elements */
+		elem_num = -1;
+		if (*fmt == ':') {
+			++fmt;
+			if (isdigit(*fmt))
+				elem_num = skip_atoi(&fmt);
+			else if (*fmt == '*') {
+				++fmt;
+				/* it's the next argument */
+				elem_num = va_arg(*args, int);
+			}
+		}
+
+		/* get the conversion qualifier */
+		qualifier = -1;
+		if (*fmt == 'h' || *fmt == 'l' || *fmt == 'L' ||
+		    *fmt =='Z' || *fmt == 'z' || *fmt == 't' ||
+		    *fmt == 'S') {
+			qualifier = *fmt;
+			++fmt;
+			if (qualifier == 'l' && *fmt == 'l') {
+				qualifier = 'L';
+				++fmt;
+			}
+		}
+
+		switch (*fmt) {
+			case 'c':
+				if (buffer)
+					*str = (char)va_arg(*args, int);
+				else
+					(void)va_arg(*args, int);
+				str += sizeof(char);
+				continue;
+
+			case 's':
+				s = va_arg(*args, const char *);
+				if ((unsigned long)s < PAGE_SIZE)
+					s = "<NULL>";
+				elem_size = strlen(s)+1;
+				if (buffer)
+					memcpy(str, s, elem_size);
+				str += elem_size;
+				/* Following alignment for genevent
+				 * compatibility */
+				if (align)
+					str += ltt_align((long)str,
+							sizeof(void*));
+				continue;
+
+			case 'p':
+				if (align)
+					str += ltt_align((long)str,
+							sizeof(void*));
+				if (buffer)
+					*(void**)str = va_arg(*args, void *);
+				else
+					(void)va_arg(*args, void *);
+				str += sizeof(void*);
+				continue;
+
+			case 'r':
+				/* For array, struct, union */
+				if (elem_alignment > 0 && align)
+					str += ltt_align((long)str,
+							elem_alignment);
+				s = va_arg(*args, const char *);
+				if (elem_size > 0) {
+					if (buffer)
+						memcpy(str, s, elem_size);
+					str += elem_size;
+				}
+				continue;
+
+			case 'v':
+				/* For sequence */
+				if (align)
+					str += ltt_align((long)str,
+						max((int)sizeof(int),
+						elem_alignment));
+				if (buffer)
+					*(int*)str = elem_num;
+				str += sizeof(int);
+				if (elem_alignment > 0 && align)
+					str += ltt_align((long)str,
+						elem_alignment);
+				s = va_arg(*args, const char *);
+				if (elem_num > 0) {
+					if (buffer)
+						memcpy(str, s,
+							elem_num*elem_size);
+					str += elem_num*elem_size;
+				}
+				/* Following alignment for genevent
+				 * compatibility */
+				if (align)
+					str += ltt_align((long)str,
+							sizeof(void*));
+				continue;
+
+			case 'k':
+				/* For callback */
+				 /* The callback will take as many arguments
+				  * as it needs from args. They won't be
+				  * type verified. */
+				if (closure->cb_idx < LTT_NR_CALLBACKS-1) {
+					ltt_serialize_cb cb;
+					closure->cb_idx++;
+					cb = closure->callbacks[closure->cb_idx];
+					str = cb(buffer, str, closure,
+							align,
+							fmt, args);
+				}
+				continue;
+
+			case 'n':
+				/* FIXME:
+				* What does C99 say about the overflow case
+				* here? */
+				if (qualifier == 'l') {
+					long * ip = va_arg(*args, long *);
+					*ip = (str - buffer);
+				} else if (qualifier == 'Z'
+					|| qualifier == 'z') {
+					size_t * ip = va_arg(*args, size_t *);
+					*ip = (str - buffer);
+				} else {
+					int * ip = va_arg(*args, int *);
+					*ip = (str - buffer);
+				}
+				continue;
+
+			case '%':
+				continue;
+
+			case 'b':
+				if (elem_size < 0)
+					elem_size = 0;
+				if (elem_size > 0 && align)
+					str += ltt_align((long)str, elem_size);
+				switch (elem_size) {
+				case 1:
+					if (buffer)
+						*(int8_t*)str =
+						(int8_t)va_arg(*args, int);
+					else
+						(void)va_arg(*args, int);
+					break;
+				case 2:
+					if (buffer)
+						*(int16_t*)str =
+						(int16_t)va_arg(*args, int);
+					else
+						(void)va_arg(*args, int);
+					break;
+				case 4:
+					if (buffer)
+						*(int32_t*)str =
+						va_arg(*args, int32_t);
+					else
+						(void)va_arg(*args, int32_t);
+					break;
+				case 8:
+					if (buffer)
+						*(int64_t*)str =
+						va_arg(*args, int64_t);
+					else
+						(void)va_arg(*args, int64_t);
+					break;
+				}
+				str += elem_size;
+				continue;
+
+			case 'o':
+			case 'X':
+			case 'x':
+			case 'd':
+			case 'i':
+			case 'u':
+				break;
+
+			default:
+				if (!*fmt)
+					--fmt;
+				continue;
+		}
+		switch (qualifier) {
+		case 'L':
+			if (align)
+				str += ltt_align((long)str, sizeof(long long));
+			if (buffer)
+				*(long long*)str = va_arg(*args, long long);
+			else
+				(void)va_arg(*args, long long);
+			str += sizeof(long long);
+			break;
+		case 'l':
+			if (align)
+				str += ltt_align((long)str, sizeof(long));
+			if (buffer)
+				*(long*)str = va_arg(*args, long);
+			else
+				(void)va_arg(*args, long);
+			str += sizeof(long);
+			break;
+		case 'Z':
+		case 'z':
+			if (align)
+				str += ltt_align((long)str, sizeof(size_t));
+			if (buffer)
+				*(size_t*)str = va_arg(*args, size_t);
+			else
+				(void)va_arg(*args, size_t);
+			str += sizeof(size_t);
+			break;
+		case 't':
+			if (align)
+				str += ltt_align((long)str, sizeof(ptrdiff_t));
+			if (buffer)
+				*(ptrdiff_t*)str = va_arg(*args, ptrdiff_t);
+			else
+				(void)va_arg(*args, ptrdiff_t);
+			str += sizeof(ptrdiff_t);
+			break;
+		case 'h':
+			if (align)
+				str += ltt_align((long)str, sizeof(short));
+			if (buffer)
+				*(short*)str = (short) va_arg(*args, int);
+			else
+				(void)va_arg(*args, int);
+			str += sizeof(short);
+			break;
+		default:
+			if (align)
+				str += ltt_align((long)str, sizeof(int));
+			if (buffer)
+				*(int*)str = va_arg(*args, int);
+			else
+				(void)va_arg(*args, int);
+			str += sizeof(int);
+		}
+	}
+	return str;
+}
+EXPORT_SYMBOL_GPL(ltt_serialize_data);
+
+/* Calculate data size */
+/* Assume that the padding for alignment starts at a
+ * sizeof(void *) address. */
+static __attribute__((no_instrument_function))
+size_t ltt_get_data_size(struct ltt_serialize_closure *closure,
+				int align,
+				const char *fmt, va_list *args)
+{
+	ltt_serialize_cb cb = closure->callbacks[0];
+	closure->cb_idx = 0;
+	return (size_t)cb(NULL, NULL, closure, align, fmt, args);
+}
+
+static __attribute__((no_instrument_function))
+void ltt_write_event_data(char *buffer,
+				struct ltt_serialize_closure *closure,
+				int align,
+				const char *fmt, va_list *args)
+{
+	ltt_serialize_cb cb = closure->callbacks[0];
+	closure->cb_idx = 0;
+	cb(buffer, buffer, closure, align, fmt, args);
+}
+
+
+__attribute__((no_instrument_function))
+void ltt_vtrace(const struct __mark_marker_data *mdata,
+		const char *fmt, va_list args)
+{
+	int flags;
+	int align;
+	struct ltt_probe_data *pdata;
+	uint8_t fID, eID;
+	size_t data_size, slot_size;
+	int channel_index;
+	struct ltt_channel_struct *channel;
+	struct ltt_trace_struct *trace, *dest_trace = NULL;
+	void *transport_data;
+	uint64_t tsc;
+	char *buffer;
+	va_list args_copy;
+	struct ltt_serialize_closure closure;
+
+	flags = mdata->flags;
+	pdata = (struct ltt_probe_data *)mdata->pdata;
+	fID = pdata->fID;
+	eID = pdata->eID;
+	align = pdata->align;
+	closure.callbacks = pdata->callbacks;
+
+	/* This test is useful for quickly exiting static tracing when no
+	 * trace is active. */
+	if (likely(ltt_traces.num_active_traces == 0
+		&& !(flags & LF_FORCE)))
+		return;
+
+	preempt_disable();
+	ltt_nesting[smp_processor_id()]++;
+
+	if (unlikely(flags & LF_TRACE))
+		dest_trace = va_arg(args, struct ltt_trace_struct *);
+	if (unlikely(flags & LF_CHANNEL))
+		channel_index = va_arg(args, int);
+	else
+		channel_index = pdata->channel_index;
+	/* Force write in the compact channel if compact flag is used */
+	if (unlikely(flags & LF_COMPACT))
+		channel_index = GET_CHANNEL_INDEX(compact);
+
+	va_copy(args_copy, args);
+	/* Skip the compact data for size calculation */
+	if (likely(flags & LF_COMPACT_DATA))
+		(void)va_arg(args_copy, u32);
+	data_size = ltt_get_data_size(&closure, align, fmt, &args_copy);
+	va_end(args_copy);
+
+	/* Iterate on each traces */
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		if (unlikely(!trace->active
+			&& !(flags & LF_FORCE)))
+			continue;
+		if (unlikely(flags & LF_TRACE && trace != dest_trace))
+			continue;
+		if (!ltt_run_filter(trace, fID, eID))
+			continue;
+		channel = ltt_get_channel_from_index(trace, channel_index);
+		/* reserve space : header and data */
+		buffer = ltt_reserve_slot(trace, channel, &transport_data,
+						data_size, &slot_size, &tsc);
+		if (unlikely(!buffer))
+			continue; /* buffer full */
+
+		va_copy(args_copy, args);
+		/* Out-of-order write : header and data */
+		if (likely(!(flags & LF_COMPACT)))
+			buffer = ltt_write_event_header(trace, channel, buffer,
+						fID, eID, data_size, tsc);
+		else {
+			u32 compact_data = 0;
+			if (likely(flags & LF_COMPACT_DATA))
+				compact_data = va_arg(args_copy, u32);
+			buffer = ltt_write_compact_header(trace, channel,
+						buffer, fID, eID, data_size,
+						tsc, compact_data);
+		}
+		ltt_write_event_data(buffer, &closure, align, fmt, &args_copy);
+		va_end(args_copy);
+		/* Out-of-order commit */
+		ltt_commit_slot(channel, &transport_data, buffer, slot_size);
+	}
+	ltt_nesting[smp_processor_id()]--;
+	preempt_enable();
+}
+EXPORT_SYMBOL_GPL(ltt_vtrace);
+
+__attribute__((no_instrument_function))
+void ltt_trace(const struct __mark_marker_data *mdata, const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	ltt_vtrace(mdata, fmt, args);
+	va_end(args);
+}
+EXPORT_SYMBOL_GPL(ltt_trace);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Serializer");
Index: linux/ltt/ltt-statedump.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-statedump.c	2007-08-30 12:39:34.166644000 +0100
@@ -0,0 +1,385 @@
+/* ltt-statedump.c
+ *
+ * Linux Trace Toolkit Kernel State Dump
+ *
+ * Copyright 2005 -
+ * Jean-Hugues Deschenes <jean-hugues.deschenes@polymtl.ca>
+ *
+ * Changes:
+ *	Eric Clement:           Add listing of network IP interface
+ *	Mathieu Desnoyers	Fix kernel threads
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+#include <linux/netlink.h>
+#include <linux/inet.h>
+#include <linux/ip.h>
+#include <linux/kthread.h>
+#include <linux/proc_fs.h>
+#include <linux/file.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/cpu.h>
+#include <linux/netdevice.h>
+#include <linux/inetdevice.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/marker.h>
+
+#define NB_PROC_CHUNK 20
+
+/*static atomic_t kernel_threads_to_run;*/
+static struct work_struct cpu_work[NR_CPUS];
+/*static struct task_struct *work_wake_task;*/
+enum lttng_thread_type {
+	LTTNG_USER_THREAD = 0,
+	LTTNG_KERNEL_THREAD = 1,
+};
+
+enum lttng_execution_mode {
+	LTTNG_USER_MODE = 0,
+	LTTNG_SYSCALL = 1,
+	LTTNG_TRAP = 2,
+	LTTNG_IRQ = 3,
+	LTTNG_SOFTIRQ = 4,
+	LTTNG_MODE_UNKNOWN = 5,
+};
+
+enum lttng_execution_submode {
+	LTTNG_NONE = 0,
+	LTTNG_UNKNOWN = 1,
+};
+
+enum lttng_process_status {
+	LTTNG_UNNAMED = 0,
+	LTTNG_WAIT_FORK = 1,
+	LTTNG_WAIT_CPU = 2,
+	LTTNG_EXIT = 3,
+	LTTNG_ZOMBIE = 4,
+	LTTNG_WAIT = 5,
+	LTTNG_RUN = 6,
+	LTTNG_DEAD = 7,
+};
+
+enum lttng_interface_state {
+	LTTNG_UP = 0,
+	LTTNG_DOWN = 1,
+};
+
+/* in modules.c */
+extern void list_modules(void);
+
+#ifdef CONFIG_INET
+static void ltt_enumerate_device(struct net_device *dev)
+{
+	struct in_device *in_dev;
+	struct in_ifaddr *ifa;
+
+	if (dev->flags & IFF_UP) {
+		in_dev = in_dev_get(dev);
+		if (in_dev) {
+			for (ifa = in_dev->ifa_list;
+					ifa != NULL;
+					ifa = ifa->ifa_next) {
+				MARK(list_network_ip_interface, "%s %4b %d",
+					dev->name, ifa->ifa_address, LTTNG_UP);
+			}
+			in_dev_put(in_dev);
+		}
+	} else
+		MARK(list_network_ip_interface, "%s %4b %d",
+			dev->name, 0, LTTNG_DOWN);
+}
+
+static inline int ltt_enumerate_network_ip_interface(void)
+{
+	struct net_device *list;
+
+	read_lock(&dev_base_lock);
+	for (list = dev_base; list != NULL; list = list->next)
+		ltt_enumerate_device(list);
+	read_unlock(&dev_base_lock);
+
+	return 0;
+}
+#else /* CONFIG_INET */
+static inline int ltt_enumerate_network_ip_interface(void)
+{
+	return 0;
+}
+#endif /* CONFIG_INET */
+
+
+static inline void ltt_enumerate_task_fd(struct task_struct *t,
+		char *tmp)
+{
+	struct fdtable *fdt;
+	struct file * filp;
+	unsigned int i;
+	const unsigned char *path;
+
+	if (!t->files)
+		return;
+
+	spin_lock(&t->files->file_lock);
+	fdt = files_fdtable(t->files);
+	for (i = 0; i < fdt->max_fds; i++) {
+		filp = fcheck_files(t->files, i);
+		if (!filp)
+			continue;
+		path = d_path(filp->f_dentry,
+				filp->f_vfsmnt, tmp, PAGE_SIZE);
+		/* Make sure we give at least some info */
+		MARK(list_file_descriptor, "%s %d %u",
+			(IS_ERR(path))?(filp->f_dentry->d_name.name):(path),
+			t->pid, i);
+	}
+	spin_unlock(&t->files->file_lock);
+}
+
+static inline int ltt_enumerate_file_descriptors(void)
+{
+	struct task_struct * t = &init_task;
+	char *tmp = (char*)__get_free_page(GFP_KERNEL);
+
+	/* Enumerate active file descriptors */
+	do {
+		read_lock(&tasklist_lock);
+		if (t != &init_task)
+			atomic_dec(&t->usage);
+		t = next_task(t);
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+		task_lock(t);
+		ltt_enumerate_task_fd(t, tmp);
+		task_unlock(t);
+	} while (t != &init_task);
+	free_page((unsigned long)tmp);
+	return 0;
+}
+
+static inline void ltt_enumerate_task_vm_maps(struct task_struct *t)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *	map;
+	unsigned long ino;
+
+	/* get_task_mm does a task_lock... */
+	mm = get_task_mm(t);
+	if (!mm)
+		return;
+
+	map = mm->mmap;
+	if (map) {
+		down_read(&mm->mmap_sem);
+		while (map) {
+			if (map->vm_file)
+				ino = map->vm_file->f_dentry->d_inode->i_ino;
+			else
+				ino = 0;
+			MARK(list_vm_map, "%d %lu %lu %lu %lu %lu",
+					t->pid,
+					map->vm_start,
+					map->vm_end,
+					map->vm_flags,
+					map->vm_pgoff << PAGE_SHIFT,
+					ino);
+			map = map->vm_next;
+		}
+		up_read(&mm->mmap_sem);
+	}
+	mmput(mm);
+}
+
+static inline int ltt_enumerate_vm_maps(void)
+{
+	struct task_struct *  t = &init_task;
+
+	do {
+		read_lock(&tasklist_lock);
+		if (t != &init_task)
+			atomic_dec(&t->usage);
+		t = next_task(t);
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+		ltt_enumerate_task_vm_maps(t);
+ 	} while (t != &init_task);
+	return 0;
+}
+
+static inline void list_interrupts(void)
+{
+        unsigned int i;
+        unsigned long flags = 0;
+
+        /* needs irq_desc */
+        for(i=0; i<NR_IRQS; i++) {
+                struct irqaction * action;
+
+                spin_lock_irqsave(&irq_desc[i].lock, flags);
+		for (action = irq_desc[i].action;
+                        action; action = action->next) {
+                        MARK(list_interrupt, "%s %s %u",
+                                "unnamed_irq_chip", action->name, i);
+                }
+
+                spin_unlock_irqrestore(&irq_desc[i].lock, flags);
+        }
+}
+
+static inline int ltt_enumerate_process_states(void)
+{
+	struct task_struct *t = &init_task;
+	struct task_struct *p = t;
+	enum lttng_process_status status;
+	enum lttng_thread_type type;
+	enum lttng_execution_mode mode;
+	enum lttng_execution_submode submode;
+
+	do {
+		mode = LTTNG_MODE_UNKNOWN;
+		submode = LTTNG_UNKNOWN;
+
+		read_lock(&tasklist_lock);
+		if (t != &init_task) {
+			atomic_dec(&t->usage);
+			t = next_thread(t);
+		}
+		if (t == p) {
+			t = p = next_task(t);
+		}
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+
+		task_lock(t);
+
+		if (t->exit_state == EXIT_ZOMBIE)
+			status = LTTNG_ZOMBIE;
+		else if (t->exit_state == EXIT_DEAD)
+			status = LTTNG_DEAD;
+		else if (t->state == TASK_RUNNING) {
+			/* Is this a forked child that has not run yet? */
+			if (list_empty(&t->run_list))
+				status = LTTNG_WAIT_FORK;
+			else
+				/* All tasks are considered as wait_cpu;
+				 * the viewer will sort out if the task was
+				 * really running at this time. */
+				status = LTTNG_WAIT_CPU;
+		}
+		else if (t->state &
+			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)) {
+			/* Task is waiting for something to complete */
+			status = LTTNG_WAIT;
+		}
+		else
+			status = LTTNG_UNNAMED;
+		submode = LTTNG_NONE;
+
+		/* Verification of t->mm is to filter out kernel threads;
+		 * Viewer will further filter out if a user-space thread was
+		 * in syscall mode or not */
+		if (t->mm)
+			type = LTTNG_USER_THREAD;
+		else
+			type = LTTNG_KERNEL_THREAD;
+
+		MARK(list_process_state, "%d %d %s %d %d %d %d %d",
+				t->pid, t->parent->pid, t->comm,
+				type, mode, submode, status, t->tgid);
+		task_unlock(t);
+	} while (t != &init_task);
+
+	return 0;
+}
+#if 0
+void ltt_statedump_work_func(struct work_struct *work)
+{
+	if (atomic_dec_and_test(&kernel_threads_to_run)) {
+		/* If we are the last thread, wake up do_ltt_statedump */
+		wake_up_process(work_wake_task);
+	}
+}
+#endif
+
+void ltt_statedump_work_func(void *sem)
+{
+        /* Our job is just to release the semaphore so
+         * that we are sure that each CPU has been in syscall
+         * mode before the end of ltt_statedump_thread */
+        up((struct semaphore *)sem);
+}
+
+static int do_ltt_statedump(void)
+{
+	struct semaphore work_sema4;
+	int cpu;
+
+	printk(KERN_DEBUG "do_ltt_statedump\n");
+ 	ltt_enumerate_process_states();
+	ltt_enumerate_file_descriptors();
+	list_modules();
+	ltt_enumerate_vm_maps();
+	list_interrupts();
+	ltt_enumerate_network_ip_interface();
+
+	/* Fire off a work queue on each CPU. Their sole purpose in life
+	 * is to guarantee that each CPU has been in a state where is was in
+	 * syscall mode (i.e. not in a trap, an IRQ or a soft IRQ) */
+	sema_init(&work_sema4, 1 - num_online_cpus());
+	lock_cpu_hotplug();
+#if 0
+	atomic_set(&kernel_threads_to_run, num_online_cpus());
+	work_wake_task = current;
+	__set_current_state(TASK_UNINTERRUPTIBLE);
+#endif
+        for_each_online_cpu(cpu)
+        {
+                INIT_WORK(&cpu_work[cpu], ltt_statedump_work_func, &work_sema4);
+                schedule_delayed_work_on(cpu,&cpu_work[cpu],0);
+        }
+	unlock_cpu_hotplug();
+	down(&work_sema4);
+	/* Wait for all threads to run */
+#if 0
+	schedule();
+	BUG_ON(atomic_read(&kernel_threads_to_run) != 0);
+#endif
+	/* Our work is done */
+	printk(KERN_DEBUG "do_ltt_statedump end\n");
+	MARK(list_statedump_end, MARK_NOARGS);
+	return 0;
+}
+
+int ltt_statedump_start(struct ltt_trace_struct *trace)
+{
+	printk(KERN_DEBUG "ltt_statedump_start\n");
+
+	return do_ltt_statedump();
+}
+
+/* Dynamic facility. */
+static int __init statedump_init(void)
+{
+	int ret;
+	printk(KERN_INFO "LTT : ltt-facility-statedump init\n");
+	ret = ltt_module_register(LTT_FUNCTION_STATEDUMP,
+			ltt_statedump_start,THIS_MODULE);
+	return ret;
+}
+
+static void __exit statedump_exit(void)
+{
+	printk(KERN_INFO "LTT : ltt-facility-statedump exit\n");
+	ltt_module_unregister(LTT_FUNCTION_STATEDUMP);
+}
+
+module_init(statedump_init)
+module_exit(statedump_exit)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Jean-Hugues Deschenes");
+MODULE_DESCRIPTION("Linux Trace Toolkit Statedump");
+
Index: linux/ltt/ltt-syscall.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-syscall.c	2007-08-30 12:39:34.172645000 +0100
@@ -0,0 +1,181 @@
+/******************************************************************************
+ * ltt-syscall.c
+ *
+ * Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ * March 2006
+ *
+ * LTT userspace tracing syscalls
+ */
+
+#include <linux/errno.h>
+#include <linux/syscalls.h>
+#include <linux/sched.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+#include <asm/uaccess.h>
+
+/* User event logging function */
+static inline int trace_user_event(unsigned int facility_id,
+		unsigned int event_id,
+		void __user *data, size_t data_size, int blocking,
+		int high_priority)
+{
+	int ret = 0;
+	unsigned int index;
+	struct ltt_channel_struct *channel;
+	struct ltt_trace_struct *trace;
+	void *transport_data;
+	void *buffer = NULL;
+	size_t real_to_base = 0; /* buffer allocated on arch_size alignment */
+	size_t *to_base = &real_to_base;
+	size_t real_to = 0;
+	size_t *to = &real_to;
+	size_t real_len = 0;
+	size_t *len = &real_len;
+	size_t reserve_size;
+	size_t slot_size;
+	u64 tsc;
+	struct user_dbg_data dbg;
+
+	dbg.avail_size = 0;
+	dbg.write = 0;
+	dbg.read = 0;
+
+	if (ltt_traces.num_active_traces == 0)
+		return 0;
+
+	/* Assume that the padding for alignment starts at a
+	 * sizeof(void *) address. */
+
+	reserve_size = data_size;
+
+	if (high_priority)
+		index = GET_CHANNEL_INDEX(processes);
+	else
+		index = GET_CHANNEL_INDEX(cpu);
+
+	preempt_disable();
+
+	if (blocking) {
+		/* User space requested blocking mode :
+		 * If one of the active traces has free space below a specific
+		 * threshold value, we reenable preemption and block. */
+block_test_begin:
+		list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+			if (!trace->active)
+  				continue;
+
+			if (trace->ops->user_blocking(trace, index, data_size,
+							&dbg))
+ 				goto block_test_begin;
+		}
+	}
+	ltt_nesting[smp_processor_id()]++;
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		if (!trace->active)
+			continue;
+		channel = ltt_get_channel_from_index(trace, index);
+		slot_size = 0;
+		buffer = ltt_reserve_slot(trace, channel, &transport_data,
+			reserve_size, &slot_size, &tsc);
+		if (!buffer) {
+			if (blocking)
+				trace->ops->user_errors(trace,
+					index, data_size, &dbg);
+			continue; /* buffer full */
+		}
+		*to_base = *to = *len = 0;
+		buffer = ltt_write_event_header(trace, channel, buffer,
+			facility_id, event_id,
+			reserve_size, tsc);
+		/* Hope the user pages are not swapped out. In the rare case
+		 * where it is, the slot will be zeroed and EFAULT returned. */
+		if (__copy_from_user_inatomic(buffer+*to_base+*to, data,
+					data_size))
+			ret = -EFAULT;	/* Data is garbage in the slot */
+		ltt_commit_slot(channel, &transport_data, buffer, slot_size);
+		if (ret != 0)
+			break;
+	}
+	ltt_nesting[smp_processor_id()]--;
+	preempt_enable_no_resched();
+	return ret;
+}
+
+asmlinkage long sys_ltt_trace_generic(unsigned int facility_id,
+		unsigned int event_id,
+		void __user *data,
+		size_t data_size,
+		int blocking,
+		int high_priority)
+{
+	if (!ltt_facility_user_access_ok(facility_id))
+		return -EPERM;
+	if (!access_ok(VERIFY_READ, data, data_size))
+			return -EFAULT;
+
+	return trace_user_event(facility_id, event_id, data, data_size,
+			blocking, high_priority);
+}
+
+asmlinkage long sys_ltt_register_generic(unsigned int __user *facility_id,
+		const struct user_facility_info __user *info)
+{
+	struct user_facility_info kinfo;
+	int fac_id;
+	unsigned int i;
+
+	/* Check if the process has already registered the maximum number of
+	 * allowed facilities */
+	if (current->ltt_facilities[LTT_FAC_PER_PROCESS-1] != 0)
+		return -EPERM;
+
+	if (copy_from_user(&kinfo, info, sizeof(*info)))
+		return -EFAULT;
+
+	/* Verify if facility is already registered */
+	printk(KERN_DEBUG "LTT register generic for %s\n", kinfo.name);
+	fac_id = ltt_facility_verify(LTT_FACILITY_TYPE_USER,
+				kinfo.name,
+				kinfo.num_events,
+				kinfo.checksum,
+				kinfo.int_size,
+				kinfo.long_size,
+				kinfo.pointer_size,
+				kinfo.size_t_size,
+				kinfo.alignment);
+
+	printk(KERN_DEBUG "LTT verify return %d\n", fac_id);
+	if (fac_id > 0)
+		goto found;
+
+	fac_id = ltt_facility_register(LTT_FACILITY_TYPE_USER,
+				kinfo.name,
+				kinfo.num_events,
+				kinfo.checksum,
+				kinfo.int_size,
+				kinfo.long_size,
+				kinfo.pointer_size,
+				kinfo.size_t_size,
+				kinfo.alignment);
+
+	printk(KERN_DEBUG "LTT register return %d\n", fac_id);
+	if (fac_id == 0)
+		return -EPERM;
+	if (fac_id < 0)
+		return fac_id;	/* Error */
+found:
+	get_task_struct(current->group_leader);
+	for (i = 0; i < LTT_FAC_PER_PROCESS; i++) {
+		if (current->group_leader->ltt_facilities[i] == 0) {
+			current->group_leader->ltt_facilities[i] =
+				(uint8_t)fac_id;
+			break;
+		}
+	}
+	put_task_struct(current->group_leader);
+	/* Write facility_id */
+	put_user((unsigned int)fac_id, facility_id);
+	return 0;
+}
Index: linux/ltt/ltt-test-tsc.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-test-tsc.c	2007-08-30 12:39:34.194644000 +0100
@@ -0,0 +1,116 @@
+/* ltt-test-tsc.c
+ *
+ * Test TSC synchronization
+ */
+#include <linux/module.h>
+#include <linux/timer.h>
+#include <asm/timex.h>
+#include <linux/jiffies.h>
+#include <linux/cpu.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+#include <linux/ltt-tracer.h>
+
+/* Workaround for 32 bits arch which does not define this */
+#ifdef CONFIG_X86_32
+#define get_cycles_sync get_cycles
+#endif
+
+#define MAX_CYCLES_DELTA 1000ULL
+
+static DEFINE_PER_CPU(cycles_t, tsc_count) = 0;
+static DECLARE_MUTEX(tscsync_mutex);
+
+static DEFINE_PER_CPU(int, wait_sync);
+static DEFINE_PER_CPU(int, wait_end_sync);
+
+int ltt_tsc_is_sync = 1;
+EXPORT_SYMBOL(ltt_tsc_is_sync);
+cycles_t ltt_last_tsc = 0;
+EXPORT_SYMBOL(ltt_last_tsc);
+
+/* Mark it noinline so we make sure it is not unrolled.
+ * Wait until value is reached. */
+static noinline void tsc_barrier(long wait_cpu, int value)
+{
+	sync_core();
+	per_cpu(wait_sync, smp_processor_id())--;
+	do {
+		barrier();
+	} while(unlikely(per_cpu(wait_sync, wait_cpu) > value));
+	__get_cpu_var(tsc_count) = get_cycles_sync();
+}
+
+/* worker thread called on each CPU.
+ * First wait with interrupts enabled, then wait with interrupt disabled,
+ * for precision. We are already bound to one CPU. */
+static void test_sync(void *arg)
+{
+	long wait_cpu = (long)arg;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	tsc_barrier(wait_cpu, 1); /* Make sure the instructions are in I-CACHE */
+	tsc_barrier(wait_cpu, 0);
+	per_cpu(wait_end_sync, smp_processor_id())--;
+	do {
+		barrier();
+	} while(unlikely(per_cpu(wait_end_sync, wait_cpu) > 0));
+	local_irq_restore(flags);
+}
+
+/* Do loops (making sure no unexpected event changes the timing), keep the
+ * best one. The result of each loop is the highest tsc delta between the
+ * master CPU and the slaves. */
+static int test_tsc_synchronization(void)
+{
+	long cpu, master;
+	cycles_t max_diff = 0, diff, best_loop, worse_loop = 0;
+	int i;
+
+	down(&tscsync_mutex);
+	preempt_disable();
+	master = smp_processor_id();
+	for_each_online_cpu(cpu) {
+		if (master == cpu)
+			continue;
+		best_loop = ULLONG_MAX;
+		for (i = 0; i < 10; i++) {
+			/* Each CPU (master and slave) must decrement the
+			 * wait_sync value twice (one for priming in cache) */
+			//atomic_set(&wait_sync, 4);
+			//atomic_set(&wait_end_sync, 2);
+			per_cpu(wait_sync, master) = 2;
+			per_cpu(wait_sync, cpu) = 2;
+			per_cpu(wait_end_sync, master) = 1;
+			per_cpu(wait_end_sync, cpu) = 1;
+			smp_call_function_single(cpu, test_sync,
+						(void*)master, 1, 0);
+			test_sync((void*)cpu);
+			diff = abs(per_cpu(tsc_count, cpu)
+				- per_cpu(tsc_count, master));
+			best_loop = min(best_loop, diff);
+			worse_loop = max(worse_loop, diff);
+		}
+		max_diff = max(best_loop, max_diff);
+	}
+	preempt_enable();
+	if (max_diff >= MAX_CYCLES_DELTA) {
+		printk(KERN_WARNING
+			"LTTng : Your timestamp counter is not reliable.\n"
+			"See LTTng documentation to find the "
+			"appropriate solution for your architecture.\n");
+		printk("TSC unsynchronized : %llu cycles delta is over "
+			"threshold %llu\n", max_diff, MAX_CYCLES_DELTA);
+	}
+	up(&tscsync_mutex);
+	return max_diff < MAX_CYCLES_DELTA;
+}
+EXPORT_SYMBOL_GPL(test_tsc_synchronization);
+
+static int __init tsc_test_init(void)
+{
+	ltt_tsc_is_sync = test_tsc_synchronization();
+	return 0;
+}
+__initcall(tsc_test_init);
Index: linux/ltt/ltt-tracer.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/ltt-tracer.c	2007-08-30 12:39:34.202644000 +0100
@@ -0,0 +1,938 @@
+/*
+ * ltt-tracer.c
+ *
+ * (C) Copyright	2005-2006 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Contains the kernel code for the Linux Trace Toolkit.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *	Karim Yaghmour (karim@opersys.com)
+ *	Tom Zanussi (zanussi@us.ibm.com)
+ *	Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  22/09/06 Move to the marker/probes mechanism. (Mathieu Desnoyers)
+ *  19/10/05, Complete lockless mechanism. (Mathieu Desnoyers)
+ *	27/05/05, Modular redesign and rewrite. (Mathieu Desnoyers)
+
+ * Comments :
+ * num_active_traces protects the functors. Changing the pointer is an atomic
+ * operation, but the functions can only be called when in tracing. It is then
+ * safe to unload a module in which sits a functor when no tracing is active.
+ *
+ * filter_control functor is protected by incrementing its module refcount.
+ *
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/ltt-facilities.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/cpu.h>
+#include <linux/kref.h>
+#include <asm/atomic.h>
+
+/* Traces list writer locking */
+DECLARE_MUTEX(ltt_traces_sem);
+
+static struct timer_list ltt_async_wakeup_timer;
+
+/* Default callbacks for modules */
+int ltt_run_filter_default(struct ltt_trace_struct *trace,
+	uint8_t fID, uint8_t eID)
+{
+	return 1;
+}
+
+int ltt_filter_control_default
+	(enum ltt_filter_control_msg msg, struct ltt_trace_struct *trace)
+{
+	return 0;
+}
+
+int ltt_statedump_default(struct ltt_trace_struct *trace)
+{
+	return 0;
+}
+
+
+
+/* Callbacks for registered modules */
+
+int (*ltt_filter_control_functor)
+	(enum ltt_filter_control_msg msg, struct ltt_trace_struct *trace) =
+					ltt_filter_control_default;
+struct module *ltt_filter_control_owner = NULL;
+
+/* These function pointers are protected by trace activation check */
+
+ltt_run_filter_functor ltt_run_filter = ltt_run_filter_default;
+EXPORT_SYMBOL_GPL(ltt_run_filter);
+struct module *ltt_run_filter_owner = NULL;
+
+// FIXME : integrate the filter in the logging chain.
+//
+int (*ltt_statedump_functor)(struct ltt_trace_struct *trace) =
+					ltt_statedump_default;
+struct module *ltt_statedump_owner = NULL;
+
+/* Module registration methods */
+
+int ltt_module_register(enum ltt_module_function name, void *function,
+		struct module *owner)
+{
+	int ret = 0;
+
+	switch (name) {
+		case LTT_FUNCTION_RUN_FILTER:
+			if (ltt_run_filter_owner != NULL) {
+				ret = -EEXIST;
+				goto end;
+			}
+			ltt_run_filter = (ltt_run_filter_functor)function;
+			ltt_run_filter_owner = owner;
+			break;
+		case LTT_FUNCTION_FILTER_CONTROL:
+			if (ltt_filter_control_owner != NULL) {
+				ret = -EEXIST;
+				goto end;
+			}
+			ltt_filter_control_functor =
+				(int (*)(enum ltt_filter_control_msg,
+				struct ltt_trace_struct *))function;
+			break;
+		case LTT_FUNCTION_STATEDUMP:
+			if (ltt_statedump_owner != NULL) {
+				ret = -EEXIST;
+				goto end;
+			}
+			ltt_statedump_functor =
+				(int (*)(struct ltt_trace_struct *))function;
+			ltt_statedump_owner = owner;
+			break;
+	}
+
+end:
+
+	return ret;
+}
+
+
+void ltt_module_unregister(enum ltt_module_function name)
+{
+	switch (name) {
+		case LTT_FUNCTION_RUN_FILTER:
+			ltt_run_filter = ltt_run_filter_default;
+			ltt_run_filter_owner = NULL;
+			/* Wait for preempt sections to finish */
+			synchronize_sched();
+			break;
+		case LTT_FUNCTION_FILTER_CONTROL:
+			ltt_filter_control_functor = ltt_filter_control_default;
+			ltt_filter_control_owner = NULL;
+			break;
+		case LTT_FUNCTION_STATEDUMP:
+			ltt_statedump_functor = ltt_statedump_default;
+			ltt_statedump_owner = NULL;
+			break;
+	}
+
+}
+
+EXPORT_SYMBOL_GPL(ltt_module_register);
+EXPORT_SYMBOL_GPL(ltt_module_unregister);
+
+static LIST_HEAD(ltt_transport_list);
+
+void ltt_transport_register(struct ltt_transport *transport)
+{
+	down(&ltt_traces_sem);
+	list_add_tail(&transport->node, &ltt_transport_list);
+	up(&ltt_traces_sem);
+}
+
+void ltt_transport_unregister(struct ltt_transport *transport)
+{
+	down(&ltt_traces_sem);
+	list_del(&transport->node);
+	up(&ltt_traces_sem);
+}
+
+EXPORT_SYMBOL_GPL(ltt_transport_register);
+EXPORT_SYMBOL_GPL(ltt_transport_unregister);
+
+
+static inline int is_channel_overwrite(enum ltt_channels chan,
+	enum trace_mode mode)
+{
+	switch (mode) {
+		case LTT_TRACE_NORMAL:
+			return 0;
+		case LTT_TRACE_FLIGHT:
+			switch (chan) {
+				case LTT_CHANNEL_FACILITIES:
+					return 0;
+				default:
+					return 1;
+			}
+		case LTT_TRACE_HYBRID:
+			switch (chan) {
+				case LTT_CHANNEL_CPU:
+					return 1;
+				default:
+					return 0;
+			}
+		default:
+			return 0;
+	}
+}
+
+
+void ltt_write_trace_header(struct ltt_trace_struct *trace,
+		struct ltt_trace_header *header)
+{
+	header->magic_number = LTT_TRACER_MAGIC_NUMBER;
+	header->major_version = LTT_TRACER_VERSION_MAJOR;
+	header->minor_version = LTT_TRACER_VERSION_MINOR;
+	header->float_word_order = 0;	 /* Kernel : no floating point */
+	header->arch_type = LTT_ARCH_TYPE;
+	header->arch_size = sizeof(void*);
+	header->arch_variant = LTT_ARCH_VARIANT;
+	switch (trace->mode) {
+		case LTT_TRACE_NORMAL:
+			header->flight_recorder = 0;
+			break;
+		case LTT_TRACE_FLIGHT:
+			header->flight_recorder = 1;
+			break;
+		case LTT_TRACE_HYBRID:
+			header->flight_recorder = 2;
+			break;
+		default:
+			header->flight_recorder = 0;
+	}
+
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	header->has_heartbeat = 1;
+	header->tsc_lsb_truncate = ltt_tsc_lsb_truncate;
+	header->tscbits = ltt_tscbits;
+#else
+	header->has_heartbeat = 0;
+	header->tsc_lsb_truncate = 0;
+	header->tscbits = 32;
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+
+#ifdef CONFIG_LTT_ALIGNMENT
+	header->has_alignment = sizeof(void*);
+#else
+	header->has_alignment = 0;
+#endif
+	header->freq_scale = trace->freq_scale;
+	header->start_freq = trace->start_freq;
+	header->start_tsc = trace->start_tsc;
+	header->start_monotonic = trace->start_monotonic;
+	header->start_time_sec = trace->start_time.tv_sec;
+	header->start_time_usec = trace->start_time.tv_usec;
+}
+EXPORT_SYMBOL_GPL(ltt_write_trace_header);
+
+static void trace_async_wakeup(struct ltt_trace_struct *trace)
+{
+	/* Must check each channel for pending read wakeup */
+	trace->ops->wakeup_channel(trace->channel.facilities);
+	trace->ops->wakeup_channel(trace->channel.interrupts);
+	trace->ops->wakeup_channel(trace->channel.processes);
+	trace->ops->wakeup_channel(trace->channel.modules);
+	trace->ops->wakeup_channel(trace->channel.network);
+	trace->ops->wakeup_channel(trace->channel.cpu);
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	trace->ops->wakeup_channel(trace->channel.compact);
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+}
+
+/* Timer to send async wakeups to the readers */
+static void async_wakeup(unsigned long data)
+{
+	struct ltt_trace_struct *trace;
+	preempt_disable();
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		trace_async_wakeup(trace);
+	}
+	preempt_enable();
+
+	del_timer(&ltt_async_wakeup_timer);
+	ltt_async_wakeup_timer.expires = jiffies + LTT_PERCPU_TIMER_INTERVAL;
+	add_timer(&ltt_async_wakeup_timer);
+}
+
+void ltt_wakeup_writers(struct work_struct *work)
+{
+	struct ltt_channel_buf_struct *ltt_buf =
+		container_of(work, struct ltt_channel_buf_struct, wake_writers);
+
+	wake_up_interruptible(&ltt_buf->write_wait);
+}
+EXPORT_SYMBOL_GPL(ltt_wakeup_writers);
+
+/* _ltt_trace_find :
+ * find a trace by given name.
+ *
+ * Returns a pointer to the trace structure, NULL if not found. */
+static struct ltt_trace_struct *_ltt_trace_find(char *trace_name)
+{
+	int compare;
+	struct ltt_trace_struct *trace, *found=NULL;
+
+	list_for_each_entry(trace, &ltt_traces.head, list) {
+		compare = strncmp(trace->trace_name, trace_name, NAME_MAX);
+
+		if (compare == 0) {
+			found = trace;
+			break;
+		}
+	}
+
+	return found;
+}
+
+/* This function must be called with traces semaphore held. */
+static int _ltt_trace_create(char *trace_name,	enum trace_mode mode,
+				struct ltt_trace_struct *new_trace)
+{
+	int err = EPERM;
+
+	if (_ltt_trace_find(trace_name) != NULL) {
+		printk(KERN_ERR "LTT : Trace %s already exists\n", trace_name);
+		err = EEXIST;
+		goto traces_error;
+	}
+	list_add_rcu(&new_trace->list, &ltt_traces.head);
+	synchronize_sched();
+	/* Everything went fine, finish creation */
+	return 0;
+
+	/* Error handling */
+traces_error:
+	return err;
+}
+
+
+void ltt_release_transport(struct kref *kref)
+{
+	struct ltt_trace_struct *trace = container_of(kref,
+			struct ltt_trace_struct, ltt_transport_kref);
+	trace->ops->remove_dirs(trace);
+}
+EXPORT_SYMBOL_GPL(ltt_release_transport);
+
+
+void ltt_release_trace(struct kref *kref)
+{
+	struct ltt_trace_struct *trace = container_of(kref,
+			struct ltt_trace_struct, kref);
+	kfree(trace);
+}
+EXPORT_SYMBOL_GPL(ltt_release_trace);
+
+static inline void prepare_chan_size_num(unsigned *subbuf_size,
+	unsigned *n_subbufs, unsigned default_size, unsigned default_n_subbufs)
+{
+	if (*subbuf_size == 0)
+		*subbuf_size = default_size;
+	if (*n_subbufs == 0)
+		*n_subbufs = default_n_subbufs;
+	*subbuf_size = 1 << get_count_order(*subbuf_size);
+	*n_subbufs = 1 << get_count_order(*n_subbufs);
+
+	/* Subbuf size and number must both be power of two */
+	WARN_ON(hweight32(*subbuf_size) != 1);
+	WARN_ON(hweight32(*n_subbufs) != 1);
+}
+
+static int ltt_trace_create(char *trace_name, char *trace_type,
+		enum trace_mode mode,
+		unsigned subbuf_size_low, unsigned n_subbufs_low,
+		unsigned subbuf_size_med, unsigned n_subbufs_med,
+		unsigned subbuf_size_high, unsigned n_subbufs_high)
+{
+	int err = 0;
+	struct ltt_trace_struct *new_trace, *trace;
+	unsigned long flags;
+	struct ltt_transport *tran, *transport = NULL;
+
+	prepare_chan_size_num(&subbuf_size_low, &n_subbufs_low,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW, LTT_DEFAULT_N_SUBBUFS_LOW);
+
+	prepare_chan_size_num(&subbuf_size_med, &n_subbufs_med,
+		LTT_DEFAULT_SUBBUF_SIZE_MED, LTT_DEFAULT_N_SUBBUFS_MED);
+
+	prepare_chan_size_num(&subbuf_size_high, &n_subbufs_high,
+		LTT_DEFAULT_SUBBUF_SIZE_HIGH, LTT_DEFAULT_N_SUBBUFS_HIGH);
+
+	new_trace = kzalloc(sizeof(struct ltt_trace_struct), GFP_KERNEL);
+	if (!new_trace) {
+		printk(KERN_ERR
+			"LTT : Unable to allocate memory for trace %s\n",
+			trace_name);
+		err = ENOMEM;
+		goto traces_error;
+	}
+
+	kref_init(&new_trace->kref);
+	kref_init(&new_trace->ltt_transport_kref);
+	new_trace->active = 0;
+	strncpy(new_trace->trace_name, trace_name, NAME_MAX);
+	new_trace->paused = 0;
+	new_trace->mode = mode;
+	new_trace->freq_scale = ltt_freq_scale();
+
+	down(&ltt_traces_sem);
+	list_for_each_entry(tran, &ltt_transport_list, node) {
+		if (!strcmp(tran->name, trace_type)) {
+			transport = tran;
+			break;
+		}
+	}
+
+	if (!transport) {
+		err = EINVAL;
+		printk(KERN_ERR	"LTT : Transport %s is not present.\n", trace_type);
+		up(&ltt_traces_sem);
+		goto trace_error;
+	}
+
+	if (!try_module_get(transport->owner)) {
+		err = ENODEV;
+		printk(KERN_ERR	"LTT : Can't lock transport module.\n");
+		up(&ltt_traces_sem);
+		goto trace_error;
+	}
+
+	trace = _ltt_trace_find(trace_name);
+	if(trace) {
+		printk(KERN_ERR	"LTT : Trace name %s already used.\n",
+			trace_name);
+		err = EEXIST;
+		goto trace_error;
+	}
+
+	new_trace->transport = transport;
+	new_trace->ops = &transport->ops;
+
+	err = new_trace->ops->create_dirs(new_trace);
+	if (err)
+		goto dirs_error;
+
+	local_irq_save(flags);
+	new_trace->start_freq = ltt_frequency();
+	new_trace->start_tsc = ltt_get_timestamp64();
+	do_gettimeofday(&new_trace->start_time);
+	local_irq_restore(flags);
+
+	/* Always put the facilities channel in non-overwrite mode :
+	 * This is a very low traffic channel and it can't afford to have its
+	 * data overwritten : this data (facilities info) is necessary to be
+	 * able to read the trace.
+	 *
+	 * WARNING : The heartbeat time _shouldn't_ write events in the
+	 * facilities channel as it would make the traffic too high. This is a
+	 * problematic case with flight recorder mode. FIXME
+	 */
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_FACILITIES_CHANNEL,
+			&new_trace->channel.facilities, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_FACILITIES, mode));
+	if (err != 0) {
+		goto facilities_error;
+	}
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_INTERRUPTS_CHANNEL,
+			&new_trace->channel.interrupts, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_INTERRUPTS, mode));
+	if (err != 0) {
+		goto interrupts_error;
+	}
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_PROCESSES_CHANNEL,
+			&new_trace->channel.processes, subbuf_size_med,
+			n_subbufs_med,
+			is_channel_overwrite(LTT_CHANNEL_PROCESSES, mode));
+	if (err != 0) {
+		goto processes_error;
+	}
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_MODULES_CHANNEL,
+			&new_trace->channel.modules, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_MODULES, mode));
+	if (err != 0) {
+		goto modules_error;
+	}
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.control_root,
+			LTT_NETWORK_CHANNEL,
+			&new_trace->channel.network, subbuf_size_low,
+			n_subbufs_low,
+			is_channel_overwrite(LTT_CHANNEL_NETWORK, mode));
+	if (err != 0) {
+		goto network_error;
+	}
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.trace_root,
+			LTT_CPU_CHANNEL,
+			&new_trace->channel.cpu, subbuf_size_high,
+			n_subbufs_high,
+			is_channel_overwrite(LTT_CHANNEL_CPU, mode));
+	if (err != 0) {
+		goto cpu_error;
+	}
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	err = new_trace->ops->create_channel(trace_name, new_trace,
+			new_trace->dentry.trace_root,
+			LTT_COMPACT_CHANNEL,
+			&new_trace->channel.compact, subbuf_size_high,
+			n_subbufs_high,
+			is_channel_overwrite(LTT_CHANNEL_COMPACT, mode));
+	if (err != 0) {
+		goto compact_error;
+	}
+#endif
+
+	err = _ltt_trace_create(trace_name, mode, new_trace);
+
+	if (err != 0)
+		goto lock_create_error;
+	up(&ltt_traces_sem);
+	return err;
+
+lock_create_error:
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	new_trace->ops->remove_channel(new_trace->channel.compact);
+compact_error:
+#endif
+	new_trace->ops->remove_channel(new_trace->channel.cpu);
+cpu_error:
+	new_trace->ops->remove_channel(new_trace->channel.network);
+network_error:
+	new_trace->ops->remove_channel(new_trace->channel.modules);
+modules_error:
+	new_trace->ops->remove_channel(new_trace->channel.processes);
+processes_error:
+	new_trace->ops->remove_channel(new_trace->channel.interrupts);
+interrupts_error:
+	new_trace->ops->remove_channel(new_trace->channel.facilities);
+facilities_error:
+	kref_put(&new_trace->ltt_transport_kref, ltt_release_transport);
+dirs_error:
+	module_put(transport->owner);
+trace_error:
+	kref_put(&new_trace->kref, ltt_release_trace);
+	up(&ltt_traces_sem);
+traces_error:
+	return err;
+}
+
+/* Must be called while sure that trace is in the list. */
+static int _ltt_trace_destroy(struct ltt_trace_struct	*trace)
+{
+	int err = EPERM;
+
+	if (trace == NULL) {
+		err = ENOENT;
+		goto traces_error;
+	}
+	if (trace->active) {
+		printk(KERN_ERR
+			"LTT : Can't destroy trace %s : tracer is active\n",
+			trace->trace_name);
+		err = EBUSY;
+		goto active_error;
+	}
+	/* Everything went fine */
+	list_del_rcu(&trace->list);
+	synchronize_sched();
+	/* If no more trace in the list, we can free the unused facilities */
+	if (list_empty(&ltt_traces.head))
+		ltt_facility_free_unused();
+	return 0;
+
+	/* error handling */
+active_error:
+traces_error:
+	return err;
+}
+
+/* Sleepable part of the destroy */
+static void __ltt_trace_destroy(struct ltt_trace_struct	*trace)
+{
+	trace->ops->finish_channel(trace->channel.facilities);
+	trace->ops->finish_channel(trace->channel.interrupts);
+	trace->ops->finish_channel(trace->channel.processes);
+	trace->ops->finish_channel(trace->channel.modules);
+	trace->ops->finish_channel(trace->channel.network);
+	trace->ops->finish_channel(trace->channel.cpu);
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	trace->ops->finish_channel(trace->channel.compact);
+#endif
+
+	flush_scheduled_work();
+
+	if (ltt_traces.num_active_traces == 0) {
+		/* We stop the asynchronous delivery of reader wakeup, but
+		 * we must make one last check for reader wakeups pending. */
+		del_timer(&ltt_async_wakeup_timer);
+	}
+	/* The currently destroyed trace is not in the trace list anymore,
+	 * so it's safe to call the async wakeup ourself. It will deliver
+	 * the last subbuffers. */
+	trace_async_wakeup(trace);
+
+	trace->ops->remove_channel(trace->channel.facilities);
+	trace->ops->remove_channel(trace->channel.interrupts);
+	trace->ops->remove_channel(trace->channel.processes);
+	trace->ops->remove_channel(trace->channel.modules);
+	trace->ops->remove_channel(trace->channel.network);
+	trace->ops->remove_channel(trace->channel.cpu);
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+	trace->ops->remove_channel(trace->channel.compact);
+#endif
+
+	kref_put(&trace->ltt_transport_kref, ltt_release_transport);
+
+	module_put(trace->transport->owner);
+
+	kref_put(&trace->kref, ltt_release_trace);
+}
+
+static int ltt_trace_destroy(char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace_struct* trace;
+
+	down(&ltt_traces_sem);
+	trace = _ltt_trace_find(trace_name);
+	err = _ltt_trace_destroy(trace);
+	if (err)
+		goto error;
+	up(&ltt_traces_sem);
+	__ltt_trace_destroy(trace);
+	return err;
+
+	/* Error handling */
+error:
+	up(&ltt_traces_sem);
+	return err;
+}
+
+#ifdef CONFIG_LTT_HEARTBEAT_EVENT
+static void full_heartbeat_ipi(void *info)
+{
+	struct ltt_trace_struct* trace = (struct ltt_trace_struct*)info;
+	#define HB_FULL_FLAGS (MF_DEFAULT | LF_TRACE | LF_CHANNEL | LF_FORCE)
+
+	/* Log a heartbeat event for each trace, each tracefile */
+	_MARK(HB_FULL_FLAGS,
+		core_time_heartbeat_full,
+		"%8b",
+		trace,
+		GET_CHANNEL_INDEX(facilities),
+		ltt_get_timestamp64());
+	_MARK(HB_FULL_FLAGS,
+		core_time_heartbeat_full,
+		"%8b",
+		trace,
+		GET_CHANNEL_INDEX(interrupts),
+		ltt_get_timestamp64());
+	_MARK(HB_FULL_FLAGS,
+		core_time_heartbeat_full,
+		"%8b",
+		trace,
+		GET_CHANNEL_INDEX(processes),
+		ltt_get_timestamp64());
+	_MARK(HB_FULL_FLAGS,
+		core_time_heartbeat_full,
+		"%8b",
+		trace,
+		GET_CHANNEL_INDEX(modules),
+		ltt_get_timestamp64());
+	_MARK(HB_FULL_FLAGS,
+		core_time_heartbeat_full,
+		"%8b",
+		trace,
+		GET_CHANNEL_INDEX(cpu),
+		ltt_get_timestamp64());
+	_MARK(HB_FULL_FLAGS,
+		core_time_heartbeat_full,
+		"%8b",
+		trace,
+		GET_CHANNEL_INDEX(network),
+		ltt_get_timestamp64());
+	_MARK(MF_DEFAULT | LF_TRACE | LF_COMPACT | LF_FORCE,
+		compact_time_heartbeat_full,
+		"%8b",
+		trace,
+		ltt_get_timestamp64());
+#undef HB_FULL_FLAGS
+}
+
+void write_full_tsc(struct ltt_trace_struct* trace)
+{
+	on_each_cpu(full_heartbeat_ipi, trace, 1, 1);
+}
+#else //CONFIG_LTT_HEARTBEAT_EVENT
+void write_full_tsc(struct ltt_trace_struct* trace)
+{
+}
+#endif //CONFIG_LTT_HEARTBEAT_EVENT
+
+/* must be called from within a traces lock. */
+static int _ltt_trace_start(struct ltt_trace_struct* trace)
+{
+	int err = 0;
+
+	if (trace == NULL) {
+		err = ENOENT;
+		goto traces_error;
+	}
+	if (trace->active)
+		printk(KERN_INFO "LTT : Tracing already active for trace %s\n",
+				trace->trace_name);
+	if (!try_module_get(ltt_run_filter_owner)) {
+		err = ENODEV;
+		printk(KERN_ERR "LTT : Can't lock filter module.\n");
+		goto get_ltt_run_filter_error;
+	}
+	if (ltt_traces.num_active_traces == 0) {
+#ifdef CONFIG_LTT_HEARTBEAT
+		if (ltt_heartbeat_trigger(LTT_HEARTBEAT_START)) {
+			err = ENODEV;
+			printk(KERN_ERR
+				"LTT : Heartbeat timer module not present.\n");
+			goto ltt_heartbeat_error;
+		}
+#endif //CONFIG_LTT_HEARTBEAT
+		init_timer(&ltt_async_wakeup_timer);
+		ltt_async_wakeup_timer.function = async_wakeup;
+		ltt_async_wakeup_timer.expires =
+			jiffies + LTT_PERCPU_TIMER_INTERVAL;
+		add_timer(&ltt_async_wakeup_timer);
+	}
+	/* Write a full 64 bits TSC in each trace. Used for successive
+	 * trace stop/start. */
+	write_full_tsc(trace);
+	trace->active = 1;
+	ltt_traces.num_active_traces++;	/* Read by trace points without
+					 * protection : be careful */
+	return err;
+
+	/* error handling */
+#ifdef CONFIG_LTT_HEARTBEAT
+ltt_heartbeat_error:
+#endif //CONFIG_LTT_HEARTBEAT
+	module_put(ltt_run_filter_owner);
+get_ltt_run_filter_error:
+traces_error:
+	return err;
+}
+
+static int ltt_trace_start(char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace_struct* trace;
+
+	down(&ltt_traces_sem);
+
+	trace = _ltt_trace_find(trace_name);
+	if (trace == NULL)
+		goto no_trace;
+	err = _ltt_trace_start(trace);
+
+	up(&ltt_traces_sem);
+
+	/* Call the kernel state dump.
+	 * Events will be mixed with real kernel events, it's ok.
+	 * Notice that there is no protection on the trace : that's exactly
+	 * why we iterate on the list and check for trace equality instead of
+	 * directly using this trace handle inside the logging function. */
+
+	ltt_facility_state_dump(trace);
+
+	if (!try_module_get(ltt_statedump_owner)) {
+		err = ENODEV;
+		printk(KERN_ERR
+			"LTT : Can't lock state dump module.\n");
+	} else {
+		ltt_statedump_functor(trace);
+		module_put(ltt_statedump_owner);
+	}
+
+	return err;
+
+	/* Error handling */
+no_trace:
+	up(&ltt_traces_sem);
+	return err;
+}
+
+
+/* must be called from within traces lock */
+static int _ltt_trace_stop(struct ltt_trace_struct* trace)
+{
+	int err = EPERM;
+
+	if (trace == NULL) {
+		err = ENOENT;
+		goto traces_error;
+	}
+	if (!trace->active)
+		printk(KERN_INFO "LTT : Tracing not active for trace %s\n",
+				trace->trace_name);
+	if (trace->active) {
+		trace->active = 0;
+		ltt_traces.num_active_traces--;
+		synchronize_sched(); /* Wait for each tracing to be finished */
+	}
+	if (ltt_traces.num_active_traces == 0) {
+#ifdef CONFIG_LTT_HEARTBEAT
+	/* stop the heartbeat if we are the last active trace */
+		ltt_heartbeat_trigger(LTT_HEARTBEAT_STOP);
+#endif //CONFIG_LTT_HEARTBEAT
+	}
+	module_put(ltt_run_filter_owner);
+	/* Everything went fine */
+	return 0;
+
+	/* Error handling */
+traces_error:
+	return err;
+}
+
+static int ltt_trace_stop(char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace_struct* trace;
+
+	down(&ltt_traces_sem);
+	trace = _ltt_trace_find(trace_name);
+	err = _ltt_trace_stop(trace);
+	up(&ltt_traces_sem);
+	return err;
+}
+
+
+/* Exported functions */
+
+int ltt_control(enum ltt_control_msg msg, char *trace_name, char *trace_type,
+		union ltt_control_args args)
+{
+	int err = EPERM;
+
+	printk(KERN_ALERT "ltt_control : trace %s\n", trace_name);
+	switch (msg) {
+		case LTT_CONTROL_START:
+			printk(KERN_DEBUG "Start tracing %s\n", trace_name);
+			err = ltt_trace_start(trace_name);
+			break;
+		case LTT_CONTROL_STOP:
+			printk(KERN_DEBUG "Stop tracing %s\n", trace_name);
+			err = ltt_trace_stop(trace_name);
+			break;
+		case LTT_CONTROL_CREATE_TRACE:
+			printk(KERN_DEBUG "Creating trace %s\n", trace_name);
+			err = ltt_trace_create(trace_name, trace_type,
+				args.new_trace.mode,
+				args.new_trace.subbuf_size_low,
+				args.new_trace.n_subbufs_low,
+				args.new_trace.subbuf_size_med,
+				args.new_trace.n_subbufs_med,
+				args.new_trace.subbuf_size_high,
+				args.new_trace.n_subbufs_high);
+			break;
+		case LTT_CONTROL_DESTROY_TRACE:
+			printk(KERN_DEBUG "Destroying trace %s\n", trace_name);
+			err = ltt_trace_destroy(trace_name);
+			break;
+	}
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_control);
+
+int ltt_filter_control(enum ltt_filter_control_msg msg, char *trace_name)
+{
+	int err;
+	struct ltt_trace_struct *trace;
+
+	printk(KERN_DEBUG "ltt_filter_control : trace %s\n", trace_name);
+	down(&ltt_traces_sem);
+	trace = _ltt_trace_find(trace_name);
+	if (trace == NULL) {
+		printk(KERN_ALERT
+			"Trace does not exist. Cannot proxy control request\n");
+		err = ENOENT;
+		goto trace_error;
+	}
+	if (!try_module_get(ltt_filter_control_owner)) {
+		err = ENODEV;
+		goto get_module_error;
+	}
+	switch (msg) {
+		case LTT_FILTER_DEFAULT_ACCEPT:
+			printk(KERN_DEBUG
+				"Proxy filter default accept %s\n", trace_name);
+			err = (*ltt_filter_control_functor)(msg, trace);
+			break;
+		case LTT_FILTER_DEFAULT_REJECT:
+			printk(KERN_DEBUG
+				"Proxy filter default reject %s\n", trace_name);
+			err = (*ltt_filter_control_functor)(msg, trace);
+			break;
+		default:
+			err = EPERM;
+	}
+	module_put(ltt_filter_control_owner);
+
+get_module_error:
+trace_error:
+	up(&ltt_traces_sem);
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_filter_control);
+
+static void __exit ltt_exit(void)
+{
+	struct ltt_trace_struct *trace;
+
+	down(&ltt_traces_sem);
+	/* Stop each trace and destroy them */
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		_ltt_trace_stop(trace);
+		_ltt_trace_destroy(trace);/* it's doing a synchronize_sched() */
+		__ltt_trace_destroy(trace);
+	}
+	up(&ltt_traces_sem);
+}
+
+module_exit(ltt_exit)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Tracer");
Index: linux/ltt/probes/Makefile
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/Makefile	2007-08-30 12:39:34.206645000 +0100
@@ -0,0 +1,13 @@
+# LTT probes makefile
+
+obj-$(CONFIG_LTT_PROBE_CORE)	+= ltt-probe-core.o
+obj-$(CONFIG_LTT_PROBE_COMPACT)	+= ltt-probe-compact.o
+obj-$(CONFIG_LTT_PROBE_FS)	+= ltt-probe-fs.o
+obj-$(CONFIG_LTT_PROBE_ARCH)	+= \
+			ltt-probe-kernel_arch_$(ARCH).o
+obj-$(CONFIG_LTT_PROBE_KERNEL)	+= ltt-probe-kernel.o
+obj-$(CONFIG_LTT_PROBE_LIST)	+= ltt-probe-list.o
+obj-$(CONFIG_LTT_PROBE_MM)	+= ltt-probe-mm.o
+obj-$(CONFIG_LTT_PROBE_NET)	+= ltt-probe-net.o
+obj-$(CONFIG_LTT_PROBE_LOCKING)	+= ltt-probe-locking.o
+obj-$(CONFIG_LTT_PROBE_STACK)	+= ltt-probe-stack_arch_$(ARCH).o
Index: linux/ltt/probes/ltt-probe-compact.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-compact.c	2007-08-30 12:39:34.212645000 +0100
@@ -0,0 +1,124 @@
+/*
+ * ltt-probe-compact.c
+ *
+ * compact probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "compact"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "compact_time_heartbeat", MARK_NOARGS },
+	{ "compact_time_heartbeat_full", "%8b" },
+	{ "compact_event_a", MARK_NOARGS, GET_CHANNEL_INDEX(compact) },
+	{ "compact_event_b", "%4b", GET_CHANNEL_INDEX(compact) },
+	{ "compact_event_c", "%8b", GET_CHANNEL_INDEX(compact) },
+	{ "compact_event_d", MARK_NOARGS, GET_CHANNEL_INDEX(compact) },
+	{ "compact_event_e", "%8b", GET_CHANNEL_INDEX(compact) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 0,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	/* First write a full heartbeat */
+	eID = 1;
+	probe_array[eID].fID = facility.id;
+	probe_array[eID].eID = eID;
+	probe_array[eID].align = facility.alignment;
+	probe_array[eID].callbacks[0] = ltt_serialize_data;
+	result = marker_set_probe(probe_array[eID].name,
+			probe_array[eID].format,
+			ltt_trace, &probe_array[eID]);
+	if (!result)
+		printk(KERN_INFO "LTT unable to register probe %s\n",
+			probe_array[eID].name);
+ 	ltt_write_full_tsc();
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		if (eID == 1)
+			continue;
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-core.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-core.c	2007-08-30 12:39:34.218645000 +0100
@@ -0,0 +1,109 @@
+/*
+ * ltt-probe-core.c
+ *
+ * Core probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "core"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "core_facility_load", "%s %4b %4b %4b %4b %4b %4b %4b",
+		GET_CHANNEL_INDEX(facilities) },
+	{ "core_facility_unload", "%4b",
+		GET_CHANNEL_INDEX(facilities) },
+	{ "core_time_heartbeat", MARK_NOARGS },
+	{ "core_time_heartbeat_full", "%8b" },
+	{ "core_state_dump_facility_load", "%s %4b %4b %4b %4b %4b %4b %4b",
+		GET_CHANNEL_INDEX(facilities) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-fs.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-fs.c	2007-08-30 12:39:34.223644000 +0100
@@ -0,0 +1,177 @@
+/*
+ * ltt-probe-fs.c
+ *
+ * FS probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+#include <asm/uaccess.h>
+
+/* Expects va args : (int elem_num, const char __user *s)
+ * Element size is implicit (sizeof(char)). */
+static char *ltt_serialize_fs_data(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	int align, const char *fmt, va_list *args)
+{
+	int elem_size;
+	int elem_num;
+	const char __user  *s;
+	unsigned long noncopy;
+
+	elem_num = va_arg(*args, int);
+	s = va_arg(*args, const char __user *);
+	elem_size = sizeof(*s);
+
+	if (align)
+		str += ltt_align((long)str, sizeof(int));
+	if (buffer)
+		*(int*)str = elem_num;
+	str += sizeof(int);
+
+	if (elem_num > 0) {
+		/* No alignment required for char */
+		if (buffer) {
+			noncopy = __copy_from_user_inatomic(str, s,
+					elem_num*elem_size);
+			memset(str+(elem_num*elem_size)-noncopy, 0, noncopy);
+		}
+		str += (elem_num*elem_size);
+	}
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+#define FACILITY_NAME "fs"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "fs_buffer_wait_start", "%p", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_buffer_wait_end", "%p", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_exec", "%s", GET_CHANNEL_INDEX(processes),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_open", "%d %s", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_close", "%u", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_read", "%u %zu", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_pread64", "%u %zu %8b", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_readv", "%lu %lu", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_write", "%u %zu", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_pwrite64", "%u %zu %8b", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_writev", "%lu %lu", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_lseek", "%u %8b %u", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_llseek", "%u %8b %u", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_ioctl", "%u %u %lu", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_select", "%d %8b", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_pollfd", "%d", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data },
+	{ "fs_read_data", "%u %zd %k", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data,
+		.callbacks[1] = ltt_serialize_fs_data},
+	{ "fs_write_data", "%u %zd %k", GET_CHANNEL_INDEX(cpu),
+		.callbacks[0] = ltt_serialize_data,
+		.callbacks[1] = ltt_serialize_fs_data },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		//probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel.c	2007-08-30 12:39:34.230644000 +0100
@@ -0,0 +1,164 @@
+/*
+ * ltt-probe-kernel.c
+ *
+ * kernel probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_process_fork", "%d %d %d", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_process_exit", "%d", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_process_free", "%d", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_process_wait", "%d", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_process_signal", "%d %d", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_sched_wait_task", "%d %ld", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_sched_try_wakeup", "%d %ld", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_sched_wakeup_new_task", "%d %ld",
+		GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_sched_migrate_task", "%d %ld %d",
+		GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_sched_schedule", "%d %d %ld", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_printk", "%p", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+	{ "kernel_vprintk", "%c %*:*v %p", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+	{ "kernel_timer_itimer_expired", "%d", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_timer_itimer_set", "%d %*.*r %*.*r",
+		GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_timer_set", "%lu %p %lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_timer_update_time", "%8b %*.*r %*.*r",
+		GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_timer_timeout", "%d", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_softirq_entry", "%lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_softirq_exit", "%lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_tasklet_low_entry", "%p %lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_tasklet_low_exit", "%p %lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_tasklet_high_entry", "%p %lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_tasklet_high_exit", "%p %lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_kthread_stop", "%d", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_kthread_stop_ret", "%d", GET_CHANNEL_INDEX(processes),
+		MF_DEFAULT },
+	{ "kernel_module_load", "%s", GET_CHANNEL_INDEX(modules),
+		MF_DEFAULT },
+	{ "kernel_module_free", "%s", GET_CHANNEL_INDEX(modules),
+		MF_DEFAULT },
+	{ "kernel_irq_entry", "%u %u", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+	{ "kernel_irq_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = _marker_set_probe(probe_array[eID].flags,
+				probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel_arch_arm.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel_arch_arm.c	2007-08-30 12:39:34.234648000 +0100
@@ -0,0 +1,108 @@
+/*
+ * ltt-probe-kernel_arch_arm.c
+ *
+ * kernel_arch probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_arch_trap_entry", "%ld %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_trap_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_ipc_call", "%u %d", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_kthread_create", "%ld %p",
+		GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel_arch_i386.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel_arch_i386.c	2007-08-30 12:39:34.240644000 +0100
@@ -0,0 +1,108 @@
+/*
+ * ltt-probe-kernel_arch_i386.c
+ *
+ * kernel_arch probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_arch_trap_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_trap_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_ipc_call", "%u %d", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_kthread_create", "%ld %p",
+		GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel_arch_mips.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel_arch_mips.c	2007-08-30 12:39:34.245646000 +0100
@@ -0,0 +1,108 @@
+/*
+ * ltt-probe-kernel_arch_mips.c
+ *
+ * kernel_arch probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_arch_trap_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_trap_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_ipc_call", "%u %d", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_kthread_create", "%ld %p",
+		GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel_arch_powerpc.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel_arch_powerpc.c	2007-08-30 12:39:34.251646000 +0100
@@ -0,0 +1,108 @@
+/*
+ * ltt-probe-kernel_arch_powerpc.c
+ *
+ * kernel_arch probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_arch_trap_entry", "%ld %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_trap_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_ipc_call", "%u %d", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_kthread_create", "%ld %p",
+		GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel_arch_ppc.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel_arch_ppc.c	2007-08-30 12:39:34.257644000 +0100
@@ -0,0 +1,108 @@
+/*
+ * ltt-probe-kernel_arch_ppc.c
+ *
+ * kernel_arch probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_arch_trap_entry", "%ld %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_trap_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_ipc_call", "%u %d", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_kthread_create", "%ld %p",
+		GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel_arch_sh.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel_arch_sh.c	2007-08-30 12:39:34.261645000 +0100
@@ -0,0 +1,106 @@
+/*
+ * ltt-probe-kernel_arch_sh.c
+ *
+ * kernel_arch probe
+ *
+ * Part of LTTng
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_arch_trap_entry", "%ld %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_trap_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_ipc_call", "%u %d", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_kthread_create", "%ld %p",
+		GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init arch_probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "%s LTT : Error in registering facility %s\n",
+			"SH", facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("%s LTT : Facility %s registered with id %hu\n", "SH", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "%s LTT unable to register probe %s\n",
+				"SH", probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit arch_probe_exit(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"%s LTT : Error in unregistering facility %s\n",
+			"SH", facility.name);
+}
+
+module_init(arch_probe_init);
+module_exit(arch_probe_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-kernel_arch_x86_64.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-kernel_arch_x86_64.c	2007-08-30 12:39:34.267645000 +0100
@@ -0,0 +1,108 @@
+/*
+ * ltt-probe-kernel_arch_x86_64.c
+ *
+ * kernel_arch probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "kernel_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "kernel_arch_trap_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_trap_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_entry", "%d %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_syscall_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_ipc_call", "%u %d", GET_CHANNEL_INDEX(cpu) },
+	{ "kernel_arch_kthread_create", "%ld %p",
+		GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-list.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-list.c	2007-08-30 12:39:34.271644000 +0100
@@ -0,0 +1,110 @@
+/*
+ * ltt-probe-list.c
+ *
+ * list probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "list"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "list_module", "%s %d %lu", GET_CHANNEL_INDEX(modules) },
+	{ "list_file_descriptor", "%s %d %u", GET_CHANNEL_INDEX(cpu) },
+	{ "list_vm_map", "%d %lu %lu %lu %lu %lu", GET_CHANNEL_INDEX(cpu) },
+	{ "list_interrupt", "%s %s %u", GET_CHANNEL_INDEX(interrupts) },
+	{ "list_process_state", "%d %d %s %d %d %d %d %d",
+		GET_CHANNEL_INDEX(processes) },
+	{ "list_network_ip_interface", "%s %4b %d",
+		GET_CHANNEL_INDEX(network) },
+	{ "list_statedump_end", MARK_NOARGS, GET_CHANNEL_INDEX(processes) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-locking.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-locking.c	2007-08-30 12:39:34.277646000 +0100
@@ -0,0 +1,114 @@
+/*
+ * ltt-probe-locking.c
+ *
+ * locking probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "locking"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "locking_hardirqs_on", "%lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+	{ "locking_hardirqs_off", "%lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+	{ "locking_softirqs_on", "%lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+	{ "locking_softirqs_off", "%lu", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+	{ "locking_lock_acquire", "%lu %u %p %d", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+	{ "locking_lock_release", "%lu %p %d", GET_CHANNEL_INDEX(cpu),
+		MF_DEFAULT & ~MF_LOCKDEP },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = _marker_set_probe(probe_array[eID].flags,
+				probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-mm.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-mm.c	2007-08-30 12:39:34.281646000 +0100
@@ -0,0 +1,109 @@
+/*
+ * ltt-probe-mm.c
+ *
+ * mm probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "mm"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "mm_handle_fault_entry", "%lu %ld", GET_CHANNEL_INDEX(cpu) },
+	{ "mm_handle_fault_exit", MARK_NOARGS, GET_CHANNEL_INDEX(cpu) },
+	{ "mm_filemap_wait_start", "%p", GET_CHANNEL_INDEX(cpu) },
+	{ "mm_filemap_wait_end", "%p", GET_CHANNEL_INDEX(cpu) },
+	{ "mm_page_alloc", "%u %p", GET_CHANNEL_INDEX(cpu) },
+	{ "mm_page_free", "%u %p", GET_CHANNEL_INDEX(cpu) },
+	{ "mm_swap_in", "%lu", GET_CHANNEL_INDEX(cpu) },
+	{ "mm_swap_out", "%p", GET_CHANNEL_INDEX(cpu) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-net.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-net.c	2007-08-30 12:39:34.287645000 +0100
@@ -0,0 +1,109 @@
+/*
+ * ltt-probe-net.c
+ *
+ * Network probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ltt-tracer.h>
+
+
+#define FACILITY_NAME "net"
+
+static struct ltt_probe_data probe_array[] =
+{
+	{ "net_socket_sendmsg", "%p %d %d %d %zu", GET_CHANNEL_INDEX(cpu) },
+	{ "net_socket_recvmsg", "%p %d %d %d %zu", GET_CHANNEL_INDEX(cpu) },
+	{ "net_socket_create", "%p %d %d %d %d", GET_CHANNEL_INDEX(cpu) },
+	{ "net_socket_call", "%d %lu", GET_CHANNEL_INDEX(cpu) },
+	{ "net_dev_xmit", "%p %2b", GET_CHANNEL_INDEX(cpu) },
+	{ "net_dev_receive", "%p %2b", GET_CHANNEL_INDEX(cpu) },
+	{ "net_insert_ifa", "%s %4b", GET_CHANNEL_INDEX(network) },
+	{ "net_del_ifa", "%s", GET_CHANNEL_INDEX(network) },
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t eID;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].name,
+				strlen(probe_array[eID].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[eID].format,
+				strlen(probe_array[eID].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		probe_array[eID].fID = facility.id;
+		probe_array[eID].eID = eID;
+		probe_array[eID].align = facility.alignment;
+		probe_array[eID].callbacks[0] = ltt_serialize_data;
+		result = marker_set_probe(probe_array[eID].name,
+				probe_array[eID].format,
+				ltt_trace, &probe_array[eID]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[eID].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t eID;
+	int err;
+
+	for (eID = 0; eID < NUM_PROBES; eID++) {
+		marker_remove_probe(probe_array[eID].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-stack_arch_i386.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-stack_arch_i386.c	2007-08-30 12:39:34.295644000 +0100
@@ -0,0 +1,423 @@
+/*
+ * ltt-probe-stack_arch_i386.c
+ *
+ * stack arch i386 probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ptrace.h>
+#include <linux/ltt-tracer.h>
+
+struct stack_dump_args {
+	unsigned long ebp;
+};
+
+#ifdef CONFIG_LTT_KERNEL_STACK
+/* Kernel specific trace stack dump routines, from arch/i386/kernel/traps.c */
+
+/* Event kernel_dump structures
+ * ONLY USE IT ON THE CURRENT STACK.
+ * It does not protect against other stack modifications and _will_
+ * cause corruption upon race between threads.
+ *
+ * FIXME : using __kernel_text_address here can break things if a module is
+ * loaded during tracing. The real function pointers are OK : if they are on our
+ * stack, it means that the module refcount must not be 0, but the problem comes
+ * from the "false positives" : that that appears to be a function pointer.
+ * The solution to this problem :
+ * Without frame pointers, we have one version with spinlock irqsave (never call
+ * this in NMI context, and another with a trylock, which can fail. */
+
+static inline int dump_valid_stack_ptr(struct thread_info *tinfo, void *p)
+{
+	return	p > (void *)tinfo &&
+		p < (void *)tinfo + THREAD_SIZE - 3;
+}
+
+static inline unsigned long dump_context_stack(struct thread_info *tinfo,
+		unsigned long *stack, unsigned long ebp,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	unsigned long addr;
+
+#ifdef	CONFIG_FRAME_POINTER
+	while (dump_valid_stack_ptr(tinfo, (void *)ebp)) {
+		unsigned long new_ebp;
+		if (buffer != NULL) {
+			unsigned long *dest = (unsigned long*)*str;
+			addr = *(unsigned long *)(ebp + 4);
+			*dest = addr;
+		} else {
+			(*len)++;
+		}
+		(*str) += sizeof(unsigned long);
+		new_ebp = *(unsigned long *)ebp;
+		if (new_ebp <= ebp)
+			break;
+		ebp = new_ebp;
+	}
+#else
+	while (dump_valid_stack_ptr(tinfo, stack)) {
+		addr = *stack++;
+		if (__kernel_text_address(addr)) {
+			if (buffer != NULL) {
+				unsigned long *dest = (unsigned long*)*str;
+				*dest = addr;
+			} else {
+				(*len)++;
+			}
+			(*str) += sizeof(unsigned long);
+		}
+	}
+#endif
+	return ebp;
+}
+
+static void dump_trace(unsigned long * stack,
+		unsigned long ebp,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	while (1) {
+		struct thread_info *context;
+		context = (struct thread_info *)
+			((unsigned long)stack & (~(THREAD_SIZE - 1)));
+		ebp = dump_context_stack(context, stack, ebp,
+				buffer, str, len);
+		stack = (unsigned long*)context->previous_esp;
+		if (!stack)
+			break;
+	}
+}
+
+static char *ltt_serialize_kernel_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	int align, const char *fmt, va_list *args)
+{
+	struct stack_dump_args *stargs;
+
+	/* Ugly and crude argument passing hack part 2. */
+	stargs = (struct stack_dump_args *)fmt;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(unsigned long)));
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(unsigned long));
+	dump_trace((unsigned long*)stargs, stargs->ebp, buffer, &str,
+			&closure->cb_args[0]);
+
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_kernel_stack(const struct __mark_marker_data *mdata,
+	const char *fmt, ...)
+{
+	struct stack_dump_args stargs;
+	va_list args;
+	struct pt_regs *regs;
+
+#ifndef CONFIG_LTT_KERNEL_STACK
+	return;
+#endif
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || user_mode_vm(regs)))
+		goto end;
+	/* Grab ebp right from our regs */
+	asm ("movl %%ebp, %0" : "=r" (stargs.ebp) : );
+	/* Ugly and crude argument passing hack part 1. */
+	ltt_vtrace(mdata, (const char*)&stargs, args);
+end:
+	va_end(args);
+}
+#endif //CONFIG_LTT_KERNEL_STACK
+
+#ifdef CONFIG_LTT_PROCESS_STACK
+/* FIXME : how could we autodetect this.... ?!? disabled for now. */
+#if 0
+static void dump_fp_process_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t next_ebp;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+
+	if (buffer)
+		*dest = regs->eip;
+	else
+		(*len)++;
+	dest++;
+
+	/* Start at the topmost ebp */
+	iter = (uint32_t*)regs->ebp;
+
+	/* Keep on going until we reach the end of the process' stack limit or
+	 * find an invalid ebp. */
+	while (!get_user(next_ebp, iter)) {
+		/* If another thread corrupts the thread user space stack
+		 * concurrently */
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		if (next_ebp <= (uint32_t)(iter+1))
+			break;
+		if (get_user(addr, (uint32_t*)next_ebp+1))
+			break;
+		if (buffer)
+			*dest = addr;
+		else
+			(*len)++;
+		dest++;
+		iter = (uint32_t*)next_ebp;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+#endif //0
+
+static void dump_nofp_process_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+	uint32_t *prev_iter, *beg_iter;
+
+	if (buffer)
+		*dest = regs->eip;
+	else
+		(*len)++;
+	dest++;
+
+	/* FIXME : currently detects code addresses from executable only,
+	 * not libraries */
+	/* Start at the top of the user stack */
+	prev_iter = beg_iter = iter = (uint32_t*) regs->esp;
+
+	while (!get_user(addr, iter)) {
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		/* Does this LOOK LIKE an address in the program */
+		if (addr > current->mm->start_code &&
+			addr < current->mm->end_code) {
+			if (buffer)
+				*dest = addr;
+			else
+				(*len)++;
+			dest++;
+		}
+		prev_iter = iter;
+		iter++;
+		if (iter >
+			prev_iter + CONFIG_LTT_PROCESS_MAX_FUNCTION_STACK)
+			break;
+		if (iter > beg_iter + CONFIG_LTT_PROCESS_MAX_STACK_LEN)
+			break;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+
+static char *ltt_serialize_process_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	int align, const char *fmt, va_list *args)
+{
+	struct pt_regs *regs;
+
+	/* Ugly and crude argument passing hack part 2 (revisited). */
+	regs = (struct pt_regs *)fmt;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(unsigned long)));
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(unsigned long));
+	dump_nofp_process_stack(regs, buffer, &str, &closure->cb_args[0]);
+
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_process_stack(const struct __mark_marker_data *mdata,
+	const char *fmt, ...)
+{
+	va_list args;
+	struct pt_regs *regs;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || !user_mode_vm(regs)))
+		goto end;
+	/* Ugly and crude argument passing hack part 1 (revisited). */
+	ltt_vtrace(mdata, (const char*)regs, args);
+end:
+	va_end(args);
+}
+#endif //CONFIG_LTT_PROCESS_STACK
+
+#define FACILITY_NAME "stack_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+#ifdef CONFIG_LTT_KERNEL_STACK
+	{ "stack_arch_irq_dump_kernel_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+		.eID = 0 },
+	{ "stack_arch_nmi_dump_kernel_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+		.eID = 1 },
+	{ "stack_arch_syscall_dump_kernel_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+		.eID = 2 },
+#endif //CONFIG_LTT_KERNEL_STACK
+#ifdef CONFIG_LTT_PROCESS_STACK
+	{ "stack_arch_irq_dump_process32_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_process_stack,
+		.callbacks[0] = ltt_serialize_process_stack,
+		.eID = 3 },
+	{ "stack_arch_syscall_dump_process32_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_process_stack,
+		.callbacks[0] = ltt_serialize_process_stack,
+		.eID = 4 },
+#endif //CONFIG_LTT_PROCESS_STACK
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t i;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (i = 0; i < NUM_PROBES; i++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[i].name,
+				strlen(probe_array[i].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[i].format,
+				strlen(probe_array[i].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (i = 0; i < NUM_PROBES; i++) {
+		probe_array[i].fID = facility.id;
+		probe_array[i].align = facility.alignment;
+		result = marker_set_probe(probe_array[i].name,
+				probe_array[i].format,
+				probe_array[i].probe_func, &probe_array[i]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[i].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t i;
+	int err;
+
+	for (i = 0; i < NUM_PROBES; i++) {
+		marker_remove_probe(probe_array[i].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/ltt/probes/ltt-probe-stack_arch_x86_64.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux/ltt/probes/ltt-probe-stack_arch_x86_64.c	2007-08-30 12:39:34.303644000 +0100
@@ -0,0 +1,691 @@
+/*
+ * ltt-probe-stack_arch_x86_64.c
+ *
+ * stack arch x86_64 probe
+ *
+ * Part of LTTng
+ *
+ * Mathieu Desnoyers, March 2007
+ *
+ * Licensed under the GPLv2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <linux/marker.h>
+#include <linux/ltt-facilities.h>
+#include <linux/ptrace.h>
+#include <linux/ltt-tracer.h>
+
+#include <asm/desc_defs.h>
+#include <asm/user32.h>
+
+#ifdef CONFIG_LTT_KERNEL_STACK
+/* Kernel specific trace stack dump routines, from arch/x86_64/kernel/traps.c */
+
+/* Event kernel_dump structures
+ * ONLY USE IT ON THE CURRENT STACK.
+ * It does not protect against other stack modifications and _will_
+ * cause corruption upon race between threads.
+ * FIXME : using __kernel_text_address here can break things if a module is
+ * loaded during tracing. The real function pointers are OK : if they are on our
+ * stack, it means that the module refcount must not be 0, but the problem comes
+ * from the "false positives" : that that appears to be a function pointer.
+ * The solution to this problem :
+ * Without frame pointers, we have one version with spinlock irqsave (never call
+ * this in NMI context, and another with a trylock, which can fail. */
+
+/* From traps.c */
+
+static unsigned long *trace_in_exception_stack(unsigned cpu,
+					unsigned long stack,
+					unsigned *usedp, const char **idp)
+{
+	static char ids[][8] = {
+		[DEBUG_STACK - 1] = "#DB",
+		[NMI_STACK - 1] = "NMI",
+		[DOUBLEFAULT_STACK - 1] = "#DF",
+		[STACKFAULT_STACK - 1] = "#SS",
+		[MCE_STACK - 1] = "#MC",
+#if DEBUG_STKSZ > EXCEPTION_STKSZ
+		[N_EXCEPTION_STACKS ... N_EXCEPTION_STACKS + DEBUG_STKSZ / EXCEPTION_STKSZ - 2] = "#DB[?]"
+#endif
+	};
+	unsigned k;
+
+	/*
+	 * Iterate over all exception stacks, and figure out whether
+	 * 'stack' is in one of them:
+	 */
+	for (k = 0; k < N_EXCEPTION_STACKS; k++) {
+		unsigned long end = per_cpu(orig_ist, cpu).ist[k];
+		/*
+		 * Is 'stack' above this exception frame's end?
+		 * If yes then skip to the next frame.
+		 */
+		if (stack >= end)
+			continue;
+		/*
+		 * Is 'stack' above this exception frame's start address?
+		 * If yes then we found the right frame.
+		 */
+		if (stack >= end - EXCEPTION_STKSZ) {
+			/*
+			 * Make sure we only iterate through an exception
+			 * stack once. If it comes up for the second time
+			 * then there's something wrong going on - just
+			 * break out and return NULL:
+			 */
+			if (*usedp & (1U << k))
+				break;
+			*usedp |= 1U << k;
+			*idp = ids[k];
+			return (unsigned long *)end;
+		}
+		/*
+		 * If this is a debug stack, and if it has a larger size than
+		 * the usual exception stacks, then 'stack' might still
+		 * be within the lower portion of the debug stack:
+		 */
+#if DEBUG_STKSZ > EXCEPTION_STKSZ
+		if (k == DEBUG_STACK - 1 && stack >= end - DEBUG_STKSZ) {
+			unsigned j = N_EXCEPTION_STACKS - 1;
+
+			/*
+			 * Black magic. A large debug stack is composed of
+			 * multiple exception stack entries, which we
+			 * iterate through now. Dont look:
+			 */
+			do {
+				++j;
+				end -= EXCEPTION_STKSZ;
+				ids[j][4] = '1' + (j - N_EXCEPTION_STACKS);
+			} while (stack < end - EXCEPTION_STKSZ);
+			if (*usedp & (1U << j))
+				break;
+			*usedp |= 1U << j;
+			*idp = ids[j];
+			return (unsigned long *)end;
+		}
+#endif
+	}
+	return NULL;
+}
+
+/*
+ * x86-64 can have up to three kernel stacks:
+ * process stack
+ * interrupt stack
+ * severe exception (double fault, nmi, stack fault, debug, mce) hardware stack
+ */
+
+static inline int valid_stack_ptr(struct thread_info *tinfo, void *p)
+{
+	void *t = (void *)tinfo;
+        return p > t && p < t + THREAD_SIZE - 3;
+}
+
+/* Must be called with preemption disabled */
+static void dump_trace(unsigned long *stack,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	const unsigned cpu = smp_processor_id();
+	unsigned long *irqstack_end = (unsigned long *)cpu_pda(cpu)->irqstackptr;
+	unsigned used = 0;
+	struct thread_info *tinfo;
+
+#define HANDLE_STACK(cond) \
+	do while (cond) { \
+		unsigned long addr = *stack++; \
+		if (__kernel_text_address(addr)) { \
+			/* \
+			 * If the address is either in the text segment of the \
+			 * kernel, or in the region which contains vmalloc'ed \
+			 * memory, it *may* be the address of a calling \
+			 * routine; if so, print it so that someone tracing \
+			 * down the cause of the crash will be able to figure \
+			 * out the call path that was taken. \
+			 */ \
+			if (buffer != NULL) { \
+				unsigned long *dest = (unsigned long*)*str; \
+				*dest = addr; \
+			} else { \
+				(*len)++; \
+			} \
+			(*str) += sizeof(unsigned long); \
+		} \
+	} while (0)
+
+	/*
+	 * Print function call entries in all stacks, starting at the
+	 * current stack address. If the stacks consist of nested
+	 * exceptions
+	 */
+	for (;;) {
+		const char *id;
+		unsigned long *estack_end;
+		estack_end = trace_in_exception_stack(cpu, (unsigned long)stack,
+						&used, &id);
+
+		if (estack_end) {
+			HANDLE_STACK (stack < estack_end);
+			/*
+			 * We link to the next stack via the
+			 * second-to-last pointer (index -2 to end) in the
+			 * exception stack:
+			 */
+			stack = (unsigned long *) estack_end[-2];
+			continue;
+		}
+		if (irqstack_end) {
+			unsigned long *irqstack;
+			irqstack = irqstack_end -
+				(IRQSTACKSIZE - 64) / sizeof(*irqstack);
+
+			if (stack >= irqstack && stack < irqstack_end) {
+				HANDLE_STACK (stack < irqstack_end);
+				/*
+				 * We link to the next stack (which would be
+				 * the process stack normally) the last
+				 * pointer (index -1 to end) in the IRQ stack:
+				 */
+				stack = (unsigned long *) (irqstack_end[-1]);
+				irqstack_end = NULL;
+				continue;
+			}
+		}
+		break;
+	}
+
+	/*
+	 * This handles the process stack:
+	 */
+	tinfo = current_thread_info();
+	HANDLE_STACK (valid_stack_ptr(tinfo, stack));
+#undef HANDLE_STACK
+}
+
+
+static char *ltt_serialize_kernel_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	int align, const char *fmt, va_list *args)
+{
+	/* Ugly and crude argument passing hack part 2. */
+	unsigned long *stack = (unsigned long*)fmt;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(unsigned long)));
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(unsigned long));
+	dump_trace(stack, buffer, &str, &closure->cb_args[0]);
+
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_kernel_stack(const struct __mark_marker_data *mdata,
+	const char *fmt, ...)
+{
+	struct pt_regs *regs;
+	va_list args;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || user_mode_vm(regs)))
+		goto end;
+	/* Ugly and crude argument passing hack part 1. */
+	ltt_vtrace(mdata, (const char*)&regs, args);
+end:
+	va_end(args);
+}
+#endif //CONFIG_LTT_KERNEL_STACK
+
+#ifdef CONFIG_LTT_PROCESS_STACK
+/* FIXME : how could we autodetect this.... ?!? disabled for now. */
+#if 0
+static void dump_fp_process32_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t next_ebp;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+
+	if (buffer)
+		*dest = PTR_LOW(regs->rip);
+	else
+		(*len)++;
+	dest++;
+
+	/* Start at the topmost ebp */
+	iter = (uint32_t*)PTR_LOW(regs->rbp);
+
+	/* Keep on going until we reach the end of the process' stack limit or
+	 * find an invalid ebp. */
+	while (!get_user(next_ebp, iter)) {
+		/* If another thread corrupts the thread user space stack
+		 * concurrently */
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		if (PTR_LOW(next_ebp) <= (unsigned long)(iter+1))
+			break;
+		if (get_user(addr, (uint32_t*)PTR_LOW(next_ebp)+1))
+			break;
+		if (buffer)
+			*dest = addr;
+		else
+			(*len)++;
+		dest++;
+		iter = (uint32_t*)PTR_LOW(next_ebp);
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+#endif //0
+
+static void dump_nofp_process32_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint32_t addr;
+	uint32_t *iter;
+	uint32_t *dest = (uint32_t*)*str;
+	uint32_t *prev_iter, *beg_iter;
+
+	if (buffer)
+		*dest = PTR_LOW(regs->rip);
+	else
+		(*len)++;
+	dest++;
+
+	/* FIXME : currently detects code addresses from executable only,
+	 * not libraries */
+	/* Start at the top of the user stack */
+	prev_iter = beg_iter = iter = (uint32_t*)PTR_LOW(regs->rsp);
+
+	while (!get_user(addr, iter)) {
+		if (buffer)
+			if (dest == (uint32_t*)(*str) + *len)
+				break;
+		/* Does this LOOK LIKE an address in the program */
+		if (addr > current->mm->start_code &&
+			addr < current->mm->end_code) {
+			if (buffer)
+				*dest = addr;
+			else
+				(*len)++;
+			dest++;
+		}
+		prev_iter = iter;
+		iter++;
+		if (iter >
+			prev_iter + CONFIG_LTT_PROCESS_MAX_FUNCTION_STACK)
+			break;
+		if (iter > beg_iter + CONFIG_LTT_PROCESS_MAX_STACK_LEN)
+			break;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint32_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+
+#if 0
+static void dump_fp_process64_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint64_t addr;
+	uint64_t next_rbp;
+	uint64_t *iter;
+	uint64_t *dest = (uint64_t*)*str;
+
+	if (buffer)
+		*dest = regs->rip;
+	else
+		(*len)++;
+	dest++;
+
+	/* Start at the topmost rbp */
+	iter = (uint64_t*)regs->rbp;
+
+	/* Keep on going until we reach the end of the process' stack limit or
+	 * find an invalid ebp. */
+	while (!get_user(next_rbp, iter)) {
+		/* If another thread corrupts the thread user space stack
+		 * concurrently */
+		if (buffer)
+			if (dest == (uint64_t*)(*str) + *len)
+				break;
+		if (next_rbp <= (uint64_t)(iter+1))
+			break;
+		if (get_user(addr, (uint64_t*)next_rbp+1))
+			break;
+		if (buffer)
+			*dest = addr;
+		else
+			(*len)++;
+		dest++;
+		iter = (uint64_t*)next_rbp;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint64_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+#endif //0
+
+static void dump_nofp_process64_stack(
+		struct pt_regs *regs,
+		char *buffer,
+		char **str,
+		long *len)
+{
+	uint64_t addr;
+	uint64_t *iter;
+	uint64_t *dest = (uint64_t*)*str;
+	uint64_t *prev_iter, *beg_iter;
+
+	if (buffer)
+		*dest = regs->rip;
+	else
+		(*len)++;
+	dest++;
+
+	/* FIXME : currently detects code addresses from executable only,
+	 * not libraries */
+	/* Start at the top of the user stack */
+	prev_iter = beg_iter = iter = (uint64_t*) regs->rsp;
+
+	while (!get_user(addr, iter)) {
+		if (buffer)
+			if (dest == (uint64_t*)(*str) + *len)
+				break;
+		/* Does this LOOK LIKE an address in the program */
+		if (addr > current->mm->start_code &&
+			addr < current->mm->end_code) {
+			if (buffer)
+				*dest = addr;
+			else
+				(*len)++;
+			dest++;
+		}
+		prev_iter = iter;
+		iter++;
+		if (iter >
+			prev_iter + CONFIG_LTT_PROCESS_MAX_FUNCTION_STACK)
+			break;
+		if (iter > beg_iter + CONFIG_LTT_PROCESS_MAX_STACK_LEN)
+			break;
+	}
+
+	/* Concurrently been changed : pad with zero */
+	if (buffer)
+		while (dest < (uint64_t*)(*str) + *len) {
+			*dest = 0;
+			dest++;
+		}
+	*str = (char*)dest;
+}
+
+static char *ltt_serialize_process32_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	int align, const char *fmt, va_list *args)
+{
+	struct pt_regs *regs;
+
+	/* Ugly and crude argument passing hack part 2 (revisited). */
+	regs = (struct pt_regs *)fmt;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(uint32_t)));
+
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(uint32_t));
+	dump_nofp_process32_stack(regs, buffer, &str,
+		&closure->cb_args[0]);
+
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+static char *ltt_serialize_process64_stack(char *buffer, char *str,
+	struct ltt_serialize_closure *closure,
+	int align, const char *fmt, va_list *args)
+{
+	struct pt_regs *regs;
+
+	/* Ugly and crude argument passing hack part 2 (revisited). */
+	regs = (struct pt_regs *)fmt;
+
+	if (align)
+		str += ltt_align((long)str,
+			max(sizeof(int), sizeof(uint64_t)));
+
+	if (buffer)
+		*(int*)str = (int)closure->cb_args[0];
+	str += sizeof(int);
+
+	if (!buffer)
+		closure->cb_args[0] = 0;
+
+	if (align)
+		str += ltt_align((long)str, sizeof(uint64_t));
+	dump_nofp_process64_stack(regs, buffer, &str,
+		&closure->cb_args[0]);
+
+	/* Following alignment for genevent
+	 * compatibility */
+	if (align)
+		str += ltt_align((long)str, sizeof(void*));
+	return str;
+}
+
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_process32_stack(const struct __mark_marker_data *mdata,
+	const char *fmt, ...)
+{
+	va_list args;
+	struct pt_regs *regs;
+
+	if (!test_thread_flag(TIF_IA32))
+		return;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || !user_mode_vm(regs)))
+		goto end;
+	/* Ugly and crude argument passing hack part 1 (revisited). */
+	ltt_vtrace(mdata, (const char*)regs, args);
+end:
+	va_end(args);
+}
+
+/* Expects va args : (struct pt_regs *regs) */
+static void ltt_trace_process64_stack(const struct __mark_marker_data *mdata,
+	const char *fmt, ...)
+{
+	va_list args;
+	struct pt_regs *regs;
+
+	if (test_thread_flag(TIF_IA32))
+		return;
+
+	va_start(args, fmt);
+	regs = va_arg(args, struct pt_regs *);
+	if (unlikely(!regs || !user_mode_vm(regs)))
+		goto end;
+	/* Ugly and crude argument passing hack part 1 (revisited). */
+	ltt_vtrace(mdata, (const char*)regs, args);
+end:
+	va_end(args);
+}
+#endif //CONFIG_LTT_PROCESS_STACK
+
+#define FACILITY_NAME "stack_arch"
+
+static struct ltt_probe_data probe_array[] =
+{
+#ifdef CONFIG_LTT_KERNEL_STACK
+	{ "stack_arch_irq_dump_kernel_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+		.eID = 0 },
+	{ "stack_arch_nmi_dump_kernel_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+		.eID = 1 },
+	{ "stack_arch_syscall_dump_kernel_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_kernel_stack,
+		.callbacks[0] = ltt_serialize_kernel_stack,
+		.eID = 2 },
+#endif //CONFIG_LTT_KERNEL_STACK
+#ifdef CONFIG_LTT_PROCESS_STACK
+	{ "stack_arch_irq_dump_process32_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_process32_stack,
+		.callbacks[0] = ltt_serialize_process32_stack,
+		.eID = 3 },
+	{ "stack_arch_syscall_dump_process32_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_process32_stack,
+		.callbacks[0] = ltt_serialize_process32_stack,
+		.eID = 4 },
+	{ "stack_arch_irq_dump_process64_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_process64_stack,
+		.callbacks[0] = ltt_serialize_process64_stack,
+		.eID = 5 },
+	{ "stack_arch_syscall_dump_process64_stack", MARK_NOARGS,
+		GET_CHANNEL_INDEX(cpu),
+		.probe_func = ltt_trace_process64_stack,
+		.callbacks[0] = ltt_serialize_process64_stack,
+		.eID = 6 },
+#endif //CONFIG_LTT_PROCESS_STACK
+};
+
+
+#define NUM_PROBES (sizeof(probe_array) / sizeof(struct ltt_probe_data))
+
+static struct ltt_facility facility = {
+	.name = FACILITY_NAME,
+	.num_events = NUM_PROBES,
+	.checksum = 0,
+	.id = 0xFF,
+	.alignment = 1,	/* 1: true, 0: false */
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	uint8_t i;
+	int ret;
+
+	/* FIXME : LTTV is unable to compute this CRC (for now) */
+	for (i = 0; i < NUM_PROBES; i++) {
+		facility.checksum =
+			crc32(facility.checksum, probe_array[i].name,
+				strlen(probe_array[i].name));
+		facility.checksum =
+			crc32(facility.checksum, probe_array[i].format,
+				strlen(probe_array[i].format));
+
+	}
+	ret = ltt_facility_kernel_register(&facility);
+	if (ret < 0) {
+		printk(KERN_WARNING "LTT : Error in registering facility %s\n",
+			facility.name);
+		return ret;
+	}
+	facility.id = (uint8_t)ret;
+
+	printk("LTT : Facility %s registered with id %hu\n", facility.name,
+		facility.id);
+
+	for (i = 0; i < NUM_PROBES; i++) {
+		probe_array[i].fID = facility.id;
+		probe_array[i].align = facility.alignment;
+		result = marker_set_probe(probe_array[i].name,
+				probe_array[i].format,
+				probe_array[i].probe_func, &probe_array[i]);
+		if (!result)
+			printk(KERN_INFO "LTT unable to register probe %s\n",
+				probe_array[i].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	uint8_t i;
+	int err;
+
+	for (i = 0; i < NUM_PROBES; i++) {
+		marker_remove_probe(probe_array[i].name);
+	}
+	synchronize_sched();	/* Wait for probes to finish */
+	err = ltt_facility_unregister(facility.id);
+	if (err)
+		printk(KERN_WARNING
+			"LTT : Error in unregistering facility %s\n",
+			facility.name);
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION(FACILITY_NAME " probe");
Index: linux/arch/sh/kernel/sh_ksyms.c
===================================================================
--- linux.orig/arch/sh/kernel/sh_ksyms.c
+++ linux/arch/sh/kernel/sh_ksyms.c
@@ -29,7 +29,6 @@
 /* platform dependent support */
 EXPORT_SYMBOL(dump_fpu);
 EXPORT_SYMBOL(kernel_thread);
-EXPORT_SYMBOL(irq_desc);
 EXPORT_SYMBOL(no_irq_type);
 
 EXPORT_SYMBOL(strlen);
